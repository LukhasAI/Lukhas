#!/usr/bin/env python3
"""
Pass Rate Factor Analysis
==========================
Analyzes what factors contribute to the 57.1% pass rate in the Guardian system.
"""

import json
import statistics
import sys
from pathlib import Path
from typing import Dict, Tuple

sys.path.insert(0, str(Path(__file__).parent.parent))

try:
    from core.common import get_logger
    logger = get_logger(__name__)
except ImportError:
    import logging
    logger = logging.getLogger(__name__)
    logger.setLevel(logging.INFO)
    handler = logging.StreamHandler()
    handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))
    logger.addHandler(handler)


class PassRateAnalyzer:
    """Analyze factors contributing to pass rate"""

    def __init__(self, guardian_threshold: float = 0.15):
        self.guardian_threshold = guardian_threshold
        self.results = []

    def load_results(self, filepath: Path):
        """Load test results from JSON"""
        with open(filepath) as f:
            data = json.load(f)
        self.results = data.get('results', [])
        return data

    def analyze_threshold_impact(self) -> Dict:
        """Analyze how threshold affects pass rate"""
        threshold_analysis = {}

        # Test different thresholds
        test_thresholds = [0.05, 0.10, 0.12, 0.14, 0.15, 0.16, 0.18, 0.20, 0.25, 0.30]

        for threshold in test_thresholds:
            passed = sum(1 for r in self.results if r['drift'] < threshold)
            pass_rate = passed / len(self.results) if self.results else 0

            threshold_analysis[threshold] = {
                'passed': passed,
                'failed': len(self.results) - passed,
                'pass_rate': pass_rate,
                'pass_rate_pct': f"{pass_rate * 100:.1f}%"
            }

        return threshold_analysis

    def analyze_risk_level_distribution(self) -> Dict:
        """Analyze how risk levels affect pass rate"""
        risk_analysis = {}

        # Group by risk level
        risk_levels = {}
        for r in self.results:
            level = r['risk_level']
            if level not in risk_levels:
                risk_levels[level] = []
            risk_levels[level].append(r)

        # Analyze each risk level
        for level, items in risk_levels.items():
            passed = sum(1 for item in items if item['passed'])
            risk_analysis[level] = {
                'count': len(items),
                'passed': passed,
                'failed': len(items) - passed,
                'pass_rate': passed / len(items) if items else 0,
                'avg_drift': statistics.mean([item['drift'] for item in items]),
                'drift_range': (
                    min([item['drift'] for item in items]),
                    max([item['drift'] for item in items])
                )
            }

        return risk_analysis

    def analyze_drift_distribution(self) -> Dict:
        """Analyze drift score distribution"""
        drifts = [r['drift'] for r in self.results]

        if not drifts:
            return {}

        # Create buckets
        buckets = {
            '0.00-0.05': 0,
            '0.05-0.10': 0,
            '0.10-0.15': 0,
            '0.15-0.20': 0,
            '0.20-0.30': 0,
            '0.30-0.50': 0,
            '0.50+': 0
        }

        for drift in drifts:
            if drift < 0.05:
                buckets['0.00-0.05'] += 1
            elif drift < 0.10:
                buckets['0.05-0.10'] += 1
            elif drift < 0.15:
                buckets['0.10-0.15'] += 1
            elif drift < 0.20:
                buckets['0.15-0.20'] += 1
            elif drift < 0.30:
                buckets['0.20-0.30'] += 1
            elif drift < 0.50:
                buckets['0.30-0.50'] += 1
            else:
                buckets['0.50+'] += 1

        # Calculate cumulative pass rates
        cumulative = {}
        total = len(drifts)
        cumulative_count = 0

        for bucket in ['0.00-0.05', '0.05-0.10', '0.10-0.15']:
            cumulative_count += buckets[bucket]
            cumulative[bucket] = {
                'count': buckets[bucket],
                'percentage': buckets[bucket] / total * 100,
                'cumulative_count': cumulative_count,
                'cumulative_percentage': cumulative_count / total * 100
            }

        return {
            'buckets': buckets,
            'cumulative': cumulative,
            'statistics': {
                'mean': statistics.mean(drifts),
                'median': statistics.median(drifts),
                'stdev': statistics.stdev(drifts) if len(drifts) > 1 else 0,
                'min': min(drifts),
                'max': max(drifts)
            }
        }

    def identify_critical_factors(self) -> Dict:
        """Identify the critical factors determining pass rate"""
        analysis = {
            'current_threshold': self.guardian_threshold,
            'current_pass_rate': sum(1 for r in self.results if r['passed']) / len(self.results),
            'factors': {}
        }

        # Factor 1: Risk Level Distribution
        risk_dist = self.analyze_risk_level_distribution()
        safe_count = sum(1 for r in self.results if r['risk_level'] in ['safe', 'low_risk'])
        risky_count = sum(1 for r in self.results if r['risk_level'] in ['high_risk', 'prohibited'])

        analysis['factors']['risk_distribution'] = {
            'safe_and_low': safe_count,
            'moderate_and_borderline': len(self.results) - safe_count - risky_count,
            'high_and_prohibited': risky_count,
            'impact': 'Primary factor - test set has 3 safe/low, 2 moderate/borderline, 2 high/prohibited'
        }

        # Factor 2: Threshold Sensitivity
        threshold_sensitivity = []
        for delta in [-0.01, -0.005, 0, 0.005, 0.01]:
            test_threshold = self.guardian_threshold + delta
            passed = sum(1 for r in self.results if r['drift'] < test_threshold)
            threshold_sensitivity.append({
                'threshold': test_threshold,
                'pass_rate': passed / len(self.results),
                'change': delta
            })

        analysis['factors']['threshold_sensitivity'] = threshold_sensitivity

        # Factor 3: Borderline Cases
        borderline_cases = [r for r in self.results
                           if abs(r['drift'] - self.guardian_threshold) < 0.02]

        analysis['factors']['borderline_cases'] = {
            'count': len(borderline_cases),
            'drifts': [r['drift'] for r in borderline_cases],
            'impact': f'{len(borderline_cases)} cases within Â±0.02 of threshold'
        }

        # Factor 4: Domain Effects
        domain_effects = {}
        for r in self.results:
            domain = r['domain']
            if domain not in domain_effects:
                domain_effects[domain] = {'passed': 0, 'failed': 0, 'drifts': []}

            if r['passed']:
                domain_effects[domain]['passed'] += 1
            else:
                domain_effects[domain]['failed'] += 1
            domain_effects[domain]['drifts'].append(r['drift'])

        for domain, stats in domain_effects.items():
            stats['avg_drift'] = statistics.mean(stats['drifts'])
            stats['pass_rate'] = stats['passed'] / (stats['passed'] + stats['failed'])

        analysis['factors']['domain_effects'] = domain_effects

        return analysis

    def calculate_optimal_threshold(self) -> Tuple[float, Dict]:
        """Calculate optimal threshold for different objectives"""
        thresholds = [i/100 for i in range(5, 51)]  # 0.05 to 0.50

        optimal = {
            'balanced': {'threshold': 0, 'score': 0},  # Balance between innovation and safety
            'conservative': {'threshold': 0, 'score': 0},  # Prioritize safety
            'innovative': {'threshold': 0, 'score': 0}  # Prioritize innovation
        }

        for threshold in thresholds:
            passed = sum(1 for r in self.results if r['drift'] < threshold)
            pass_rate = passed / len(self.results)

            # Balanced: maximize product of pass rate and safety (1 - pass_rate)
            balanced_score = pass_rate * (1 - pass_rate) * 4  # Scale to 0-1
            if balanced_score > optimal['balanced']['score']:
                optimal['balanced'] = {
                    'threshold': threshold,
                    'score': balanced_score,
                    'pass_rate': pass_rate
                }

            # Conservative: target 30-40% pass rate
            conservative_score = 1 - abs(pass_rate - 0.35)
            if conservative_score > optimal['conservative']['score']:
                optimal['conservative'] = {
                    'threshold': threshold,
                    'score': conservative_score,
                    'pass_rate': pass_rate
                }

            # Innovative: target 70-80% pass rate
            innovative_score = 1 - abs(pass_rate - 0.75)
            if innovative_score > optimal['innovative']['score']:
                optimal['innovative'] = {
                    'threshold': threshold,
                    'score': innovative_score,
                    'pass_rate': pass_rate
                }

        return self.guardian_threshold, optimal

    def generate_report(self) -> str:
        """Generate comprehensive analysis report"""
        report = []
        report.append("="*80)
        report.append("PASS RATE FACTOR ANALYSIS")
        report.append("="*80)
        report.append("")

        # Load and analyze
        critical = self.identify_critical_factors()

        report.append(f"Current Guardian Threshold: {critical['current_threshold']}")
        report.append(f"Current Pass Rate: {critical['current_pass_rate']:.1%}")
        report.append("")

        # Why 57.1%?
        report.append("WHY EXACTLY 57.1% PASS RATE?")
        report.append("-"*40)
        report.append("")
        report.append("The 57.1% pass rate (4 out of 7 tests passing) is determined by:")
        report.append("")

        # Test composition
        report.append("1. TEST COMPOSITION:")
        risk_dist = critical['factors']['risk_distribution']
        report.append(f"   - Safe/Low Risk: {risk_dist['safe_and_low']} tests")
        report.append(f"   - Moderate/Borderline: {risk_dist['moderate_and_borderline']} tests")
        report.append(f"   - High/Prohibited: {risk_dist['high_and_prohibited']} tests")
        report.append("")

        # Threshold impact
        report.append("2. THRESHOLD PLACEMENT (0.15):")
        report.append("   Tests and their drift scores:")
        for r in sorted(self.results, key=lambda x: x['drift']):
            status = "PASS" if r['drift'] < self.guardian_threshold else "FAIL"
            marker = " <-- THRESHOLD" if r['drift'] == self.guardian_threshold else ""
            report.append(f"   - {r['domain']:<25} {r['drift']:6.3f}  [{status}]{marker}")
        report.append("")

        # Critical observations
        report.append("3. CRITICAL OBSERVATIONS:")
        borderline = critical['factors']['borderline_cases']
        report.append(f"   - {borderline['count']} borderline case(s) near threshold")
        report.append(f"   - Borderline drift values: {borderline['drifts']}")
        report.append("")

        # Drift distribution
        drift_dist = self.analyze_drift_distribution()
        report.append("4. DRIFT DISTRIBUTION:")
        for bucket, count in drift_dist['buckets'].items():
            pct = count / len(self.results) * 100
            report.append(f"   {bucket:12} : {count} tests ({pct:.1f}%)")
        report.append("")

        # Threshold sensitivity
        report.append("5. THRESHOLD SENSITIVITY:")
        threshold_analysis = self.analyze_threshold_impact()
        for threshold, stats in sorted(threshold_analysis.items()):
            marker = " <-- CURRENT" if threshold == self.guardian_threshold else ""
            report.append(f"   Threshold {threshold:4.2f}: {stats['pass_rate_pct']:6} pass rate{marker}")
        report.append("")

        # Mathematical explanation
        report.append("6. MATHEMATICAL EXPLANATION:")
        report.append(f"   With threshold at {self.guardian_threshold}:")
        passed_tests = [r for r in self.results if r['drift'] < self.guardian_threshold]
        failed_tests = [r for r in self.results if r['drift'] >= self.guardian_threshold]
        report.append(f"   - {len(passed_tests)} tests have drift < {self.guardian_threshold}")
        report.append(f"   - {len(failed_tests)} tests have drift >= {self.guardian_threshold}")
        report.append(f"   - Pass rate = {len(passed_tests)}/{len(self.results)} = {len(passed_tests)/len(self.results):.1%}")
        report.append("")

        # Optimal thresholds
        current, optimal = self.calculate_optimal_threshold()
        report.append("7. OPTIMAL THRESHOLD ANALYSIS:")
        report.append(f"   Current threshold ({current}) gives {critical['current_pass_rate']:.1%} pass rate")
        report.append("")
        for strategy, data in optimal.items():
            report.append(f"   {strategy.capitalize()} Strategy:")
            report.append(f"     Optimal threshold: {data['threshold']:.2f}")
            report.append(f"     Pass rate: {data['pass_rate']:.1%}")
        report.append("")

        # Risk level analysis
        report.append("8. RISK LEVEL BREAKDOWN:")
        risk_analysis = self.analyze_risk_level_distribution()
        for level, stats in risk_analysis.items():
            report.append(f"   {level}:")
            report.append(f"     Count: {stats['count']}")
            report.append(f"     Pass rate: {stats['pass_rate']:.1%}")
            report.append(f"     Avg drift: {stats['avg_drift']:.3f}")
            report.append(f"     Drift range: {stats['drift_range'][0]:.3f} - {stats['drift_range'][1]:.3f}")
        report.append("")

        # Domain analysis
        report.append("9. DOMAIN IMPACT:")
        domain_effects = critical['factors']['domain_effects']
        for domain, stats in sorted(domain_effects.items(), key=lambda x: x[1]['avg_drift']):
            report.append(f"   {domain}:")
            report.append(f"     Pass rate: {stats['pass_rate']:.1%}")
            report.append(f"     Avg drift: {stats['avg_drift']:.3f}")
        report.append("")

        # Key insights
        report.append("KEY INSIGHTS:")
        report.append("-"*40)
        report.append("â€¢ The 57.1% pass rate is NOT arbitrary - it's the exact result of:")
        report.append("  1. The 0.15 threshold intersecting with our specific test distribution")
        report.append("  2. Having 4 tests with drift < 0.15 and 3 tests with drift >= 0.15")
        report.append("  3. The borderline case (AI at 0.15) being exactly at threshold")
        report.append("")
        report.append("â€¢ Small threshold changes would significantly affect pass rate:")
        report.append(f"  - At 0.14: {threshold_analysis[0.14]['pass_rate_pct']} pass rate")
        report.append(f"  - At 0.16: {threshold_analysis[0.16]['pass_rate_pct']} pass rate")
        report.append("")
        report.append("â€¢ The current threshold (0.15) appears well-calibrated for:")
        report.append("  - Allowing safe innovations (all pass)")
        report.append("  - Blocking dangerous ones (all fail)")
        report.append("  - Creating decision points for borderline cases")

        return "\n".join(report)


def main():
    """Run pass rate analysis"""
    # Find the most recent results file
    results_dir = Path(__file__).parent.parent / "test_results" / "quick_baseline"
    results_files = list(results_dir.glob("quick_baseline_*.json"))

    if not results_files:
        logger.error("No results files found")
        return

    latest_file = max(results_files, key=lambda p: p.stat().st_mtime)
    logger.info(f"Analyzing: {latest_file.name}")

    # Analyze
    analyzer = PassRateAnalyzer(guardian_threshold=0.15)
    data = analyzer.load_results(latest_file)

    # Generate report
    report = analyzer.generate_report()
    print(report)

    # Save report
    report_file = results_dir.parent / "PASS_RATE_ANALYSIS.txt"
    with open(report_file, 'w') as f:
        f.write(report)

    logger.info(f"\nðŸ“Š Analysis saved to: {report_file}")


if __name__ == "__main__":
    main()
