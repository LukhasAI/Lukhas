# Ollama Configuration for LUKHAS Auto-Improvement

# Ollama API endpoint (default: http://localhost:11434)
OLLAMA_HOST=http://localhost:11434

# Primary model for code fixing
# Options: codellama:7b, qwen2.5-coder:1.5b, deepseek-coder:6.7b
OLLAMA_MODEL=codellama:7b

# Fallback model (smaller/faster)
OLLAMA_FALLBACK_MODEL=qwen2.5-coder:1.5b

# Temperature for code generation (lower = more deterministic)
OLLAMA_TEMPERATURE=0.3

# Maximum tokens for responses
OLLAMA_MAX_TOKENS=500

# Guardian threshold for auto-applying fixes (0.0-1.0)
GUARDIAN_THRESHOLD=0.85

# Enable learning from successful fixes
LEARNING_ENABLED=true
