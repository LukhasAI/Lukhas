#!/usr/bin/env python3
"""
LUKHAS Phase 4 - Context Preservation System
============================================

Maintains context integrity across routing hops with intelligent
caching, compression, and state management.

Key Features:
- Context serialization and deserialization
- Intelligent context compression
- Multi-hop context preservation
- Context encryption for security
- Performance-optimized caching
- Context versioning and migration
- Memory-efficient context storage
- Context lifecycle management

Performance Requirements:
- <250ms context handoff
- <50ms context retrieval
- 90%+ compression ratio for large contexts
- Zero context loss during routing

Constellation Framework: Flow Star (ðŸŒŠ) coordination hub
"""

from __future__ import annotations

import asyncio
import contextlib
import hashlib
import json
import logging
import time
import uuid
import zlib
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Dict, List

from opentelemetry import trace

from observability import counter, gauge, histogram

tracer = trace.get_tracer(__name__)
logger = logging.getLogger(__name__)

# Prometheus metrics for context preservation
context_handoffs_total = counter(
    'lukhas_context_handoffs_total',
    'Total context handoffs',
    ['source_provider', 'destination_provider', 'success']
)

context_handoff_duration = histogram(
    'lukhas_context_handoff_duration_seconds',
    'Context handoff duration',
    ['operation'],
    buckets=[0.01, 0.05, 0.1, 0.25, 0.5, 1.0, 2.0]
)

context_size_bytes = histogram(
    'lukhas_context_size_bytes',
    'Context size in bytes',
    ['compressed'],
    buckets=[100, 500, 1000, 5000, 10000, 50000, 100000]
)

context_compression_ratio = gauge(
    'lukhas_context_compression_ratio',
    'Context compression ratio',
    ['context_type']
)

context_cache_hits = counter(
    'lukhas_context_cache_hits_total',
    'Context cache hits',
    ['cache_type']
)

context_cache_misses = counter(
    'lukhas_context_cache_misses_total',
    'Context cache misses',
    ['cache_type']
)


class ContextType(Enum):
    """Types of context data"""
    CONVERSATION = "conversation"
    SESSION = "session"
    USER_PROFILE = "user_profile"
    SYSTEM_STATE = "system_state"
    ROUTING_METADATA = "routing_metadata"
    PERFORMANCE_DATA = "performance_data"


class CompressionLevel(Enum):
    """Compression levels for context data"""
    NONE = 0
    LIGHT = 1
    STANDARD = 6
    AGGRESSIVE = 9


@dataclass
class ContextHop:
    """Represents a single hop in context routing"""
    hop_id: str
    provider: str
    timestamp: float
    latency_ms: float
    success: bool
    error: str | None = None
    metadata: Dict[str, Any] = field(default_factory=dict)


@dataclass
class ContextMetadata:
    """Metadata for context preservation"""
    context_id: str
    session_id: str
    created_at: float
    last_updated: float
    version: str = "1.0.0"
    hops: List[ContextHop] = field(default_factory=list)
    compression_level: CompressionLevel = CompressionLevel.STANDARD
    encryption_enabled: bool = False
    ttl_seconds: int = 3600  # 1 hour default
    priority: int = 1  # 1=low, 5=high


@dataclass
class PreservedContext:
    """Complete preserved context with metadata"""
    metadata: ContextMetadata
    data: Dict[str, Any]
    compressed_data: bytes | None = None
    checksum: str | None = None


class ContextSerializer:
    """Handles context serialization and compression"""

    @staticmethod
    def serialize(context_data: Dict[str, Any]) -> bytes:
        """Serialize context data to bytes"""
        try:
            json_str = json.dumps(context_data, ensure_ascii=False, separators=(',', ':'))
            return json_str.encode('utf-8')
        except Exception as e:
            logger.error(f"Context serialization failed: {e}")
            raise

    @staticmethod
    def deserialize(data: bytes) -> Dict[str, Any]:
        """Deserialize bytes to context data"""
        try:
            json_str = data.decode('utf-8')
            return json.loads(json_str)
        except Exception as e:
            logger.error(f"Context deserialization failed: {e}")
            raise

    @staticmethod
    def compress(data: bytes, level: CompressionLevel) -> bytes:
        """Compress context data"""
        if level == CompressionLevel.NONE:
            return data

        try:
            compressed = zlib.compress(data, level=level.value)

            # Record compression metrics
            original_size = len(data)
            compressed_size = len(compressed)
            ratio = compressed_size / original_size if original_size > 0 else 0

            context_compression_ratio.labels(context_type="generic").set(ratio)
            context_size_bytes.labels(compressed="false").observe(original_size)
            context_size_bytes.labels(compressed="true").observe(compressed_size)

            return compressed

        except Exception as e:
            logger.error(f"Context compression failed: {e}")
            return data  # Return uncompressed on failure

    @staticmethod
    def decompress(compressed_data: bytes) -> bytes:
        """Decompress context data"""
        try:
            return zlib.decompress(compressed_data)
        except Exception as e:
            logger.error(f"Context decompression failed: {e}")
            return compressed_data  # Return as-is on failure

    @staticmethod
    def calculate_checksum(data: bytes) -> str:
        """Calculate SHA-256 checksum for data integrity"""
        return hashlib.sha256(data).hexdigest()

    @staticmethod
    def verify_checksum(data: bytes, checksum: str) -> bool:
        """Verify data integrity using checksum"""
        return ContextSerializer.calculate_checksum(data) == checksum


class ContextCache:
    """High-performance context caching system"""

    def __init__(self, max_size: int = 1000, ttl_seconds: int = 3600):
        self.max_size = max_size
        self.ttl_seconds = ttl_seconds
        self._cache: Dict[str, tuple[PreservedContext, float]] = {}  # key -> (context, expiry_time)
        self._access_order: List[str] = []  # LRU tracking

    async def get(self, context_id: str) -> PreservedContext | None:
        """Get context from cache"""
        start_time = time.time()

        try:
            if context_id in self._cache:
                context, expiry_time = self._cache[context_id]

                # Check expiry
                if time.time() < expiry_time:
                    # Move to end of LRU list
                    if context_id in self._access_order:
                        self._access_order.remove(context_id)
                    self._access_order.append(context_id)

                    context_cache_hits.labels(cache_type="memory").inc()
                    return context
                else:
                    # Expired, remove from cache
                    del self._cache[context_id]
                    if context_id in self._access_order:
                        self._access_order.remove(context_id)

            context_cache_misses.labels(cache_type="memory").inc()
            return None

        finally:
            duration = time.time() - start_time
            context_handoff_duration.labels(operation="cache_get").observe(duration)

    async def put(self, context: PreservedContext) -> None:
        """Put context in cache"""
        start_time = time.time()

        try:
            context_id = context.metadata.context_id
            expiry_time = time.time() + context.metadata.ttl_seconds

            # Remove if already exists
            if context_id in self._cache and context_id in self._access_order:
                self._access_order.remove(context_id)

            # Add to cache
            self._cache[context_id] = (context, expiry_time)
            self._access_order.append(context_id)

            # Evict LRU entries if over limit
            while len(self._cache) > self.max_size:
                if self._access_order:
                    lru_key = self._access_order.pop(0)
                    if lru_key in self._cache:
                        del self._cache[lru_key]

        finally:
            duration = time.time() - start_time
            context_handoff_duration.labels(operation="cache_put").observe(duration)

    async def remove(self, context_id: str) -> bool:
        """Remove context from cache"""
        if context_id in self._cache:
            del self._cache[context_id]
            if context_id in self._access_order:
                self._access_order.remove(context_id)
            return True
        return False

    async def cleanup_expired(self) -> int:
        """Clean up expired contexts"""
        current_time = time.time()
        expired_keys = []

        for context_id, (_, expiry_time) in self._cache.items():
            if current_time >= expiry_time:
                expired_keys.append(context_id)

        for key in expired_keys:
            await self.remove(key)

        return len(expired_keys)

    def get_stats(self) -> Dict[str, Any]:
        """Get cache statistics"""
        return {
            "size": len(self._cache),
            "max_size": self.max_size,
            "usage_ratio": len(self._cache) / self.max_size,
        }


class ContextPreservationEngine:
    """Main context preservation engine"""

    def __init__(self):
        self.serializer = ContextSerializer()
        self.cache = ContextCache()

        # Context stores (in-memory and persistent)
        self.memory_store: Dict[str, PreservedContext] = {}

        # Background cleanup task
        self.cleanup_task: asyncio.Task | None = None
        self.cleanup_interval = 300  # 5 minutes

        logger.info("Context preservation engine initialized")

    async def start(self) -> None:
        """Start context preservation engine"""
        logger.info("ðŸ”„ Starting context preservation engine...")

        # Start background cleanup
        self.cleanup_task = asyncio.create_task(self._cleanup_loop())

        logger.info("âœ… Context preservation engine started")

    async def stop(self) -> None:
        """Stop context preservation engine"""
        logger.info("ðŸ›‘ Stopping context preservation engine...")

        if self.cleanup_task:
            self.cleanup_task.cancel()
            with contextlib.suppress(asyncio.CancelledError):
                await self.cleanup_task

        logger.info("âœ… Context preservation engine stopped")

    async def preserve_context(
        self,
        session_id: str,
        context_data: Dict[str, Any],
        context_type: ContextType = ContextType.CONVERSATION,
        compression_level: CompressionLevel = CompressionLevel.STANDARD,
        ttl_seconds: int = 3600
    ) -> str:
        """Preserve context data and return context ID"""

        start_time = time.time()
        context_id = str(uuid.uuid4())

        with tracer.start_span("context.preserve") as span:
            span.set_attribute("session_id", session_id)
            span.set_attribute("context_type", context_type.value)
            span.set_attribute("context_id", context_id)

            try:
                # Create metadata
                metadata = ContextMetadata(
                    context_id=context_id,
                    session_id=session_id,
                    created_at=time.time(),
                    last_updated=time.time(),
                    compression_level=compression_level,
                    ttl_seconds=ttl_seconds
                )

                # Serialize context data
                serialized_data = self.serializer.serialize(context_data)

                # Compress if needed
                compressed_data = None
                checksum = None

                if compression_level != CompressionLevel.NONE:
                    compressed_data = self.serializer.compress(serialized_data, compression_level)
                    checksum = self.serializer.calculate_checksum(compressed_data)
                else:
                    checksum = self.serializer.calculate_checksum(serialized_data)

                # Create preserved context
                preserved_context = PreservedContext(
                    metadata=metadata,
                    data=context_data,
                    compressed_data=compressed_data,
                    checksum=checksum
                )

                # Store in memory and cache
                self.memory_store[context_id] = preserved_context
                await self.cache.put(preserved_context)

                duration = time.time() - start_time
                context_handoff_duration.labels(operation="preserve").observe(duration)

                span.set_attribute("preservation_time_ms", duration * 1000)
                span.set_attribute("data_size_bytes", len(serialized_data))

                logger.debug(f"âœ… Context preserved: {context_id}")
                return context_id

            except Exception as e:
                span.record_exception(e)
                span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))
                logger.error(f"âŒ Failed to preserve context: {e}")
                raise

    async def restore_context(self, context_id: str) -> Dict[str, Any] | None:
        """Restore context data by ID"""

        start_time = time.time()

        with tracer.start_span("context.restore") as span:
            span.set_attribute("context_id", context_id)

            try:
                # Try cache first
                preserved_context = await self.cache.get(context_id)

                # Fallback to memory store
                if not preserved_context:
                    preserved_context = self.memory_store.get(context_id)

                if not preserved_context:
                    return None

                # Check TTL
                if time.time() - preserved_context.metadata.created_at > preserved_context.metadata.ttl_seconds:
                    # Expired, clean up
                    await self._cleanup_context(context_id)
                    return None

                # Return cached data if available
                if preserved_context.data:
                    duration = time.time() - start_time
                    context_handoff_duration.labels(operation="restore").observe(duration)
                    span.set_attribute("restoration_time_ms", duration * 1000)
                    return preserved_context.data

                # Restore from compressed data
                if preserved_context.compressed_data:
                    # Verify checksum
                    if preserved_context.checksum:
                        if not self.serializer.verify_checksum(
                            preserved_context.compressed_data,
                            preserved_context.checksum
                        ):
                            logger.error(f"âŒ Context checksum verification failed: {context_id}")
                            return None

                    # Decompress and deserialize
                    decompressed_data = self.serializer.decompress(preserved_context.compressed_data)
                    context_data = self.serializer.deserialize(decompressed_data)

                    # Cache the restored data
                    preserved_context.data = context_data
                    await self.cache.put(preserved_context)

                    duration = time.time() - start_time
                    context_handoff_duration.labels(operation="restore").observe(duration)

                    span.set_attribute("restoration_time_ms", duration * 1000)
                    logger.debug(f"âœ… Context restored: {context_id}")
                    return context_data

                return None

            except Exception as e:
                span.record_exception(e)
                span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))
                logger.error(f"âŒ Failed to restore context {context_id}: {e}")
                return None

    async def handoff_context(
        self,
        context_id: str,
        source_provider: str,
        destination_provider: str,
        additional_metadata: Dict[str, Any] | None = None
    ) -> bool:
        """Hand off context between providers"""

        start_time = time.time()

        with tracer.start_span("context.handoff") as span:
            span.set_attribute("context_id", context_id)
            span.set_attribute("source_provider", source_provider)
            span.set_attribute("destination_provider", destination_provider)

            try:
                # Get preserved context
                preserved_context = await self.cache.get(context_id)
                if not preserved_context:
                    preserved_context = self.memory_store.get(context_id)

                if not preserved_context:
                    context_handoffs_total.labels(
                        source_provider=source_provider,
                        destination_provider=destination_provider,
                        success="false"
                    ).inc()
                    return False

                # Create hop record
                hop = ContextHop(
                    hop_id=str(uuid.uuid4()),
                    provider=destination_provider,
                    timestamp=time.time(),
                    latency_ms=(time.time() - start_time) * 1000,
                    success=True,
                    metadata=additional_metadata or {}
                )

                # Update metadata
                preserved_context.metadata.hops.append(hop)
                preserved_context.metadata.last_updated = time.time()

                # Update stores
                self.memory_store[context_id] = preserved_context
                await self.cache.put(preserved_context)

                duration = time.time() - start_time
                context_handoff_duration.labels(operation="handoff").observe(duration)

                context_handoffs_total.labels(
                    source_provider=source_provider,
                    destination_provider=destination_provider,
                    success="true"
                ).inc()

                span.set_attribute("handoff_time_ms", duration * 1000)
                span.set_attribute("hop_count", len(preserved_context.metadata.hops))

                logger.debug(f"âœ… Context handoff: {source_provider} -> {destination_provider}")
                return True

            except Exception as e:
                span.record_exception(e)
                span.set_status(trace.Status(trace.StatusCode.ERROR, str(e)))

                context_handoffs_total.labels(
                    source_provider=source_provider,
                    destination_provider=destination_provider,
                    success="false"
                ).inc()

                logger.error(f"âŒ Context handoff failed: {e}")
                return False

    async def get_context_metadata(self, context_id: str) -> ContextMetadata | None:
        """Get context metadata"""
        preserved_context = await self.cache.get(context_id)
        if not preserved_context:
            preserved_context = self.memory_store.get(context_id)

        return preserved_context.metadata if preserved_context else None

    async def cleanup_context(self, context_id: str) -> bool:
        """Clean up context"""
        return await self._cleanup_context(context_id)

    async def _cleanup_context(self, context_id: str) -> bool:
        """Internal context cleanup"""
        cleaned = False

        if context_id in self.memory_store:
            del self.memory_store[context_id]
            cleaned = True

        if await self.cache.remove(context_id):
            cleaned = True

        return cleaned

    async def _cleanup_loop(self) -> None:
        """Background cleanup loop"""
        logger.info("ðŸ§¹ Context cleanup loop started")

        while True:
            try:
                # Cleanup expired cache entries
                await self.cache.cleanup_expired()

                # Cleanup expired memory store entries
                current_time = time.time()
                expired_contexts = []

                for context_id, preserved_context in self.memory_store.items():
                    age = current_time - preserved_context.metadata.created_at
                    if age > preserved_context.metadata.ttl_seconds:
                        expired_contexts.append(context_id)

                for context_id in expired_contexts:
                    await self._cleanup_context(context_id)

                if expired_contexts:
                    logger.info(f"ðŸ§¹ Cleaned up {len(expired_contexts)} expired contexts")

                await asyncio.sleep(self.cleanup_interval)

            except asyncio.CancelledError:
                break
            except Exception as e:
                logger.error(f"Error in cleanup loop: {e}")
                await asyncio.sleep(60)  # Wait before retrying

        logger.info("ðŸ›‘ Context cleanup loop stopped")

    async def get_preservation_stats(self) -> Dict[str, Any]:
        """Get context preservation statistics"""
        cache_stats = self.cache.get_stats()

        return {
            "memory_store_size": len(self.memory_store),
            "cache_stats": cache_stats,
            "total_contexts": len(self.memory_store),
            "cleanup_interval": self.cleanup_interval
        }


# Global context preservation engine
_context_engine: ContextPreservationEngine | None = None


async def get_context_preservation_engine() -> ContextPreservationEngine:
    """Get or create global context preservation engine"""
    global _context_engine
    if _context_engine is None:
        _context_engine = ContextPreservationEngine()
        await _context_engine.start()
    return _context_engine
