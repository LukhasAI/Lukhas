"""
SLO Burn Rate Alerting System
=============================

P0-3 OBS-BURN: Advanced burn rate alerting system for SLO monitoring
with multi-window detection, early warning, and CI/CD integration.

Features:
- Multi-window burn rate calculation
- Early warning system for SLO violations
- Integration with Prometheus alerting
- CI/CD pipeline integration
- Emergency escalation procedures
- T4/0.01% excellence SLO enforcement
"""

import logging
from collections import deque
from dataclasses import dataclass, field
from datetime import datetime, timedelta, timezone
from enum import Enum
from typing import Any, Callable, Dict, List, Optional, Tuple

logger = logging.getLogger(__name__)


class AlertSeverity(Enum):
    """Alert severity levels"""
    INFO = "info"
    WARNING = "warning"
    CRITICAL = "critical"
    EMERGENCY = "emergency"


class BurnRateWindow(Enum):
    """Time windows for burn rate calculation"""
    FAST_1H = "1h"      # Fast burn: 1 hour window
    FAST_6H = "6h"      # Medium burn: 6 hour window
    SLOW_24H = "24h"    # Slow burn: 24 hour window
    SLOW_7D = "7d"      # Very slow burn: 7 day window


@dataclass
class SLODefinition:
    """Service Level Objective definition"""
    service: str
    slo_name: str
    target: float  # e.g., 0.999 for 99.9%
    time_period: str  # e.g., "30d" for 30 days
    error_budget_percent: float  # Calculated from target
    description: str = ""


@dataclass
class BurnRateThreshold:
    """Burn rate threshold configuration"""
    window: BurnRateWindow
    threshold: float  # Burn rate multiplier (e.g., 2.0 = 2x normal)
    alert_severity: AlertSeverity
    notification_channels: list[str] = field(default_factory=list)
    cooldown_minutes: int = 15  # Minimum time between same alerts


@dataclass
class BurnRateCalculation:
    """Burn rate calculation result"""
    slo: SLODefinition
    window: BurnRateWindow
    window_duration_hours: float
    error_rate: float
    burn_rate: float  # How fast we're burning error budget
    budget_consumed_percent: float
    time_to_exhaustion_hours: Optional[float]
    alert_triggered: bool
    severity: Optional[AlertSeverity] = None


@dataclass
class BurnRateAlert:
    """Burn rate alert"""
    alert_id: str
    slo: SLODefinition
    burn_calculation: BurnRateCalculation
    severity: AlertSeverity
    timestamp: datetime
    message: str
    details: dict[str, Any] = field(default_factory=dict)
    acknowledged: bool = False
    resolved: bool = False


class BurnRateCalculator:
    """
    SLO burn rate calculator with multi-window detection.

    Implements Google SRE burn rate alerting methodology with:
    - Fast burn detection (1h, 6h windows)
    - Slow burn detection (24h, 7d windows)
    - Alert fatigue reduction through proper thresholds
    """

    def __init__(self):
        # Standard SRE burn rate thresholds
        self.burn_rate_thresholds = {
            # Fast burn alerts (high urgency)
            BurnRateWindow.FAST_1H: BurnRateThreshold(
                window=BurnRateWindow.FAST_1H,
                threshold=14.4,  # Burns 2% budget in 1h (exhausts in ~50h)
                alert_severity=AlertSeverity.CRITICAL,
                cooldown_minutes=5
            ),
            BurnRateWindow.FAST_6H: BurnRateThreshold(
                window=BurnRateWindow.FAST_6H,
                threshold=6.0,   # Burns 2% budget in 6h (exhausts in ~12.5d)
                alert_severity=AlertSeverity.CRITICAL,
                cooldown_minutes=15
            ),

            # Slow burn alerts (advance warning)
            BurnRateWindow.SLOW_24H: BurnRateThreshold(
                window=BurnRateWindow.SLOW_24H,
                threshold=3.0,   # Burns 5% budget in 24h (exhausts in ~20d)
                alert_severity=AlertSeverity.WARNING,
                cooldown_minutes=60
            ),
            BurnRateWindow.SLOW_7D: BurnRateThreshold(
                window=BurnRateWindow.SLOW_7D,
                threshold=1.0,   # Burns 10% budget in 7d (exhausts in ~70d)
                alert_severity=AlertSeverity.INFO,
                cooldown_minutes=240
            )
        }

    def calculate_burn_rate(self,
                          slo: SLODefinition,
                          window: BurnRateWindow,
                          error_count: int,
                          total_requests: int,
                          window_duration_minutes: int) -> BurnRateCalculation:
        """Calculate burn rate for given window"""

        # Calculate error rate
        error_rate = error_count / max(total_requests, 1)

        # Calculate allowed error rate for this SLO
        allowed_error_rate = 1.0 - slo.target

        # Calculate burn rate (how fast we're consuming error budget)
        burn_rate = error_rate / allowed_error_rate if allowed_error_rate > 0 else 0

        # Convert window duration to hours
        window_duration_hours = window_duration_minutes / 60.0

        # Calculate budget consumed in this window
        slo_period_hours = self._parse_time_period_to_hours(slo.time_period)
        budget_consumed_percent = (error_rate * window_duration_hours / slo_period_hours) * 100

        # Calculate time to exhaustion
        time_to_exhaustion_hours = None
        if burn_rate > 0:
            remaining_budget = slo.error_budget_percent - budget_consumed_percent
            if remaining_budget > 0:
                time_to_exhaustion_hours = remaining_budget / (burn_rate * allowed_error_rate * 100)

        # Check if alert should be triggered
        threshold_config = self.burn_rate_thresholds.get(window)
        alert_triggered = False
        severity = None

        if threshold_config and burn_rate >= threshold_config.threshold:
            alert_triggered = True
            severity = threshold_config.alert_severity

        return BurnRateCalculation(
            slo=slo,
            window=window,
            window_duration_hours=window_duration_hours,
            error_rate=error_rate,
            burn_rate=burn_rate,
            budget_consumed_percent=budget_consumed_percent,
            time_to_exhaustion_hours=time_to_exhaustion_hours,
            alert_triggered=alert_triggered,
            severity=severity
        )

    def _parse_time_period_to_hours(self, time_period: str) -> float:
        """Parse time period string to hours"""
        if time_period.endswith('d'):
            return float(time_period[:-1]) * 24
        elif time_period.endswith('h'):
            return float(time_period[:-1])
        elif time_period.endswith('m'):
            return float(time_period[:-1]) / 60
        else:
            # Default to 30 days
            return 30 * 24


class BurnRateAlertManager:
    """
    Burn rate alert manager with intelligent alerting,
    deduplication, and escalation procedures.
    """

    def __init__(self):
        self.slos: dict[str, SLODefinition] = {}
        self.calculator = BurnRateCalculator()
        self.active_alerts: dict[str, BurnRateAlert] = {}
        self.alert_history: deque = deque(maxlen=1000)
        self.notification_handlers: dict[str, Callable] = {}
        self.last_alert_times: dict[str, datetime] = {}

        # Metrics storage (would integrate with actual metrics backend)
        self.metrics_data: dict[str, deque] = {}

        logger.info("BurnRateAlertManager initialized")

    def register_slo(self, slo: SLODefinition):
        """Register an SLO for monitoring"""
        slo.error_budget_percent = (1.0 - slo.target) * 100
        self.slos[slo.slo_name] = slo
        logger.info(f"Registered SLO: {slo.slo_name} (target: {slo.target:.3f})")

    def register_notification_handler(self, channel: str, handler: Callable):
        """Register notification handler for alerts"""
        self.notification_handlers[channel] = handler
        logger.info(f"Registered notification handler: {channel}")

    async def evaluate_burn_rates(self,
                                metrics: dict[str, dict[str, Any]]) -> list[BurnRateCalculation]:
        """Evaluate burn rates for all registered SLOs"""
        calculations = []

        for slo_name, slo in self.slos.items():
            if slo_name not in metrics:
                continue

            slo_metrics = metrics[slo_name]

            # Evaluate each window
            for window in BurnRateWindow:
                window_minutes = self._window_to_minutes(window)

                # Get metrics for this window
                error_count = slo_metrics.get(f'errors_{window.value}', 0)
                total_requests = slo_metrics.get(f'requests_{window.value}', 0)

                if total_requests == 0:
                    continue

                # Calculate burn rate
                calculation = self.calculator.calculate_burn_rate(
                    slo, window, error_count, total_requests, window_minutes
                )

                calculations.append(calculation)

                # Generate alert if needed
                if calculation.alert_triggered:
                    await self._process_burn_rate_alert(calculation)

        return calculations

    def _window_to_minutes(self, window: BurnRateWindow) -> int:
        """Convert window enum to minutes"""
        window_mapping = {
            BurnRateWindow.FAST_1H: 60,
            BurnRateWindow.FAST_6H: 360,
            BurnRateWindow.SLOW_24H: 1440,
            BurnRateWindow.SLOW_7D: 10080
        }
        return window_mapping.get(window, 60)

    async def _process_burn_rate_alert(self, calculation: BurnRateCalculation):
        """Process burn rate alert with deduplication and notification"""

        # Create alert ID
        alert_id = f"{calculation.slo.slo_name}_{calculation.window.value}_{calculation.severity.value}"

        # Check cooldown period
        if self._is_in_cooldown(alert_id):
            return

        # Create alert
        alert = BurnRateAlert(
            alert_id=alert_id,
            slo=calculation.slo,
            burn_calculation=calculation,
            severity=calculation.severity,
            timestamp=datetime.now(timezone.utc),
            message=self._generate_alert_message(calculation),
            details={
                'burn_rate': calculation.burn_rate,
                'error_rate': calculation.error_rate,
                'budget_consumed_percent': calculation.budget_consumed_percent,
                'time_to_exhaustion_hours': calculation.time_to_exhaustion_hours,
                'window': calculation.window.value
            }
        )

        # Store alert
        self.active_alerts[alert_id] = alert
        self.alert_history.append(alert)
        self.last_alert_times[alert_id] = alert.timestamp

        # Send notifications
        await self._send_notifications(alert)

        logger.warning(f"Burn rate alert triggered: {alert.message}")

    def _is_in_cooldown(self, alert_id: str) -> bool:
        """Check if alert is in cooldown period"""
        if alert_id not in self.last_alert_times:
            return False

        last_alert = self.last_alert_times[alert_id]

        # Extract window from alert_id to get cooldown period
        for window, threshold in self.calculator.burn_rate_thresholds.items():
            if window.value in alert_id:
                cooldown_delta = timedelta(minutes=threshold.cooldown_minutes)
                return datetime.now(timezone.utc) - last_alert < cooldown_delta

        return False

    def _generate_alert_message(self, calculation: BurnRateCalculation) -> str:
        """Generate human-readable alert message"""
        slo = calculation.slo
        window = calculation.window.value
        burn_rate = calculation.burn_rate

        message = f"SLO burn rate alert for {slo.service}.{slo.slo_name}: "
        message += f"{burn_rate:.1f}x normal rate in {window} window"

        if calculation.time_to_exhaustion_hours:
            if calculation.time_to_exhaustion_hours < 24:
                message += f" (exhausts in {calculation.time_to_exhaustion_hours:.1f}h)"
            else:
                days = calculation.time_to_exhaustion_hours / 24
                message += f" (exhausts in {days:.1f}d)"

        return message

    async def _send_notifications(self, alert: BurnRateAlert):
        """Send notifications for alert"""
        window = alert.burn_calculation.window
        threshold_config = self.calculator.burn_rate_thresholds.get(window)

        if not threshold_config:
            return

        # Send to configured channels
        for channel in threshold_config.notification_channels:
            if channel in self.notification_handlers:
                try:
                    await self.notification_handlers[channel](alert)
                except Exception as e:
                    logger.error(f"Notification failed for channel {channel}: {e}")

    def get_active_alerts(self) -> list[BurnRateAlert]:
        """Get all active alerts"""
        return list(self.active_alerts.values())

    def acknowledge_alert(self, alert_id: str) -> bool:
        """Acknowledge an alert"""
        if alert_id in self.active_alerts:
            self.active_alerts[alert_id].acknowledged = True
            logger.info(f"Alert acknowledged: {alert_id}")
            return True
        return False

    def resolve_alert(self, alert_id: str) -> bool:
        """Resolve an alert"""
        if alert_id in self.active_alerts:
            alert = self.active_alerts[alert_id]
            alert.resolved = True
            del self.active_alerts[alert_id]
            logger.info(f"Alert resolved: {alert_id}")
            return True
        return False

    def get_slo_status_dashboard(self) -> dict[str, Any]:
        """Get SLO status dashboard data"""
        dashboard = {
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'slos': {},
            'active_alerts': len(self.active_alerts),
            'alert_summary': {
                'critical': 0,
                'warning': 0,
                'info': 0
            }
        }

        # Count alerts by severity
        for alert in self.active_alerts.values():
            dashboard['alert_summary'][alert.severity.value] += 1

        # SLO status
        for slo_name, slo in self.slos.items():
            dashboard['slos'][slo_name] = {
                'target': slo.target,
                'error_budget_percent': slo.error_budget_percent,
                'service': slo.service,
                'alerts': [
                    alert.alert_id for alert in self.active_alerts.values()
                    if alert.slo.slo_name == slo_name
                ]
            }

        return dashboard


class CIPipelineIntegration:
    """
    CI/CD pipeline integration for SLO burn rate monitoring.

    Provides gates and checks for deployment safety based on
    current SLO burn rates.
    """

    def __init__(self, alert_manager: BurnRateAlertManager):
        self.alert_manager = alert_manager

    def check_deployment_safety(self, service: str) -> tuple[bool, str, dict[str, Any]]:
        """
        Check if it's safe to deploy based on current burn rates.

        Returns:
            (is_safe, reason, details)
        """
        active_alerts = self.alert_manager.get_active_alerts()

        # Filter alerts for this service
        service_alerts = [
            alert for alert in active_alerts
            if alert.slo.service == service
        ]

        # Check for critical alerts
        critical_alerts = [
            alert for alert in service_alerts
            if alert.severity in [AlertSeverity.CRITICAL, AlertSeverity.EMERGENCY]
        ]

        if critical_alerts:
            return False, f"Critical SLO burn rate alerts active: {len(critical_alerts)}", {
                'critical_alerts': [alert.alert_id for alert in critical_alerts],
                'recommendation': 'Wait for burn rate to stabilize before deploying'
            }

        # Check for fast burn rates even without alerts
        fast_burns = []
        for alert in service_alerts:
            if alert.burn_calculation.window in [BurnRateWindow.FAST_1H, BurnRateWindow.FAST_6H] and alert.burn_calculation.burn_rate > 2.0:
                fast_burns.append(alert)

        if fast_burns:
            return False, f"Fast burn rates detected: {len(fast_burns)}", {
                'fast_burns': [alert.alert_id for alert in fast_burns],
                'recommendation': 'Monitor burn rates closely, consider postponing deployment'
            }

        # Check overall error budget
        low_budget_slos = []
        for slo in self.alert_manager.slos.values():
            if slo.service == service:
                # This would normally check actual budget consumption
                # For now, we'll use a placeholder
                remaining_budget = 50.0  # Mock: 50% budget remaining

                if remaining_budget < 10.0:  # Less than 10% budget
                    low_budget_slos.append(slo.slo_name)

        if low_budget_slos:
            return False, f"Low error budget: {low_budget_slos}", {
                'low_budget_slos': low_budget_slos,
                'recommendation': 'Wait for error budget to replenish'
            }

        return True, "Deployment safety check passed", {
            'service_alerts': len(service_alerts),
            'recommendation': 'Safe to deploy'
        }

    def generate_deployment_report(self, service: str) -> dict[str, Any]:
        """Generate deployment safety report"""
        is_safe, reason, details = self.check_deployment_safety(service)

        return {
            'service': service,
            'timestamp': datetime.now(timezone.utc).isoformat(),
            'deployment_safe': is_safe,
            'safety_reason': reason,
            'details': details,
            'slo_dashboard': self.alert_manager.get_slo_status_dashboard()
        }


# Notification handlers
async def slack_burn_rate_notification(alert: BurnRateAlert):
    """Send burn rate alert to Slack"""
    message = {
        "text": f"ðŸ”¥ SLO Burn Rate Alert: {alert.slo.service}",
        "attachments": [{
            "color": "danger" if alert.severity == AlertSeverity.CRITICAL else "warning",
            "fields": [
                {"title": "SLO", "value": alert.slo.slo_name, "short": True},
                {"title": "Burn Rate", "value": f"{alert.burn_calculation.burn_rate:.1f}x", "short": True},
                {"title": "Window", "value": alert.burn_calculation.window.value, "short": True},
                {"title": "Budget Used", "value": f"{alert.burn_calculation.budget_consumed_percent:.1f}%", "short": True},
                {"title": "Message", "value": alert.message, "short": False}
            ]
        }]
    }

    # This would send to actual Slack webhook
    logger.info(f"Slack notification: {message}")


async def pagerduty_burn_rate_notification(alert: BurnRateAlert):
    """Send burn rate alert to PagerDuty"""
    if alert.severity in [AlertSeverity.CRITICAL, AlertSeverity.EMERGENCY]:
        incident = {
            "summary": alert.message,
            "severity": alert.severity.value,
            "source": f"{alert.slo.service}.{alert.slo.slo_name}",
            "details": alert.details
        }

        # This would create actual PagerDuty incident
        logger.warning(f"PagerDuty incident: {incident}")


def create_lukhas_slos() -> list[SLODefinition]:
    """Create standard LUKHAS SLOs"""
    return [
        SLODefinition(
            service="memory",
            slo_name="search_availability",
            target=0.999,  # 99.9%
            time_period="30d",
            description="Memory search operations availability"
        ),
        SLODefinition(
            service="memory",
            slo_name="search_latency",
            target=0.95,   # 95% under 50ms
            time_period="30d",
            description="Memory search p95 latency < 50ms"
        ),
        SLODefinition(
            service="identity",
            slo_name="auth_availability",
            target=0.9999,  # 99.99%
            time_period="30d",
            description="Identity authentication availability"
        ),
        SLODefinition(
            service="governance",
            slo_name="consent_availability",
            target=0.9999,  # 99.99%
            time_period="30d",
            description="Consent checking availability"
        )
    ]
