#!/usr/bin/env python3
"""
üîß T4 Lens Core LUKHAS Batch Processor
====================================

Processes exactly 10 core LUKHAS Python files per batch using existing toolchain.
Targets: lukhas/, identity/, api/ directories only.

T4 Lens Framework:
‚öõÔ∏è Scale & Automation (Sam Altman): Batch processing for systematic improvement
üß† Constitutional Safety (Dario Amodei): Conservative fixes with validation
üî¨ Scientific Rigor (Demis Hassabis): Evidence-based tracking and measurement
üé® Experience Discipline (Steve Jobs): User-focused, elegant implementation
"""

import hashlib
import json
import subprocess
from datetime import datetime, timezone
from pathlib import Path


class T4CoreBatchProcessor:
    def __init__(self, workspace_path="/Users/agi_dev/LOCAL-REPOS/Lukhas", timezone):
        self.workspace = Path(workspace_path)
        self.core_dirs = ["lukhas", "identity", "api"]
        self.batch_size = 10
        self.venv_python = self.workspace / ".venv_test" / "bin" / "python"

        # T4 Lens validation
        self.verification_dir = self.workspace / "verification_artifacts"
        self.verification_dir.mkdir(exist_ok=True)

    def discover_core_files(self):
        """üîç Discover Python files in core LUKHAS directories"""
        core_files = []

        for core_dir in self.core_dirs:
            dir_path = self.workspace / core_dir
            if dir_path.exists():
                for py_file in dir_path.rglob("*.py"):
                    if py_file.is_file():
                        core_files.append(str(py_file.relative_to(self.workspace)))

        print(f"üîç T4 LENS: Discovered {len(core_files)} core LUKHAS Python files")
        return core_files

    def analyze_file_issues(self, file_path):
        """üî¨ Analyze single file using available tools"""
        issues = []
        full_path = self.workspace / file_path

        if not full_path.exists():
            return issues

        try:
            # Check for common issues using Python AST
            with open(full_path, encoding="utf-8") as f:
                content = f.read()

            # Basic syntax check
            try:
                compile(content, str(full_path), "exec")
            except SyntaxError as e:
                issues.append(f"SYNTAX_ERROR: {e}")

            # Check for unused imports (simplified)
            import ast

            tree = ast.parse(content)
            imports = []
            for node in ast.walk(tree):
                if isinstance(node, (ast.Import, ast.ImportFrom)):
                    for alias in node.names:
                        imports.append(alias.name)

            # Simple heuristic for potentially unused imports
            if len(imports) > 10:
                issues.append("HIGH_IMPORT_COUNT: Consider reviewing import usage")

        except Exception as e:
            issues.append(f"ANALYSIS_ERROR: {e}")

        return issues

    def apply_safe_fixes(self, file_path):
        """üõ°Ô∏è Apply safe fixes using existing tools"""
        fixes_applied = []
        full_path = self.workspace / file_path

        if not full_path.exists():
            return fixes_applied

        # Create backup
        backup_path = full_path.with_suffix(full_path.suffix + ".backup")

        try:
            # Calculate original file hash
            with open(full_path, "rb") as f:
                original_hash = hashlib.sha256(f.read()).hexdigest()

            # Copy to backup
            subprocess.run(["cp", str(full_path), str(backup_path)], check=True)

            # Apply black formatting if available
            black_path = self.workspace / ".venv_test" / "bin" / "black"
            if black_path.exists():
                result = subprocess.run([str(black_path), "--quiet", str(full_path)], capture_output=True, text=True)

                if result.returncode == 0:
                    fixes_applied.append("BLACK_FORMATTING")

            # Check if file changed
            with open(full_path, "rb") as f:
                new_hash = hashlib.sha256(f.read()).hexdigest()

            if original_hash != new_hash:
                # Store verification artifact
                verification = {
                    "file": str(file_path),
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "original_hash": original_hash,
                    "new_hash": new_hash,
                    "fixes_applied": fixes_applied,
                    "t4_framework": "Constitutional Safety validation",
                }

                verification_file = (
                    self.verification_dir
                    / f"fix_{datetime.now(timezone.utc).strftime('%Y%m%d_%H%M%S')}_{file_path.replace('/', '_'}.json"
                )
                with open(verification_file, "w") as f:
                    json.dump(verification, f, indent=2)
            else:
                # Remove backup if no changes
                backup_path.unlink()

        except Exception as e:
            # Restore from backup on error
            if backup_path.exists():
                subprocess.run(["cp", str(backup_path), str(full_path)], check=True)
                backup_path.unlink()
            print(f"‚ùå Error fixing {file_path}: {e}")

        return fixes_applied

    def process_batch(self, batch_num=1):
        """üéØ Process exactly 10 core LUKHAS files"""
        print("üöÄ T4 LENS: Processing Core LUKHAS Batch ")
        print("=" * 50)

        # Discover core files
        core_files = self.discover_core_files()

        if not core_files:
            print("‚ùå No core LUKHAS Python files found")
            return

        # Take exactly 10 files for this batch
        start_idx = (batch_num - 1) * self.batch_size
        batch_files = core_files[start_idx : start_idx + self.batch_size]

        if not batch_files:
            print(f"‚úÖ All batches complete! Total files: {len(core_files)}")
            return

        print(f"üìÅ Batch {batch_num}: Processing {len(batch_files)} files")
        print(f"üìä Range: {start_idx + 1}-{start_idx + len(batch_files)} of {len(core_files)} total")
        print("")

        # Process each file
        total_issues = 0
        total_fixes = 0

        for i, file_path in enumerate(batch_files, 1):
            print(f"üìÑ {i:2d}/10: {file_path}")

            # Analyze issues
            issues = self.analyze_file_issues(file_path)
            if issues:
                print(f"    üîç Issues found: {len(issues)}")
                for issue in issues[:3]:  # Show first 3 issues
                    print(f"      ‚Ä¢ {issue}")
                total_issues += len(issues)

                # Apply safe fixes
                fixes = self.apply_safe_fixes(file_path)
                if fixes:
                    print(f"    ‚úÖ Fixes applied: {', '.join(fixes}")
                    total_fixes += len(fixes)
            else:
                print("    ‚úÖ Clean")

            print("")

        # Summary
        print("üéØ T4 LENS BATCH SUMMARY")
        print("-" * 25)
        print(f"Files processed: {len(batch_files)}")
        print(f"Issues found: {total_issues}")
        print(f"Fixes applied: {total_fixes}")
        print(f"Next batch would be: {batch_num + 1}")
        print("")
        print("üî¨ Scientific Rigor: All changes tracked in verification_artifacts/")
        print("üõ°Ô∏è Constitutional Safety: Conservative fixes only, backups created")

        return len(batch_files) == self.batch_size  # True if more batches available


def main():
    processor = T4CoreBatchProcessor()

    print("üîß T4 LENS: LUKHAS Core Code Quality Processor")
    print("=" * 50)
    print("‚öõÔ∏è Scale & Automation: Systematic batch processing")
    print("üß† Constitutional Safety: Conservative, validated fixes")
    print("üî¨ Scientific Rigor: Evidence-based improvement tracking")
    print("üé® Experience Discipline: Elegant, user-focused implementation")
    print("")

    # Process first batch
    has_more = processor.process_batch(1)

    if has_more:
        print("üí° To process next batch, run:")
        print("python t4_core_batch_processor.py --batch 2")


if __name__ == "__main__":
    main()