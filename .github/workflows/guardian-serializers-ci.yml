name: Guardian Serializers CI

on:
  push:
    branches: [ main, develop ]
    paths:
      - 'lukhas/governance/**'
      - 'tests/test_guardian_serializers.py'
      - 'governance/**'
      - '.github/workflows/guardian-serializers-ci.yml'
  pull_request:
    branches: [ main, develop ]
    paths:
      - 'lukhas/governance/**'
      - 'tests/test_guardian_serializers.py'
      - 'governance/**'
      - '.github/workflows/guardian-serializers-ci.yml'

env:
  PYTHON_VERSION: "3.11"
  POETRY_VERSION: "1.7.1"

jobs:
  preflight-validation:
    name: Preflight Validation
    runs-on: ubuntu-latest
    outputs:
      should_run: ${{ steps.changes.outputs.guardian }}

    steps:
    - name: Checkout code
      uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332  # v4

    - name: Check for Guardian changes
      uses: dorny/paths-filter@v2
      id: changes
      with:
        filters: |
          guardian:
            - 'lukhas/governance/**'
            - 'tests/test_guardian_serializers.py'
            - 'governance/guardian_schema.json'

    - name: Validate Guardian schema
      if: steps.changes.outputs.guardian == 'true'
      run: |
        python -c "
        import json
        with open('governance/guardian_schema.json') as f:
            schema = json.load(f)
        print(f'Guardian schema version: {schema.get(\"title\", \"Unknown\")}')
        assert '\$schema' in schema, 'Schema must have \$schema field'
        assert schema.get('type') == 'object', 'Root schema must be object type'
        print('✓ Guardian schema validation passed')
        "

  lint-and-format:
    name: Lint and Format Check
    runs-on: ubuntu-latest
    needs: preflight-validation
    if: needs.preflight-validation.outputs.should_run == 'true'

    steps:
    - name: Checkout code
      uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332  # v4

    - name: Set up Python
      uses: actions/setup-python@65d7f2d534ac1bc67fcd62888c5f4f3d2cb2b236  # v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install black isort flake8 mypy

    - name: Check code formatting
      run: |
        black --check --diff lukhas/governance/
        isort --check-only --diff lukhas/governance/

    - name: Lint code
      run: |
        flake8 lukhas/governance/ --max-line-length=120 --ignore=E203,W503

    - name: Type checking
      run: |
        mypy lukhas/governance/ --ignore-missing-imports --no-strict-optional

  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    needs: preflight-validation
    if: needs.preflight-validation.outputs.should_run == 'true'

    strategy:
      matrix:
        python-version: ["3.9", "3.10", "3.11", "3.12"]

    steps:
    - name: Checkout code
      uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332  # v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@65d7f2d534ac1bc67fcd62888c5f4f3d2cb2b236  # v4
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y build-essential

    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-cov pytest-asyncio pytest-benchmark
        pip install msgpack jsonschema
        pip install zstandard lz4
        pip install numpy  # For performance optimization

    - name: Run unit tests
      run: |
        cd tests
        python -m pytest test_guardian_serializers.py -v \
          --cov=../lukhas/governance \
          --cov-report=xml \
          --cov-report=term-missing \
          --cov-fail-under=80

    - name: Upload coverage to Codecov
      if: matrix.python-version == '3.11'
      uses: codecov/codecov-action@v3
      with:
        file: ./tests/coverage.xml
        flags: guardian-serializers
        name: guardian-serializers-coverage

  performance-tests:
    name: Performance Benchmarks
    runs-on: ubuntu-latest
    needs: preflight-validation
    if: needs.preflight-validation.outputs.should_run == 'true'

    steps:
    - name: Checkout code
      uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332  # v4

    - name: Set up Python
      uses: actions/setup-python@65d7f2d534ac1bc67fcd62888c5f4f3d2cb2b236  # v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest pytest-benchmark msgpack jsonschema
        pip install zstandard lz4 numpy

    - name: Run performance benchmarks
      run: |
        cd tests
        python -m pytest test_guardian_serializers.py::TestPerformanceBenchmarks -v \
          --benchmark-json=../artifacts/benchmark_results.json \
          --benchmark-min-rounds=10

    - name: Validate performance targets
      run: |
        python -c "
        import json
        with open('artifacts/benchmark_results.json') as f:
            results = json.load(f)

        # Check performance targets
        for benchmark in results['benchmarks']:
            name = benchmark['name']
            mean_time = benchmark['stats']['mean']

            if 'throughput' in name:
                ops_per_sec = 1.0 / mean_time
                assert ops_per_sec > 500, f'{name}: {ops_per_sec:.1f} ops/sec < 500 target'
                print(f'✓ {name}: {ops_per_sec:.1f} ops/sec')
            elif 'latency' in name:
                latency_ms = mean_time * 1000
                assert latency_ms < 100, f'{name}: {latency_ms:.2f}ms > 100ms target'
                print(f'✓ {name}: {latency_ms:.2f}ms')

        print('✓ All performance targets met')
        "

    - name: Upload benchmark results
      uses: actions/upload-artifact@a8a3f3ad30e3422c9c7b888a15615d19a852ae32  # v3
      with:
        name: benchmark-results
        path: artifacts/benchmark_results.json

  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    needs: preflight-validation
    if: needs.preflight-validation.outputs.should_run == 'true'

    steps:
    - name: Checkout code
      uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332  # v4

    - name: Set up Python
      uses: actions/setup-python@65d7f2d534ac1bc67fcd62888c5f4f3d2cb2b236  # v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest msgpack jsonschema zstandard lz4 numpy

    - name: Run integration tests
      run: |
        cd tests
        python -m pytest test_guardian_serializers.py::TestSystemIntegration -v

    - name: Test Guardian schema compatibility
      run: |
        python -c "
        from lukhas.governance.schema_migration import check_schema_compatibility
        from lukhas.governance.schema_migration import CompatibilityType

        # Test version compatibility
        compat = check_schema_compatibility('2.0.0', '2.1.0')
        assert compat in [CompatibilityType.FORWARD, CompatibilityType.FULL], f'Compatibility issue: {compat}'

        compat = check_schema_compatibility('1.0.0', '2.0.0')
        print(f'1.0.0 -> 2.0.0 compatibility: {compat}')

        print('✓ Schema compatibility checks passed')
        "

  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest
    needs: preflight-validation
    if: needs.preflight-validation.outputs.should_run == 'true'

    steps:
    - name: Checkout code
      uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332  # v4

    - name: Set up Python
      uses: actions/setup-python@65d7f2d534ac1bc67fcd62888c5f4f3d2cb2b236  # v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install security tools
      run: |
        python -m pip install --upgrade pip
        pip install bandit safety semgrep

    - name: Run Bandit security scan
      run: |
        bandit -r lukhas/governance/ -f json -o artifacts/bandit_results.json || true
        bandit -r lukhas/governance/ -ll

    - name: Run Safety check
      run: |
        pip install msgpack jsonschema zstandard lz4 numpy
        safety check --json --output artifacts/safety_results.json || true
        safety check

    - name: Run Semgrep scan
      run: |
        semgrep --config=auto lukhas/governance/ --json --output=artifacts/semgrep_results.json || true
        semgrep --config=auto lukhas/governance/

    - name: Upload security scan results
      uses: actions/upload-artifact@a8a3f3ad30e3422c9c7b888a15615d19a852ae32  # v3
      with:
        name: security-scan-results
        path: artifacts/

  schema-drift-detection:
    name: Schema Drift Detection
    runs-on: ubuntu-latest
    needs: preflight-validation
    if: needs.preflight-validation.outputs.should_run == 'true'

    steps:
    - name: Checkout code
      uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332  # v4

    - name: Set up Python
      uses: actions/setup-python@65d7f2d534ac1bc67fcd62888c5f4f3d2cb2b236  # v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install jsonschema msgpack

    - name: Check schema drift
      run: |
        python -c "
        import json
        import hashlib

        # Load current Guardian schema
        with open('governance/guardian_schema.json') as f:
            current_schema = json.load(f)

        # Calculate schema hash
        schema_str = json.dumps(current_schema, sort_keys=True)
        schema_hash = hashlib.sha256(schema_str.encode()).hexdigest()[:16]

        print(f'Current schema hash: {schema_hash}')
        print(f'Schema version: {current_schema.get(\"title\", \"Unknown\")}')

        # Validate schema structure
        required_fields = ['schema_version', 'decision', 'subject', 'context', 'metrics', 'enforcement', 'audit', 'integrity']
        schema_props = current_schema.get('properties', {})

        missing_fields = [field for field in required_fields if field not in schema_props]
        if missing_fields:
            raise ValueError(f'Missing required schema fields: {missing_fields}')

        print('✓ Schema drift detection passed')
        "

  memory-leak-detection:
    name: Memory Leak Detection
    runs-on: ubuntu-latest
    needs: preflight-validation
    if: needs.preflight-validation.outputs.should_run == 'true'

    steps:
    - name: Checkout code
      uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332  # v4

    - name: Set up Python
      uses: actions/setup-python@65d7f2d534ac1bc67fcd62888c5f4f3d2cb2b236  # v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install pytest memory-profiler msgpack jsonschema zstandard lz4 numpy

    - name: Run memory leak tests
      run: |
        python -c "
        import gc
        import tracemalloc
        from lukhas.governance.guardian_serializers import serialize_guardian

        # Test for memory leaks
        tracemalloc.start()

        # Create test data
        test_data = {
            'schema_version': '2.0.0',
            'decision': {'status': 'allow', 'policy': 'test', 'timestamp': '2023-01-01T00:00:00Z'},
            'subject': {
                'correlation_id': 'test-123',
                'actor': {'type': 'user', 'id': 'test'},
                'operation': {'name': 'test'}
            },
            'context': {
                'environment': {'region': 'test', 'runtime': 'test'},
                'features': {'enforcement_enabled': True}
            },
            'metrics': {'latency_ms': 10},
            'enforcement': {'mode': 'enforced'},
            'audit': {'event_id': 'test-123', 'timestamp': '2023-01-01T00:00:00Z'},
            'integrity': {'content_sha256': 'a' * 64}
        }

        # Baseline memory
        baseline = tracemalloc.take_snapshot()

        # Run operations
        for i in range(1000):
            result = serialize_guardian(test_data)
            if i % 100 == 0:
                gc.collect()

        # Check memory growth
        current = tracemalloc.take_snapshot()
        top_stats = current.compare_to(baseline, 'lineno')

        total_growth = sum(stat.size for stat in top_stats if stat.size > 0) / 1024 / 1024  # MB
        print(f'Memory growth: {total_growth:.2f}MB')

        # Should not grow more than 50MB
        assert total_growth < 50, f'Memory leak detected: {total_growth:.2f}MB growth'

        tracemalloc.stop()
        print('✓ Memory leak detection passed')
        "

  load-testing:
    name: Load Testing
    runs-on: ubuntu-latest
    needs: preflight-validation
    if: needs.preflight-validation.outputs.should_run == 'true'

    steps:
    - name: Checkout code
      uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332  # v4

    - name: Set up Python
      uses: actions/setup-python@65d7f2d534ac1bc67fcd62888c5f4f3d2cb2b236  # v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install msgpack jsonschema zstandard lz4 numpy

    - name: Run load tests
      timeout-minutes: 10
      run: |
        python -c "
        import time
        import concurrent.futures
        from lukhas.governance.guardian_serializers import serialize_guardian, validate_guardian

        # Test data
        test_data = {
            'schema_version': '2.0.0',
            'decision': {'status': 'allow', 'policy': 'test', 'timestamp': '2023-01-01T00:00:00Z'},
            'subject': {
                'correlation_id': 'load-test-123',
                'actor': {'type': 'user', 'id': 'test'},
                'operation': {'name': 'test'}
            },
            'context': {
                'environment': {'region': 'test', 'runtime': 'test'},
                'features': {'enforcement_enabled': True}
            },
            'metrics': {'latency_ms': 10},
            'enforcement': {'mode': 'enforced'},
            'audit': {'event_id': 'load-test-123', 'timestamp': '2023-01-01T00:00:00Z'},
            'integrity': {'content_sha256': 'b' * 64}
        }

        def run_operations(thread_id):
            operations = 0
            errors = 0
            start_time = time.time()

            while time.time() - start_time < 30:  # Run for 30 seconds
                try:
                    # Test serialization
                    serialize_result = serialize_guardian(test_data)
                    if not serialize_result.success:
                        errors += 1

                    # Test validation
                    validate_result = validate_guardian(test_data)
                    if not validate_result.success:
                        errors += 1

                    operations += 2
                except Exception:
                    errors += 1

            return operations, errors

        # Run load test with multiple threads
        print('Running load test...')
        start_time = time.time()

        with concurrent.futures.ThreadPoolExecutor(max_workers=4) as executor:
            futures = [executor.submit(run_operations, i) for i in range(4)]
            results = [future.result() for future in concurrent.futures.as_completed(futures)]

        end_time = time.time()
        test_duration = end_time - start_time

        total_operations = sum(ops for ops, _ in results)
        total_errors = sum(errors for _, errors in results)

        throughput = total_operations / test_duration
        error_rate = total_errors / total_operations if total_operations > 0 else 0

        print(f'Load test completed:')
        print(f'  Duration: {test_duration:.1f}s')
        print(f'  Total operations: {total_operations}')
        print(f'  Throughput: {throughput:.1f} ops/sec')
        print(f'  Error rate: {error_rate:.3f}')

        # Validate performance targets
        assert throughput > 1000, f'Throughput {throughput:.1f} < 1000 ops/sec target'
        assert error_rate < 0.001, f'Error rate {error_rate:.3f} > 0.1% target'

        print('✓ Load test passed')
        "

  constitutional-ai-compliance:
    name: Constitutional AI Compliance
    runs-on: ubuntu-latest
    needs: preflight-validation
    if: needs.preflight-validation.outputs.should_run == 'true'

    steps:
    - name: Checkout code
      uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332  # v4

    - name: Set up Python
      uses: actions/setup-python@65d7f2d534ac1bc67fcd62888c5f4f3d2cb2b236  # v4
      with:
        python-version: ${{ env.PYTHON_VERSION }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install msgpack jsonschema zstandard lz4 numpy

    - name: Test Constitutional AI compliance
      run: |
        python -c "
        from lukhas.governance.validation_framework import ValidationTier, validate_guardian_data

        # Test cases for Constitutional AI compliance
        test_cases = [
            {
                'name': 'Deny without reasoning',
                'data': {
                    'schema_version': '2.0.0',
                    'decision': {'status': 'deny', 'policy': 'test', 'timestamp': '2023-01-01T00:00:00Z'},
                    'subject': {
                        'correlation_id': 'const-test-1',
                        'actor': {'type': 'user', 'id': 'test'},
                        'operation': {'name': 'test'}
                    },
                    'context': {
                        'environment': {'region': 'test', 'runtime': 'test'},
                        'features': {'enforcement_enabled': True}
                    },
                    'metrics': {'latency_ms': 10},
                    'enforcement': {'mode': 'enforced'},
                    'audit': {'event_id': 'const-test-1', 'timestamp': '2023-01-01T00:00:00Z'},
                    'integrity': {'content_sha256': 'c' * 64}
                },
                'expect_warnings': True
            }
        ]

        for test_case in test_cases:
            print(f'Testing: {test_case[\"name\"]}')
            result = validate_guardian_data(
                test_case['data'],
                fail_fast=False,
                include_constitutional=True
            )

            if test_case['expect_warnings']:
                warnings = result.get_warnings()
                assert len(warnings) > 0, f'Expected warnings for {test_case[\"name\"]}'
                print(f'  ✓ Found {len(warnings)} constitutional warnings as expected')

            print(f'  Compliance score: {result.compliance_score:.2f}')

        print('✓ Constitutional AI compliance tests passed')
        "

  artifacts-and-reports:
    name: Generate Artifacts
    runs-on: ubuntu-latest
    needs: [unit-tests, performance-tests, integration-tests, security-scan]
    if: always() && needs.preflight-validation.outputs.should_run == 'true'

    steps:
    - name: Checkout code
      uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332  # v4

    - name: Download all artifacts
      uses: actions/download-artifact@9bc31d5ccc31df68ecc42ccf4149144866c47d8a  # v3
      with:
        path: collected-artifacts

    - name: Generate summary report
      run: |
        python -c "
        import json
        import os
        from datetime import datetime

        report = {
            'timestamp': datetime.now().isoformat(),
            'guardian_serializers_ci': {
                'status': 'completed',
                'components_tested': [
                    'Schema Registry',
                    'Serialization Engine',
                    'Validation Framework',
                    'Schema Migration',
                    'Performance Optimizer',
                    'Integration Layer',
                    'Observability'
                ],
                'test_categories': [
                    'Unit Tests',
                    'Performance Benchmarks',
                    'Integration Tests',
                    'Security Scans',
                    'Load Testing',
                    'Constitutional AI Compliance'
                ],
                'performance_validated': True,
                'security_scanned': True,
                'constitutional_compliance': True
            }
        }

        # Save report
        os.makedirs('artifacts', exist_ok=True)
        with open('artifacts/guardian_serializers_ci_report.json', 'w') as f:
            json.dump(report, f, indent=2)

        print('Guardian Serializers CI Report Generated:')
        print(json.dumps(report, indent=2))
        "

    - name: Upload final artifacts
      uses: actions/upload-artifact@a8a3f3ad30e3422c9c7b888a15615d19a852ae32  # v3
      with:
        name: guardian-serializers-ci-artifacts
        path: |
          artifacts/
          collected-artifacts/

  notify-completion:
    name: Notify Completion
    runs-on: ubuntu-latest
    needs: [artifacts-and-reports]
    if: always()

    steps:
    - name: Notify success
      if: needs.artifacts-and-reports.result == 'success'
      run: |
        echo "✅ Guardian Serializers CI completed successfully!"
        echo "All components validated:"
        echo "  • Schema Registry with versioning"
        echo "  • High-performance serialization engine"
        echo "  • Multi-tier validation framework"
        echo "  • Schema migration system"
        echo "  • Performance optimization"
        echo "  • System integration"
        echo "  • Observability and monitoring"

    - name: Notify failure
      if: needs.artifacts-and-reports.result != 'success'
      run: |
        echo "❌ Guardian Serializers CI failed!"
        echo "Check the job logs for details"
        exit 1