name: ðŸ” Identity & WebAuthn Test Suite
on:
  pull_request:
    types: [opened, synchronize, reopened]
    paths:
      - 'lukhas/identity/**'
      - 'tests/identity/**'
      - 'tests/orchestration/test_multi_ai_router.py'  # For integration tests
  push:
    branches: [main, develop]
    paths:
      - 'lukhas/identity/**'
      - 'tests/identity/**'

env:
  GUARDIAN_MODE: production
  LUKHAS_MODE: test
  PYTHONHASHSEED: 0
  PYTHONDONTWRITEBYTECODE: 1

jobs:
  # Core Identity System Tests
  identity-core-tests:
    name: ðŸ”‘ Identity Core Tests
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332  # v4

      - name: Set up Python
        uses: actions/setup-python@65d7f2d534ac1bc67fcd62888c5f4f3d2cb2b236  # v4
        with:
          python-version: '3.9'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Create artifacts directory
        run: mkdir -p artifacts test-results

      - name: Run Identity Core Tests
        run: |
          python3 -m pytest tests/identity/ \
            -v --tb=short \
            --junitxml=test-results/identity-junit.xml \
            --json-report --json-report-file=artifacts/identity-test-results.json \
            -x --strict-markers

      - name: Upload Test Results
        uses: actions/upload-artifact@a8a3f3ad30e3422c9c7b888a15615d19a852ae32  # v3
        if: always()
        with:
          name: identity-test-results
          path: |
            test-results/identity-junit.xml
            artifacts/identity-test-results.json

  # WebAuthn Production Tests
  webauthn-production-tests:
    name: ðŸ›¡ï¸ WebAuthn Production Tests
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332  # v4

      - name: Set up Python
        uses: actions/setup-python@65d7f2d534ac1bc67fcd62888c5f4f3d2cb2b236  # v4
        with:
          python-version: '3.9'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Create artifacts directory
        run: mkdir -p artifacts test-results

      - name: Run WebAuthn Production Performance Tests
        run: |
          python3 -m pytest tests/identity/test_webauthn_production.py \
            -v --tb=short \
            --junitxml=test-results/webauthn-junit.xml \
            --json-report --json-report-file=artifacts/webauthn-test-results.json

      - name: Validate WebAuthn Performance Artifacts
        run: |
          python3 -c "
          import json
          import os
          from pathlib import Path

          artifacts_dir = Path('artifacts')

          # Check for WebAuthn performance artifacts
          webauthn_files = list(artifacts_dir.glob('webauthn_*_latency_*.json'))

          if not webauthn_files:
              print('âš ï¸ No WebAuthn performance artifacts found')
              exit(1)

          print(f'âœ… Found {len(webauthn_files)} WebAuthn performance artifacts')

          for artifact_file in webauthn_files:
              with open(artifact_file) as f:
                  data = json.load(f)

              test_name = data.get('test', 'Unknown')
              p95_ms = data['metrics']['p95_ms']
              target_ms = data['metrics']['target_p95_ms']
              passed = data['metrics']['passed']

              status = 'âœ… PASS' if passed else 'âŒ FAIL'
              print(f'{test_name}: {p95_ms:.1f}ms (target: {target_ms}ms) - {status}')

              if not passed:
                  print(f'âŒ WebAuthn performance target missed: {p95_ms:.1f}ms > {target_ms}ms')
                  exit(1)

          print('âœ… All WebAuthn performance targets met')
          "

      - name: Upload WebAuthn Artifacts
        uses: actions/upload-artifact@a8a3f3ad30e3422c9c7b888a15615d19a852ae32  # v3
        if: always()
        with:
          name: webauthn-performance-artifacts
          path: |
            artifacts/webauthn_*_latency_*.json
            test-results/webauthn-junit.xml

  # Identity â†” Orchestration Integration Tests
  identity-orchestration-integration:
    name: ðŸ”„ Identityâ†”Orchestration Integration
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332  # v4

      - name: Set up Python
        uses: actions/setup-python@65d7f2d534ac1bc67fcd62888c5f4f3d2cb2b236  # v4
        with:
          python-version: '3.9'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Create artifacts directory
        run: mkdir -p artifacts test-results

      - name: Test Identity-Orchestration Integration
        run: |
          python3 -c "
          import asyncio
          import json
          import time
          from datetime import datetime

          async def test_integration():
              print('ðŸ”„ Testing Identity â†” Orchestration Integration')

              # Test 1: Identity system can authenticate orchestration requests
              from lukhas.identity.auth_service import AuthService
              from lukhas.identity.rate_limiting import get_rate_limiter, RateLimitType

              auth_service = AuthService()
              rate_limiter = get_rate_limiter()

              # Test token generation and validation for orchestration
              start_time = time.time()

              # Simulate orchestration service requesting authentication
              test_token = auth_service.create_service_token('orchestration-service', ['orchestration:read', 'orchestration:write'])

              # Validate token for orchestration request
              token_payload = await auth_service.verify_token(test_token)
              assert token_payload['sub'] == 'orchestration-service'
              assert 'orchestration:read' in token_payload.get('permissions', [])

              token_duration = (time.time() - start_time) * 1000

              # Test 2: Rate limiting applies to orchestration endpoints
              start_time = time.time()

              allowed, metadata = await rate_limiter.check_rate_limit(
                  'orchestration-client',
                  RateLimitType.API_GENERAL
              )
              assert allowed == True

              rate_limit_duration = (time.time() - start_time) * 1000

              # Test 3: Multi-AI router can handle identity-validated requests
              from lukhas.orchestration.multi_ai_router import MultiAIRouter

              router = MultiAIRouter()
              start_time = time.time()

              # Simulate authenticated orchestration request
              result = await router.route_request(
                  prompt='Test orchestration with identity validation',
                  user_context={'user_id': token_payload['sub'], 'permissions': token_payload.get('permissions', [])},
                  providers=['openai']  # Mock provider
              )

              orchestration_duration = (time.time() - start_time) * 1000

              # Generate integration test artifact
              integration_results = {
                  'test': 'identity_orchestration_integration',
                  'timestamp': datetime.utcnow().isoformat(),
                  'metrics': {
                      'token_validation_ms': token_duration,
                      'rate_limiting_ms': rate_limit_duration,
                      'orchestration_routing_ms': orchestration_duration,
                      'total_integration_ms': token_duration + rate_limit_duration + orchestration_duration,
                      'target_total_ms': 300,  # Target: <300ms end-to-end
                      'passed': (token_duration + rate_limit_duration + orchestration_duration) < 300
                  },
                  'components': {
                      'identity_auth': {'status': 'pass', 'latency_ms': token_duration},
                      'rate_limiting': {'status': 'pass', 'latency_ms': rate_limit_duration},
                      'orchestration': {'status': 'pass', 'latency_ms': orchestration_duration}
                  }
              }

              with open('artifacts/identity_orchestration_integration.json', 'w') as f:
                  json.dump(integration_results, f, indent=2)

              print(f'âœ… Integration test completed: {integration_results[\"metrics\"][\"total_integration_ms\"]:.1f}ms')

              if not integration_results['metrics']['passed']:
                  print(f'âŒ Integration performance target missed')
                  exit(1)

              return integration_results['metrics']['passed']

          success = asyncio.run(test_integration())
          if not success:
              exit(1)
          "

      - name: Upload Integration Test Results
        uses: actions/upload-artifact@a8a3f3ad30e3422c9c7b888a15615d19a852ae32  # v3
        if: always()
        with:
          name: identity-orchestration-integration-results
          path: artifacts/identity_orchestration_integration.json

  # Rate Limiting Stress Tests
  rate-limiting-stress:
    name: âš¡ Rate Limiting Stress Tests
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@692973e3d937129bcbf40652eb9f2f61becf3332  # v4

      - name: Set up Python
        uses: actions/setup-python@65d7f2d534ac1bc67fcd62888c5f4f3d2cb2b236  # v4
        with:
          python-version: '3.9'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Rate Limiting Stress Test
        run: |
          python3 -c "
          import asyncio
          import json
          import time
          from datetime import datetime
          from lukhas.identity.rate_limiting import get_rate_limiter, RateLimitType

          async def stress_test_rate_limiting():
              print('âš¡ Starting rate limiting stress test')

              rate_limiter = get_rate_limiter()
              results = {
                  'test': 'rate_limiting_stress',
                  'timestamp': datetime.utcnow().isoformat(),
                  'scenarios': {}
              }

              # Scenario 1: Normal load
              print('Testing normal load...')
              start_time = time.time()
              success_count = 0

              for i in range(50):
                  allowed, metadata = await rate_limiter.check_rate_limit(
                      f'normal-client-{i % 10}',  # 10 different clients
                      RateLimitType.API_GENERAL
                  )
                  if allowed:
                      success_count += 1

              normal_duration = time.time() - start_time
              results['scenarios']['normal_load'] = {
                  'requests': 50,
                  'success_count': success_count,
                  'duration_ms': normal_duration * 1000,
                  'avg_latency_ms': (normal_duration * 1000) / 50
              }

              # Scenario 2: Burst load (should trigger rate limiting)
              print('Testing burst load...')
              start_time = time.time()
              burst_success = 0
              burst_rejected = 0

              for i in range(100):
                  allowed, metadata = await rate_limiter.check_rate_limit(
                      'burst-client',  # Single client
                      RateLimitType.WEBAUTHN_REGISTRATION
                  )
                  if allowed:
                      burst_success += 1
                  else:
                      burst_rejected += 1

              burst_duration = time.time() - start_time
              results['scenarios']['burst_load'] = {
                  'requests': 100,
                  'success_count': burst_success,
                  'rejected_count': burst_rejected,
                  'duration_ms': burst_duration * 1000,
                  'avg_latency_ms': (burst_duration * 1000) / 100
              }

              # Scenario 3: Recovery after rate limiting
              print('Testing recovery after rate limiting...')
              await asyncio.sleep(2)  # Wait for rate limit window to reset

              allowed, metadata = await rate_limiter.check_rate_limit(
                  'burst-client',
                  RateLimitType.WEBAUTHN_REGISTRATION
              )

              results['scenarios']['recovery'] = {
                  'recovered': allowed,
                  'metadata': metadata if allowed else {'error': 'still_limited'}
              }

              # Overall assessment
              results['assessment'] = {
                  'normal_load_healthy': results['scenarios']['normal_load']['success_count'] > 45,
                  'burst_protection_working': results['scenarios']['burst_load']['rejected_count'] > 0,
                  'recovery_working': results['scenarios']['recovery']['recovered'],
                  'avg_latency_acceptable': results['scenarios']['normal_load']['avg_latency_ms'] < 10
              }

              all_passed = all(results['assessment'].values())
              results['overall_status'] = 'PASS' if all_passed else 'FAIL'

              with open('artifacts/rate_limiting_stress_test.json', 'w') as f:
                  json.dump(results, f, indent=2)

              print(f'Rate limiting stress test: {results[\"overall_status\"]}')

              if not all_passed:
                  print('âŒ Rate limiting stress test failed')
                  for check, passed in results['assessment'].items():
                      status = 'âœ…' if passed else 'âŒ'
                      print(f'{status} {check}: {passed}')
                  exit(1)
              else:
                  print('âœ… All rate limiting stress tests passed')

          asyncio.run(stress_test_rate_limiting())
          "

      - name: Upload Rate Limiting Results
        uses: actions/upload-artifact@a8a3f3ad30e3422c9c7b888a15615d19a852ae32  # v3
        if: always()
        with:
          name: rate-limiting-stress-results
          path: artifacts/rate_limiting_stress_test.json

  # Identity Test Suite Summary
  identity-suite-summary:
    name: ðŸ“‹ Identity Suite Summary
    runs-on: ubuntu-latest
    needs: [identity-core-tests, webauthn-production-tests, identity-orchestration-integration, rate-limiting-stress]
    if: always()
    steps:
      - name: Identity Suite Results Summary
        run: |
          echo "## ðŸ” Identity & WebAuthn Test Suite Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Test Component | Status | Duration |" >> $GITHUB_STEP_SUMMARY
          echo "|---------------|--------|----------|" >> $GITHUB_STEP_SUMMARY
          echo "| Identity Core Tests | ${{ needs.identity-core-tests.result == 'success' && 'âœ… PASSED' || 'âŒ FAILED' }} | - |" >> $GITHUB_STEP_SUMMARY
          echo "| WebAuthn Production Tests | ${{ needs.webauthn-production-tests.result == 'success' && 'âœ… PASSED' || 'âŒ FAILED' }} | - |" >> $GITHUB_STEP_SUMMARY
          echo "| Identityâ†”Orchestration Integration | ${{ needs.identity-orchestration-integration.result == 'success' && 'âœ… PASSED' || 'âŒ FAILED' }} | - |" >> $GITHUB_STEP_SUMMARY
          echo "| Rate Limiting Stress Tests | ${{ needs.rate-limiting-stress.result == 'success' && 'âœ… PASSED' || 'âŒ FAILED' }} | - |" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Check overall suite status
          if [[ "${{ needs.identity-core-tests.result }}" == "success" &&
                "${{ needs.webauthn-production-tests.result }}" == "success" &&
                "${{ needs.identity-orchestration-integration.result }}" == "success" &&
                "${{ needs.rate-limiting-stress.result }}" == "success" ]]; then
            echo "### âœ… **Identity Suite: ALL TESTS PASSED**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**Ready for Production Canary Deployment**" >> $GITHUB_STEP_SUMMARY
            echo "- WebAuthn performance targets met" >> $GITHUB_STEP_SUMMARY
            echo "- Identityâ†”Orchestration integration validated" >> $GITHUB_STEP_SUMMARY
            echo "- Rate limiting stress tested and operational" >> $GITHUB_STEP_SUMMARY
            echo "- Core identity functionality verified" >> $GITHUB_STEP_SUMMARY
          else
            echo "### âŒ **Identity Suite: TESTS FAILED**" >> $GITHUB_STEP_SUMMARY
            echo "" >> $GITHUB_STEP_SUMMARY
            echo "**Not ready for production - fixes required**" >> $GITHUB_STEP_SUMMARY
            exit 1
          fi