name: ‚ö° Performance Gates - T4/0.01%
on:
  pull_request:
    types: [opened, synchronize, reopened]
  push:
    branches: [main, develop]

env:
  GUARDIAN_MODE: production
  LUKHAS_MODE: test
  PYTHONHASHSEED: 0
  PYTHONDONTWRITEBYTECODE: 1

jobs:
  # Performance Artifact Generation
  performance-tests:
    name: üèÉ Performance Test Suite
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Create artifacts directory
        run: mkdir -p artifacts

      - name: Run WebAuthn Performance Tests
        run: |
          python3 -m pytest tests/identity/test_webauthn_production.py::TestWebAuthnProduction::test_webauthn_registration_latency -v --tb=short
          python3 -m pytest tests/identity/test_webauthn_production.py::TestWebAuthnProduction::test_webauthn_authentication_latency -v --tb=short

      - name: Run Orchestration Performance Tests
        run: |
          python3 -m pytest tests/orchestration/test_multi_ai_router.py::TestMultiAIRouterPerformance::test_routing_latency_t4 -v --tb=short

      - name: Run Memory System Performance Tests
        run: |
          python3 -m pytest tests/memory/ -v -k "performance"

      - name: Run Consciousness Performance Tests
        run: |
          python3 -m pytest tests/consciousness/test_c1_consciousness_components.py::TestAwarenessEngine::test_awareness_update_basic -v --tb=short

      - name: Upload Performance Artifacts
        uses: actions/upload-artifact@v3
        with:
          name: performance-artifacts
          path: artifacts/*.json

  # Performance Validation
  performance-validation:
    name: ‚úÖ Performance SLO Validation
    runs-on: ubuntu-latest
    needs: performance-tests
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Download Performance Artifacts
        uses: actions/download-artifact@v3
        with:
          name: performance-artifacts
          path: artifacts/

      - name: Validate Performance SLOs
        run: |
          python3 scripts/validate_performance.py
          echo "Performance validation exit code: $?"

      - name: Generate Performance Report
        run: |
          python3 -c "
          import json
          import os
          from pathlib import Path

          artifacts_dir = Path('artifacts')
          results = []

          for artifact_file in artifacts_dir.glob('*.json'):
              with open(artifact_file) as f:
                  data = json.load(f)
                  results.append(data)

          print('## ‚ö° T4/0.01% Performance Gates Report', file=open('performance_report.md', 'w'))
          print('', file=open('performance_report.md', 'a'))
          print('| Test | P95 Latency | Target | Status |', file=open('performance_report.md', 'a'))
          print('|------|-------------|--------|--------|', file=open('performance_report.md', 'a'))

          for result in results:
              test_name = result.get('test', 'Unknown')
              p95_ms = result['metrics']['p95_ms']
              target_ms = result['metrics']['target_p95_ms']
              passed = result['metrics']['passed']
              status = '‚úÖ PASS' if passed else '‚ùå FAIL'

              print(f'| {test_name} | {p95_ms:.1f}ms | {target_ms}ms | {status} |', file=open('performance_report.md', 'a'))

          print('', file=open('performance_report.md', 'a'))
          if all(r['metrics']['passed'] for r in results):
              print('‚úÖ **All performance targets met - T4/0.01% compliant**', file=open('performance_report.md', 'a'))
          else:
              print('‚ùå **Performance targets missed - optimization required**', file=open('performance_report.md', 'a'))
          "

      - name: Upload Performance Report
        uses: actions/upload-artifact@v3
        with:
          name: performance-report
          path: performance_report.md

  # Memory Performance Validation
  memory-performance:
    name: üß† Memory System Performance
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run Memory Validation Scripts
        run: |
          # Run M1 memory system validation
          PYTHONHASHSEED=0 LUKHAS_MODE=release PYTHONDONTWRITEBYTECODE=1 python3 scripts/validate_m1_memory.py

          # Run M1 optimized validation
          PYTHONHASHSEED=0 LUKHAS_MODE=release PYTHONDONTWRITEBYTECODE=1 python3 scripts/validate_m1_optimized.py

      - name: Memory System Stress Test
        run: |
          python3 -c "
          import time
          import psutil
          import os
          from lukhas.memory.fold_system import FoldSystem

          print('Starting memory stress test...')
          process = psutil.Process(os.getpid())
          initial_memory = process.memory_info().rss

          fold_system = FoldSystem()
          start_time = time.time()

          # Stress test: create many memory operations
          for i in range(100):
              fold_system.store(f'test-{i}', f'data-{i}' * 100)
              if i % 10 == 0:
                  current_memory = process.memory_info().rss
                  memory_mb = (current_memory - initial_memory) / 1024 / 1024
                  print(f'Iteration {i}: Memory usage: {memory_mb:.1f}MB')

          duration = time.time() - start_time
          final_memory = process.memory_info().rss
          memory_increase_mb = (final_memory - initial_memory) / 1024 / 1024

          print(f'Stress test completed in {duration:.2f}s')
          print(f'Memory increase: {memory_increase_mb:.1f}MB')

          # T4/0.01% memory efficiency requirement
          assert memory_increase_mb < 100, f'Memory usage too high: {memory_increase_mb:.1f}MB'
          assert duration < 5.0, f'Operation too slow: {duration:.2f}s'
          print('‚úÖ Memory performance within T4/0.01% targets')
          "

  # API Performance Benchmarks
  api-performance:
    name: üöÄ API Performance Benchmarks
    runs-on: ubuntu-latest
    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.9'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Start LUKHAS API
        run: |
          python lukhas/main.py &
          API_PID=$!
          echo "API_PID=$API_PID" >> $GITHUB_ENV
          sleep 10  # Wait for startup

      - name: API Load Test
        run: |
          python3 -c "
          import asyncio
          import aiohttp
          import time
          import json
          from statistics import median, quantiles

          async def api_load_test():
              async with aiohttp.ClientSession() as session:
                  latencies = []

                  print('Running API load test...')
                  for i in range(100):
                      start = time.time()
                      try:
                          async with session.get('http://localhost:8080/health') as resp:
                              await resp.json()
                          latencies.append((time.time() - start) * 1000)
                      except Exception as e:
                          print(f'Request {i} failed: {e}')

                  if latencies:
                      p95 = quantiles(latencies, n=20)[18]  # 95th percentile
                      median_latency = median(latencies)

                      print(f'API Load Test Results:')
                      print(f'Requests: {len(latencies)}')
                      print(f'Median latency: {median_latency:.1f}ms')
                      print(f'P95 latency: {p95:.1f}ms')

                      # T4/0.01% API performance requirement
                      assert p95 < 500, f'P95 latency too high: {p95:.1f}ms > 500ms'
                      print('‚úÖ API performance within T4/0.01% targets')

                      # Save results
                      with open('artifacts/api_load_test.json', 'w') as f:
                          json.dump({
                              'test': 'api_load_test',
                              'metrics': {
                                  'p95_ms': p95,
                                  'median_ms': median_latency,
                                  'target_p95_ms': 500,
                                  'passed': p95 < 500
                              }
                          }, f)

          asyncio.run(api_load_test())
          "

      - name: Stop API
        if: always()
        run: |
          if [ ! -z "$API_PID" ]; then
            kill $API_PID 2>/dev/null || true
          fi

  # Performance Gate Summary
  performance-gate-summary:
    name: üìä Performance Gate Summary
    runs-on: ubuntu-latest
    needs: [performance-tests, performance-validation, memory-performance, api-performance]
    if: always()
    steps:
      - name: Performance Gate Results
        run: |
          echo "## ‚ö° T4/0.01% Performance Gates Summary" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "| Performance Check | Status |" >> $GITHUB_STEP_SUMMARY
          echo "|------------------|--------|" >> $GITHUB_STEP_SUMMARY
          echo "| Performance Tests | ${{ needs.performance-tests.result == 'success' && '‚úÖ PASSED' || '‚ùå FAILED' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Performance Validation | ${{ needs.performance-validation.result == 'success' && '‚úÖ PASSED' || '‚ùå FAILED' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| Memory Performance | ${{ needs.memory-performance.result == 'success' && '‚úÖ PASSED' || '‚ùå FAILED' }} |" >> $GITHUB_STEP_SUMMARY
          echo "| API Performance | ${{ needs.api-performance.result == 'success' && '‚úÖ PASSED' || '‚ùå FAILED' }} |" >> $GITHUB_STEP_SUMMARY

          # Check if all gates passed
          if [[ "${{ needs.performance-tests.result }}" == "success" &&
                "${{ needs.performance-validation.result }}" == "success" &&
                "${{ needs.memory-performance.result }}" == "success" &&
                "${{ needs.api-performance.result }}" == "success" ]]; then
            echo ""
            echo "‚úÖ **All performance gates passed - T4/0.01% compliant**"
          else
            echo ""
            echo "‚ùå **Performance gates failed - optimization required**"
            exit 1
          fi