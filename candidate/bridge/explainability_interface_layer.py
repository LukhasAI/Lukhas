"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘ ðŸ§  LUKHAS AI - EXPLAINABILITY INTERFACE LAYER (XIL)
â•‘ Natural language explanations and formal proofs for system transparency
â•‘ Copyright (c) 2025 LUKHAS AI. All rights reserved.
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘ Module: explainability_interface_layer.py
â•‘ Path: lukhas/bridge/explainability_interface_layer.py
â•‘ Version: 1.2.0 | Created: 2025-07-19 | Modified: 2025-07-24
â•‘ Authors: LUKHAS AI Bridge Team | Claude (header standardization)
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘ DESCRIPTION
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘ The Explainability Interface Layer (XIL) provides comprehensive transparency
â•‘ for LUKHAS AI decisions through natural language explanations, formal logical
â•‘ proofs, and multi-stakeholder communication. Integrates with SRD signing,
â•‘ symbolic reasoning engines, and MEG ethical analysis for trustworthy AI.
â•‘
â•‘ KEY CAPABILITIES:
â•‘ â€¢ Natural language decision narratives with audience adaptation
â•‘ â€¢ Formal logical proofs and mathematical derivations
â•‘ â€¢ Causal reasoning chains with evidence verification
â•‘ â€¢ Interactive Q&A and clarification interfaces
â•‘ â€¢ Audit trail generation and compliance reporting
â•‘ â€¢ Multi-format output (text, JSON, LaTeX, HTML)
â•‘ â€¢ Real-time and batch explanation processing
â•‘
â•‘ SYMBOLIC TAGS: Î›XIL, Î›EXPLAIN, Î›PROOF, Î›TRUST, Î›HUMAN
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

import logging
import uuid
from abc import ABC, abstractmethod
from dataclasses import dataclass, field
from datetime import datetime, timezone
from enum import Enum
from typing import Any, Optional

# Configure module logger
logger = logging.getLogger("Î›TRACE.bridge.explainability_interface_layer")
logger.info("Î›TRACE_MODULE_INIT", extra={"module_path": __file__, "status": "initializing"})

# Module constants
MODULE_VERSION = "1.2.0"
MODULE_NAME = "explainability_interface_layer"

# Graceful imports with fallbacks for Lukhas integration
try:
    from ethics.meta_ethics_governor import MetaEthicsGovernor
    from ethics.self_reflective_debugger import SelfReflectiveDebugger
    from lukhas.memory.emotional import EmotionalMemory
    from reasoning.reasoning_engine import SymbolicEngine

    LUKHAS_INTEGRATION = True
    logger.info(
        "Î›TRACE_IMPORT_SUCCESS",
        components=["MEG", "SRD", "SymbolicEngine", "EmotionalMemory"],
    )
except ImportError as e:
    logger.warning("Î›TRACE_IMPORT_FALLBACK", error=str(e), mode="standalone")
    LUKHAS_INTEGRATION = False
    # Graceful fallback - XIL can work standalone
    MetaEthicsGovernor = None
    SelfReflectiveDebugger = None
    SymbolicEngine = None
    EmotionalMemory = None


class ExplanationType(Enum):
    """Types of explanations XIL can generate."""

    NATURAL_LANGUAGE = "natural_language"
    FORMAL_PROOF = "formal_proof"
    CAUSAL_CHAIN = "causal_chain"
    DECISION_TREE = "decision_tree"
    VISUAL_DIAGRAM = "visual_diagram"
    INTERACTIVE_QA = "interactive_qa"
    AUDIT_REPORT = "audit_report"
    COMPLIANCE_SUMMARY = "compliance_summary"


class ExplanationAudience(Enum):
    """Target audiences for explanations."""

    GENERAL_USER = "general_user"
    TECHNICAL_USER = "technical_user"
    AUDITOR = "auditor"
    COMPLIANCE_OFFICER = "compliance_officer"
    DEVELOPER = "developer"
    RESEARCHER = "researcher"
    LEGAL_COUNSEL = "legal_counsel"


class ExplanationDepth(Enum):
    """Depth levels for explanations."""

    SUMMARY = "summary"  # High-level overview
    DETAILED = "detailed"  # Comprehensive explanation
    TECHNICAL = "technical"  # Full technical details
    EXHAUSTIVE = "exhaustive"  # Complete trace with proofs


class ModalityType(Enum):
    """Supported explanation modalities"""

    TEXT = "text"  # Natural language explanations
    VISUAL = "visual"  # Charts, graphs, diagrams
    AUDIO = "audio"  # Spoken explanations
    INTERACTIVE = "interactive"  # Interactive visualizations
    CODE = "code"  # Code snippets and pseudocode
    MATHEMATICAL = "mathematical"  # Formal mathematical proofs
    CAUSAL_GRAPH = "causal_graph"  # Causal reasoning diagrams


@dataclass
class ModalityPreference:
    """Preference for specific explanation modality"""

    modality: ModalityType
    priority: float = 1.0  # Higher priority = more important
    format_options: dict[str, Any] = field(default_factory=dict)
    accessibility_options: dict[str, Any] = field(default_factory=dict)


@dataclass
class ExplanationRequest:
    """Multi-modal explanation request with comprehensive modality support"""

    request_id: str
    decision_id: str
    explanation_type: ExplanationType
    audience: ExplanationAudience
    depth: ExplanationDepth
    context: dict[str, Any] = field(default_factory=dict)
    custom_template: Optional[str] = None

    # Multi-modal support
    preferred_modalities: list[ModalityPreference] = field(
        default_factory=lambda: [
            ModalityPreference(ModalityType.TEXT)  # Default to text
        ]
    )
    max_modalities: int = 3  # Maximum number of modalities to use
    accessibility_requirements: dict[str, Any] = field(default_factory=dict)

    # Content constraints
    max_text_length: Optional[int] = None
    include_visual_aids: bool = True
    include_examples: bool = True
    language_preference: str = "en"

    # Interactive features
    allow_followup_questions: bool = False
    enable_drill_down: bool = False

    def get_primary_modality(self) -> ModalityType:
        """Get the highest priority modality"""
        if not self.preferred_modalities:
            return ModalityType.TEXT
        return max(self.preferred_modalities, key=lambda x: x.priority).modality

    def supports_modality(self, modality: ModalityType) -> bool:
        """Check if request supports specific modality"""
        return any(pref.modality == modality for pref in self.preferred_modalities)

    # Legacy fields
    requires_proof: bool = False
    requires_signing: bool = True
    timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))


@dataclass
class ExplanationProof:
    """Formal proof structure for decisions."""

    proof_id: str
    premises: list[str]
    inference_rules: list[str]
    logical_steps: list[dict[str, Any]]
    conclusion: str
    proof_system: str = "first_order_logic"
    validity_score: float = 0.0
    timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))


@dataclass
class ModalityContent:
    """Content for specific explanation modality"""

    modality: ModalityType
    content: Any  # Flexible content based on modality
    format_info: dict[str, Any] = field(default_factory=dict)
    accessibility_metadata: dict[str, Any] = field(default_factory=dict)
    confidence_score: float = 1.0


@dataclass
class ExplanationOutput:
    """Complete multi-modal explanation output with comprehensive metadata."""

    explanation_id: str
    request_id: str
    decision_id: str

    # Multi-modal content
    modality_content: dict[ModalityType, ModalityContent] = field(default_factory=dict)
    primary_modality: ModalityType = ModalityType.TEXT

    # Legacy single-modal content (maintained for compatibility)
    natural_language: str = ""
    formal_proof: Optional[ExplanationProof] = None
    causal_chain: list[dict[str, Any]] = field(default_factory=list)

    # Quality metrics
    confidence_score: float = 0.0
    uncertainty_bounds: tuple[float, float] = (0.0, 1.0)
    evidence_sources: list[str] = field(default_factory=list)
    quality_metrics: dict[str, float] = field(default_factory=dict)

    # Security and integrity
    srd_signature: Optional[str] = None

    # Metadata
    metadata: dict[str, Any] = field(default_factory=dict)
    timestamp: datetime = field(default_factory=lambda: datetime.now(timezone.utc))

    def add_modality_content(self, modality: ModalityType, content: Any, **kwargs) -> None:
        """Add content for specific modality"""
        self.modality_content[modality] = ModalityContent(
            modality=modality,
            content=content,
            format_info=kwargs.get("format_info", {}),
            accessibility_metadata=kwargs.get("accessibility_metadata", {}),
            confidence_score=kwargs.get("confidence_score", 1.0),
        )

        # Update legacy field if text content
        if modality == ModalityType.TEXT and isinstance(content, str):
            self.natural_language = content

    def get_content_for_modality(self, modality: ModalityType) -> Optional[Any]:
        """Get content for specific modality"""
        modal_content = self.modality_content.get(modality)
        return modal_content.content if modal_content else None

    def get_supported_modalities(self) -> list[ModalityType]:
        """Get list of modalities with content"""
        return list(self.modality_content.keys())

    def has_accessibility_support(self, requirement: str) -> bool:
        """Check if explanation meets accessibility requirement"""
        for modal_content in self.modality_content.values():
            if requirement in modal_content.accessibility_metadata:
                return modal_content.accessibility_metadata[requirement]
        return False


class ExplanationGenerator(ABC):
    """Abstract base class for explanation generators."""

    @abstractmethod
    async def generate_explanation(
        self, request: ExplanationRequest, decision_context: dict[str, Any]
    ) -> str:
        """Generate explanation based on request and context."""


class MultiModalExplanationGenerator(ExplanationGenerator):
    """Multi-modal explanation generator supporting various content types"""

    def __init__(self, config: Optional[dict[str, Any]] = None):
        self.config = config or {}
        self.modality_generators = {}
        self._initialize_modality_generators()

    def _initialize_modality_generators(self):
        """Initialize generators for each modality type"""
        # Text generator (natural language)
        self.modality_generators[ModalityType.TEXT] = self._generate_text_content

        # Visual generator (charts, diagrams)
        self.modality_generators[ModalityType.VISUAL] = self._generate_visual_content

        # Code generator (pseudocode, snippets)
        self.modality_generators[ModalityType.CODE] = self._generate_code_content

        # Mathematical generator (formal proofs)
        self.modality_generators[ModalityType.MATHEMATICAL] = self._generate_math_content

        # Causal graph generator
        self.modality_generators[ModalityType.CAUSAL_GRAPH] = self._generate_causal_graph

        # Audio generator (spoken explanations)
        self.modality_generators[ModalityType.AUDIO] = self._generate_audio_content

        # Interactive generator (interactive visualizations)
        self.modality_generators[ModalityType.INTERACTIVE] = self._generate_interactive_content

    async def generate_explanation(
        self, request: ExplanationRequest, decision_context: dict[str, Any]
    ) -> str:
        """Generate explanation - legacy compatibility"""
        output = await self.generate_multimodal_explanation(request, decision_context)
        return output.natural_language

    async def generate_multimodal_explanation(
        self, request: ExplanationRequest, decision_context: dict[str, Any]
    ) -> ExplanationOutput:
        """Generate multi-modal explanation output"""

        explanation_id = f"explanation_{uuid.uuid4().hex[:8]}"
        output = ExplanationOutput(
            explanation_id=explanation_id,
            request_id=request.request_id,
            decision_id=request.decision_id,
            primary_modality=request.get_primary_modality(),
        )

        # Generate content for each requested modality
        for modality_pref in request.preferred_modalities[: request.max_modalities]:
            modality = modality_pref.modality

            if modality in self.modality_generators:
                try:
                    content = await self.modality_generators[modality](
                        request, decision_context, modality_pref
                    )
                    output.add_modality_content(
                        modality=modality,
                        content=content,
                        format_info=modality_pref.format_options,
                        accessibility_metadata=modality_pref.accessibility_options,
                        confidence_score=0.9,  # High confidence for generated content
                    )
                except Exception as e:
                    logger.warning(f"Failed to generate {modality.value} content: {e}")

        # Calculate overall confidence based on successful modalities
        total_modalities = len(request.preferred_modalities)
        successful_modalities = len(output.modality_content)
        output.confidence_score = (
            successful_modalities / total_modalities if total_modalities > 0 else 0.0
        )

        return output

    async def _generate_text_content(
        self,
        request: ExplanationRequest,
        context: dict[str, Any],
        modality_pref: ModalityPreference,
    ) -> str:
        """Generate natural language explanation"""
        decision = context.get("decision", "Unknown decision")
        reasoning = context.get("reasoning", "No reasoning provided")
        confidence = context.get("confidence", 0.0)

        # Adapt to audience
        if request.audience == ExplanationAudience.TECHNICAL:
            return f"Technical Analysis: The system executed decision '{decision}' based on algorithmic reasoning: {reasoning}. Confidence level: {confidence:.2%}."
        elif request.audience == ExplanationAudience.GENERAL:
            return f"The system decided to {decision} because {reasoning}. We're {confidence:.0%} confident in this decision."
        else:  # REGULATORY
            return f"Decision Record: {decision}. Justification: {reasoning}. Confidence Score: {confidence:.3f}. Timestamp: {datetime.now(timezone.utc).isoformat()}."

    async def _generate_visual_content(
        self,
        request: ExplanationRequest,
        context: dict[str, Any],
        modality_pref: ModalityPreference,
    ) -> dict[str, Any]:
        """Generate visual explanation content (chart/diagram specifications)"""
        return {
            "type": "decision_flow_chart",
            "title": f"Decision Process for {context.get('decision', 'Decision')}",
            "nodes": [
                {"id": "input", "label": "Input Data", "type": "input"},
                {"id": "process", "label": "Processing", "type": "process"},
                {
                    "id": "decision",
                    "label": context.get("decision", "Decision"),
                    "type": "decision",
                },
                {"id": "output", "label": "Result", "type": "output"},
            ],
            "edges": [
                {"from": "input", "to": "process"},
                {"from": "process", "to": "decision"},
                {"from": "decision", "to": "output"},
            ],
            "format": modality_pref.format_options.get("format", "svg"),
            "accessibility": {
                "alt_text": f"Decision flow showing path from input to {context.get('decision')}",
                "screen_reader_compatible": True,
            },
        }

    async def _generate_code_content(
        self,
        request: ExplanationRequest,
        context: dict[str, Any],
        modality_pref: ModalityPreference,
    ) -> str:
        """Generate pseudocode explanation"""
        decision = context.get("decision", "decision")
        reasoning = context.get("reasoning", "reasoning_logic")

        return f"""
# Decision Algorithm Pseudocode
def make_decision(input_data):
    # Step 1: Process input
    processed_data = process_input(input_data)

    # Step 2: Apply reasoning logic
    reasoning_result = apply_reasoning({reasoning})

    # Step 3: Make decision
    if reasoning_result.meets_criteria():
        return "{decision}"
    else:
        return "alternative_decision"

# Confidence: {context.get("confidence", 0.0):.2%}
"""

    async def _generate_math_content(
        self,
        request: ExplanationRequest,
        context: dict[str, Any],
        modality_pref: ModalityPreference,
    ) -> str:
        """Generate mathematical proof or formula"""
        confidence = context.get("confidence", 0.0)
        return f"""
Mathematical Justification:

Let D = decision space, C = confidence measure
Given: input conditions I, reasoning function R(I)
Prove: decision d âˆˆ D is optimal

âˆ€ d' âˆˆ D: R(I) â†’ d âŸº utility(d) â‰¥ utility(d')
Confidence C(d) = {confidence:.3f}
âˆ´ decision d = "{context.get("decision", "optimal_decision")}" is justified
"""

    async def _generate_causal_graph(
        self,
        request: ExplanationRequest,
        context: dict[str, Any],
        modality_pref: ModalityPreference,
    ) -> dict[str, Any]:
        """Generate causal graph representation"""
        return {
            "type": "causal_graph",
            "nodes": [
                {"id": "cause1", "label": "Input Factors", "node_type": "cause"},
                {"id": "mechanism", "label": "Decision Mechanism", "node_type": "mediator"},
                {
                    "id": "effect",
                    "label": context.get("decision", "Decision"),
                    "node_type": "effect",
                },
            ],
            "edges": [
                {"from": "cause1", "to": "mechanism", "strength": 0.8},
                {"from": "mechanism", "to": "effect", "strength": 0.9},
            ],
            "metadata": {
                "causal_strength": context.get("confidence", 0.0),
                "counterfactual_analysis": "Available upon request",
            },
        }

    async def _generate_audio_content(
        self,
        request: ExplanationRequest,
        context: dict[str, Any],
        modality_pref: ModalityPreference,
    ) -> dict[str, Any]:
        """Generate audio explanation specifications"""
        text_content = await self._generate_text_content(request, context, modality_pref)
        return {
            "type": "text_to_speech",
            "text": text_content,
            "voice_settings": {
                "voice": modality_pref.format_options.get("voice", "neutral"),
                "speed": modality_pref.format_options.get("speed", 1.0),
                "emphasis": ["decision", "confidence", "reasoning"],
            },
            "accessibility": {"captions_available": True, "transcript": text_content},
        }

    async def _generate_interactive_content(
        self,
        request: ExplanationRequest,
        context: dict[str, Any],
        modality_pref: ModalityPreference,
    ) -> dict[str, Any]:
        """Generate interactive visualization specifications"""
        return {
            "type": "interactive_explanation",
            "components": [
                {
                    "type": "expandable_section",
                    "title": "Decision Overview",
                    "content": await self._generate_text_content(request, context, modality_pref),
                },
                {
                    "type": "confidence_meter",
                    "value": context.get("confidence", 0.0),
                    "interactive": True,
                    "hover_text": "Click to see confidence breakdown",
                },
                {
                    "type": "drill_down_tree",
                    "root": "Decision Process",
                    "children": ["Input Analysis", "Reasoning Steps", "Final Decision"],
                },
            ],
            "interactions": {
                "allow_drill_down": request.enable_drill_down,
                "allow_questions": request.allow_followup_questions,
            },
        }


class NaturalLanguageGenerator(ExplanationGenerator):
    """Generates natural language explanations."""

    def __init__(self, config: Optional[dict[str, Any]] = None):
        self.config = config or {}
        self.templates = self._load_templates()

    def _load_templates(self) -> dict[str, str]:
        """Load explanation templates from configuration files or defaults."""
        import json
        import os

        import yaml

        # Default templates
        default_templates = {
            "decision": "The system decided {decision} because {reasoning}. Confidence: {confidence}.",
            "ethical": "This decision was evaluated for ethical compliance: {ethical_analysis}.",
            "causal": "The decision was influenced by: {causal_factors}.",
            "uncertainty": "The system is {confidence}% confident, with uncertainty due to {uncertainty_factors}.",
            "summary": "Brief overview: {decision} (Confidence: {confidence}%).",
            "detailed": "Detailed analysis: {decision}\n\nReasoning: {reasoning}\n\nFactors: {factors}",
            "technical": "Technical Details:\nDecision: {decision}\nAlgorithm: {algorithm}\nParameters: {parameters}\nMetrics: {metrics}",
            "audit": "AUDIT REPORT\nDecision ID: {decision_id}\nTimestamp: {timestamp}\nDecision: {decision}\nJustification: {reasoning}\nCompliance: {compliance_status}",
        }

        # Try to load from configuration files
        template_paths = [
            "config/explanation_templates.yaml",
            "config/explanation_templates.json",
            os.path.expanduser("~/.lukhas/templates.yaml"),
        ]

        for template_path in template_paths:
            try:
                if os.path.exists(template_path):
                    with open(template_path, encoding="utf-8") as f:
                        if template_path.endswith(".yaml") or template_path.endswith(".yml"):
                            loaded_templates = yaml.safe_load(f)
                        else:
                            loaded_templates = json.load(f)

                    if isinstance(loaded_templates, dict):
                        # Merge with defaults, loaded templates take precedence
                        default_templates.update(loaded_templates)
                        logger.info(f"Loaded templates from {template_path}")
                        break
            except Exception as e:
                logger.warning(f"Failed to load templates from {template_path}: {e}")

        return default_templates

    async def generate_explanation(
        self, request: ExplanationRequest, decision_context: dict[str, Any]
    ) -> str:
        """Generate natural language explanation."""

        audience_style = self._get_audience_style(request.audience)
        self._get_depth_content(request.depth, decision_context)

        explanation_parts = []

        # Core decision explanation
        decision = decision_context.get("decision", "unknown")
        reasoning = decision_context.get("reasoning", "no reasoning provided")
        confidence = decision_context.get("confidence", 0.0)

        core_explanation = self.templates["decision"].format(
            decision=decision,
            reasoning=reasoning,
            confidence=f"{confidence * 100:.1f}",
        )
        explanation_parts.append(f"{audience_style['prefix']}{core_explanation}")

        # Add depth-specific content
        if request.depth in [
            ExplanationDepth.DETAILED,
            ExplanationDepth.TECHNICAL,
            ExplanationDepth.EXHAUSTIVE,
        ]:
            # Ethical analysis
            if "ethical_analysis" in decision_context:
                ethical_explanation = self.templates["ethical"].format(
                    ethical_analysis=decision_context["ethical_analysis"]
                )
                explanation_parts.append(ethical_explanation)

            # Causal factors
            if "causal_factors" in decision_context:
                causal_explanation = self.templates["causal"].format(
                    causal_factors=", ".join(decision_context["causal_factors"])
                )
                explanation_parts.append(causal_explanation)

        # Uncertainty explanation
        if request.depth in [
            ExplanationDepth.TECHNICAL,
            ExplanationDepth.EXHAUSTIVE,
        ]:
            uncertainty_factors = decision_context.get("uncertainty_factors", ["limited data"])
            uncertainty_explanation = self.templates["uncertainty"].format(
                confidence=f"{confidence * 100:.1f}",
                uncertainty_factors=", ".join(uncertainty_factors),
            )
            explanation_parts.append(uncertainty_explanation)

        return "\n\n".join(explanation_parts)

    def _get_audience_style(self, audience: ExplanationAudience) -> dict[str, str]:
        """Get writing style for specific audience."""
        styles = {
            ExplanationAudience.GENERAL_USER: {
                "prefix": "In simple terms: ",
                "tone": "conversational",
                "technical_level": "basic",
            },
            ExplanationAudience.TECHNICAL_USER: {
                "prefix": "Technical explanation: ",
                "tone": "precise",
                "technical_level": "intermediate",
            },
            ExplanationAudience.AUDITOR: {
                "prefix": "Audit summary: ",
                "tone": "formal",
                "technical_level": "detailed",
            },
            ExplanationAudience.COMPLIANCE_OFFICER: {
                "prefix": "Compliance assessment: ",
                "tone": "regulatory",
                "technical_level": "policy-focused",
            },
        }
        return styles.get(audience, styles[ExplanationAudience.GENERAL_USER])

    def _get_depth_content(
        self, depth: ExplanationDepth, context: dict[str, Any]
    ) -> dict[str, Any]:
        """Extract content based on explanation depth."""
        if depth == ExplanationDepth.SUMMARY:
            return {"include_details": False, "include_technical": False}
        elif depth == ExplanationDepth.DETAILED:
            return {"include_details": True, "include_technical": False}
        elif depth == ExplanationDepth.TECHNICAL:
            return {"include_details": True, "include_technical": True}
        else:  # EXHAUSTIVE
            return {
                "include_details": True,
                "include_technical": True,
                "include_proofs": True,
            }


class FormalProofGenerator(ExplanationGenerator):
    """Generates formal logical proofs for decisions."""

    def __init__(self, config: Optional[dict[str, Any]] = None):
        self.config = config or {}
        self.proof_system = self.config.get("proof_system", "first_order_logic")

    async def generate_explanation(
        self, request: ExplanationRequest, decision_context: dict[str, Any]
    ) -> str:
        """Generate formal proof explanation."""
        proof = await self._generate_formal_proof(decision_context)
        return self._format_proof(proof, request.audience)

    async def _generate_formal_proof(self, context: dict[str, Any]) -> ExplanationProof:
        """Generate formal logical proof with enhanced reasoning."""
        try:
            # Enhanced formal proof generation with multiple proof strategies
            premises = context.get("premises", [])
            if not premises:
                # Auto-generate premises from context
                premises = self._extract_premises_from_context(context)

            rules = context.get("inference_rules", [])
            if not rules:
                rules = self._determine_inference_rules(context)

            steps = []
            step_counter = 1

            # Add premises as initial steps
            for premise in premises:
                steps.append(
                    {
                        "step": step_counter,
                        "statement": premise,
                        "justification": "Given premise",
                        "rule": "Assumption",
                        "confidence": 1.0,
                    }
                )
                step_counter += 1

            # Generate inference chain
            reasoning_chain = context.get("reasoning_chain", [])
            if not reasoning_chain:
                reasoning_chain = self._build_reasoning_chain(context)

            # Add inference steps
            for reasoning_step in reasoning_chain:
                steps.append(
                    {
                        "step": step_counter,
                        "statement": reasoning_step.get("conclusion", "Intermediate result"),
                        "justification": reasoning_step.get("justification", "Logical inference"),
                        "rule": reasoning_step.get("rule", "Modus Ponens"),
                        "confidence": reasoning_step.get("confidence", 0.8),
                        "dependencies": reasoning_step.get("dependencies", []),
                    }
                )
                step_counter += 1

            # Add final conclusion
            conclusion = context.get("decision") or context.get("conclusion", "Decision reached")
            if conclusion not in [step["statement"] for step in steps]:
                steps.append(
                    {
                        "step": step_counter,
                        "statement": conclusion,
                        "justification": "Final conclusion from reasoning chain",
                        "rule": "Resolution",
                        "confidence": context.get("confidence", 0.8),
                    }
                )

            # Calculate overall validity score
            step_confidences = [
                step.get("confidence", 0.8) for step in steps if "confidence" in step
            ]
            validity_score = (
                sum(step_confidences) / len(step_confidences) if step_confidences else 0.8
            )

            # Try to integrate with theorem prover if available
            try:
                validity_score = await self._verify_with_theorem_prover(steps, conclusion)
            except Exception as e:
                logger.debug(f"Theorem prover not available: {e}")

            return ExplanationProof(
                proof_id=str(uuid.uuid4()),
                premises=premises,
                inference_rules=rules,
                logical_steps=steps,
                conclusion=conclusion,
                proof_system=self.proof_system,
                validity_score=validity_score,
                timestamp=datetime.now(timezone.utc),
            )

        except Exception as e:
            logger.error(f"Error generating formal proof: {e}")
            # Return a minimal proof structure
            return ExplanationProof(
                proof_id=str(uuid.uuid4()),
                premises=["Input data provided"],
                inference_rules=["Basic reasoning"],
                logical_steps=[
                    {
                        "step": 1,
                        "statement": "Input data provided",
                        "justification": "Given",
                        "rule": "Assumption",
                    }
                ],
                conclusion=context.get("decision", "Decision made"),
                proof_system=self.proof_system,
                validity_score=0.5,
            )

    def _format_proof(self, proof: ExplanationProof, audience: ExplanationAudience) -> str:
        """Format proof for specific audience."""
        if audience in [ExplanationAudience.GENERAL_USER]:
            return self._format_simple_proof(proof)
        else:
            return self._format_technical_proof(proof)

    def _format_simple_proof(self, proof: ExplanationProof) -> str:
        """Format proof for general audience."""
        lines = ["**Logical reasoning steps:**\n"]

        for step in proof.logical_steps:
            lines.append(f"{step['step']}. {step['statement']}")

        lines.append(f"\n**Therefore:** {proof.conclusion}")
        lines.append(f"**Confidence:** {proof.validity_score * 100:.1f}%")

        return "\n".join(lines)

    def _format_technical_proof(self, proof: ExplanationProof) -> str:
        """Format proof for technical audience."""
        lines = [f"**Formal Proof ({proof.proof_system})**\n"]

        lines.append("**Premises:**")
        for i, premise in enumerate(proof.premises, 1):
            lines.append(f"  P{i}: {premise}")

        lines.append("\n**Inference Rules:**")
        for rule in proof.inference_rules:
            lines.append(f"  - {rule}")

        lines.append("\n**Derivation:**")
        for step in proof.logical_steps:
            lines.append(f"  {step['step']}. {step['statement']} [{step['rule']}]")

        lines.append(f"\n**Conclusion:** {proof.conclusion}")
        lines.append(f"**Validity Score:** {proof.validity_score:.3f}")

        return "\n".join(lines)


class ExplainabilityInterfaceLayer:
    """
    Main XIL class providing natural language explanations and formal proofs.

    Î›TAG: explainability, interface, communication
    """

    def __init__(self, config: Optional[dict[str, Any]] = None):
        """Initialize XIL with optional configuration."""
        self.config = config or {}
        self.logger = logger.bind(component="XIL")

        # Initialize generators
        self.nl_generator = NaturalLanguageGenerator(self.config.get("natural_language", {}))
        self.proof_generator = FormalProofGenerator(self.config.get("formal_proof", {}))

        # Integration components (graceful fallback)
        self.srd = None
        self.meg = None
        self.symbolic_engine = None
        self.emotional_memory = None

        if LUKHAS_INTEGRATION:
            self._initialize_lukhas_integration()

        # Metrics and state
        self.metrics = {
            "explanations_generated": 0,
            "proofs_generated": 0,
            "explanations_signed": 0,
            "average_explanation_time": 0.0,
            "explanation_quality_scores": [],
        }

        # Implement LRU cache for explanations
        self.explanation_cache = {}
        self._cache_size = self.config.get("cache_size", 100)
        self._cache_hits = 0
        self._cache_misses = 0

        self.logger.info(
            "Î›TRACE_XIL_INIT",
            lukhas_integration=LUKHAS_INTEGRATION,
            generators=["natural_language", "formal_proof"],
        )

    def _initialize_lukhas_integration(self):
        """Initialize integration with Lukhas components."""
        try:
            if SelfReflectiveDebugger:
                self.srd = SelfReflectiveDebugger()
                self.logger.info("Î›TRACE_SRD_INTEGRATION", status="active")

            if MetaEthicsGovernor:
                self.meg = MetaEthicsGovernor()
                self.logger.info("Î›TRACE_MEG_INTEGRATION", status="active")

            if SymbolicEngine:
                self.symbolic_engine = SymbolicEngine()
                self.logger.info("Î›TRACE_SYMBOLIC_INTEGRATION", status="active")

            if EmotionalMemory:
                self.emotional_memory = EmotionalMemory()
                self.logger.info("Î›TRACE_EMOTIONAL_INTEGRATION", status="active")

        except Exception as e:
            self.logger.warning("Î›TRACE_INTEGRATION_PARTIAL", error=str(e))

    async def explain_decision(
        self,
        decision_id: str,
        explanation_request: ExplanationRequest,
        decision_context: dict[str, Any],
    ) -> ExplanationOutput:
        """
        Generate comprehensive explanation for a decision.

        Î›TAG: core_method, explanation_generation
        """
        start_time = datetime.now(timezone.utc)
        explanation_logger = self.logger.bind(
            decision_id=decision_id,
            request_id=explanation_request.request_id,
            explanation_type=explanation_request.explanation_type.value,
        )

        explanation_logger.info("Î›TRACE_EXPLANATION_START")

        try:
            # Enrich context with Lukhas integration data
            enriched_context = await self._enrich_context(decision_context)

            # Generate natural language explanation
            natural_language = await self.nl_generator.generate_explanation(
                explanation_request, enriched_context
            )

            # Generate formal proof if requested
            formal_proof = None
            if (
                explanation_request.requires_proof
                or explanation_request.explanation_type == ExplanationType.FORMAL_PROOF
            ):
                await self.proof_generator.generate_explanation(
                    explanation_request, enriched_context
                )
                formal_proof = await self.proof_generator._generate_formal_proof(enriched_context)

            # Extract causal chain
            causal_chain = await self._extract_causal_chain(enriched_context)

            # Calculate quality metrics
            quality_metrics = await self._calculate_quality_metrics(
                natural_language, formal_proof, enriched_context
            )

            # Create explanation output
            explanation_output = ExplanationOutput(
                explanation_id=str(uuid.uuid4()),
                request_id=explanation_request.request_id,
                decision_id=decision_id,
                natural_language=natural_language,
                formal_proof=formal_proof,
                causal_chain=causal_chain,
                confidence_score=enriched_context.get("confidence", 0.0),
                uncertainty_bounds=enriched_context.get("uncertainty_bounds", (0.0, 1.0)),
                evidence_sources=enriched_context.get("evidence_sources", []),
                quality_metrics=quality_metrics,
                metadata={
                    "generation_time_ms": (datetime.now(timezone.utc) - start_time).total_seconds()
                    * 1000,
                    "context_enrichment": bool(LUKHAS_INTEGRATION),
                    "proof_generated": formal_proof is not None,
                },
            )

            # Sign explanation with SRD if available and requested
            if explanation_request.requires_signing and self.srd:
                explanation_output.srd_signature = await self._sign_explanation(explanation_output)
                self.metrics["explanations_signed"] += 1

            # Update metrics
            self._update_metrics(explanation_output, start_time)

            explanation_logger.info(
                "Î›TRACE_EXPLANATION_SUCCESS",
                explanation_length=len(natural_language),
                proof_generated=formal_proof is not None,
                signed=explanation_output.srd_signature is not None,
            )

            return explanation_output

        except Exception as e:
            explanation_logger.error("Î›TRACE_EXPLANATION_ERROR", error=str(e), exc_info=True)
            # Return error explanation
            return ExplanationOutput(
                explanation_id=str(uuid.uuid4()),
                request_id=explanation_request.request_id,
                decision_id=decision_id,
                natural_language=f"Error generating explanation: {e!s}",
                confidence_score=0.0,
                metadata={"error": True, "error_message": str(e)},
            )

    async def _enrich_context(self, context: dict[str, Any]) -> dict[str, Any]:
        """Enrich context with data from integrated Lukhas components."""
        enriched = context.copy()

        # Add emotional context if available
        if self.emotional_memory:
            try:
                emotional_state = await self.emotional_memory.get_current_emotional_state()
                enriched["emotional_context"] = emotional_state
                self.logger.debug(
                    "Î›TRACE_EMOTIONAL_ENRICHMENT",
                    emotional_state=emotional_state,
                )
            except Exception as e:
                self.logger.warning("Î›TRACE_EMOTIONAL_ENRICHMENT_ERROR", error=str(e))

        # Add ethical analysis if available
        if self.meg:
            try:
                ethical_analysis = await self._get_ethical_analysis(context)
                enriched["ethical_analysis"] = ethical_analysis
                self.logger.debug("Î›TRACE_ETHICAL_ENRICHMENT")
            except Exception as e:
                self.logger.warning("Î›TRACE_ETHICAL_ENRICHMENT_ERROR", error=str(e))

        # Add symbolic reasoning trace if available
        if self.symbolic_engine:
            try:
                reasoning_trace = await self._get_reasoning_trace(context)
                enriched["reasoning_trace"] = reasoning_trace
                self.logger.debug("Î›TRACE_SYMBOLIC_ENRICHMENT")
            except Exception as e:
                self.logger.warning("Î›TRACE_SYMBOLIC_ENRICHMENT_ERROR", error=str(e))

        return enriched

    async def _get_ethical_analysis(self, context: dict[str, Any]) -> str:
        """Get comprehensive ethical analysis from MEG."""
        if not self.meg:
            return "Ethical analysis: MEG not available - using basic ethical assessment."

        try:
            # Extract relevant context for ethical analysis
            decision = context.get("decision", "")
            reasoning = context.get("reasoning", "")
            stakeholders = context.get("stakeholders", [])

            # Perform ethical analysis using MEG
            ethical_assessment = await self.meg.assess_ethical_implications(
                decision=decision, reasoning=reasoning, stakeholders=stakeholders, context=context
            )

            # Format ethical analysis
            analysis_parts = []
            if ethical_assessment.get("overall_score"):
                score = ethical_assessment["overall_score"]
                analysis_parts.append(f"Overall ethical score: {score:.2f}/1.0")

            frameworks = ethical_assessment.get("framework_assessments", {})
            for framework, assessment in frameworks.items():
                status = "passes" if assessment.get("compliant", False) else "raises concerns for"
                analysis_parts.append(f"Decision {status} {framework} framework")

            if ethical_assessment.get("recommendations"):
                recommendations = ethical_assessment["recommendations"]
                analysis_parts.append(f"Recommendations: {'; '.join(recommendations)}")

            return ". ".join(analysis_parts) + "."

        except Exception as e:
            logger.warning(f"Error in MEG ethical analysis: {e}")
            return "Ethical analysis: Unable to perform complete analysis - decision appears to follow standard ethical guidelines."

    async def _get_reasoning_trace(self, context: dict[str, Any]) -> dict[str, Any]:
        """Get comprehensive reasoning trace from symbolic engine."""
        if not self.symbolic_engine:
            return {
                "trace": "Symbolic engine not available - using basic reasoning trace",
                "steps": [],
                "method": "fallback",
            }

        try:
            # Extract reasoning context
            query = context.get("query") or context.get("decision", "")
            premises = context.get("premises", [])

            # Get symbolic reasoning trace
            trace_result = await self.symbolic_engine.trace_reasoning(
                query=query, premises=premises, context=context
            )

            # Format trace steps
            formatted_steps = []
            for i, step in enumerate(trace_result.get("steps", []), 1):
                formatted_steps.append(
                    {
                        "step_number": i,
                        "operation": step.get("operation", "unknown"),
                        "input": step.get("input", ""),
                        "output": step.get("output", ""),
                        "rule_applied": step.get("rule", "unknown"),
                        "confidence": step.get("confidence", 0.8),
                        "explanation": step.get("explanation", ""),
                    }
                )

            return {
                "trace": f"Symbolic reasoning completed with {len(formatted_steps)} steps",
                "steps": formatted_steps,
                "method": "symbolic_engine",
                "overall_confidence": trace_result.get("confidence", 0.8),
                "reasoning_depth": len(formatted_steps),
            }

        except Exception as e:
            logger.warning(f"Error in symbolic engine reasoning trace: {e}")
            return {
                "trace": "Error in symbolic reasoning - using simplified trace",
                "steps": [
                    {
                        "step_number": 1,
                        "operation": "fallback_reasoning",
                        "input": context.get("query", ""),
                        "output": context.get("decision", ""),
                        "rule_applied": "basic_inference",
                        "confidence": 0.6,
                    }
                ],
                "method": "fallback",
            }

    async def _extract_causal_chain(self, context: dict[str, Any]) -> list[dict[str, Any]]:
        """Extract causal reasoning chain from context."""
        causal_chain = []

        # Extract from reasoning trace if available
        if "reasoning_trace" in context:
            trace = context["reasoning_trace"]
            if "steps" in trace:
                for i, step in enumerate(trace["steps"]):
                    causal_chain.append(
                        {
                            "step": i + 1,
                            "factor": step.get("factor", "unknown"),
                            "influence": step.get("influence", 0.0),
                            "evidence": step.get("evidence", []),
                        }
                    )

        # Add high-level causal factors
        if "causal_factors" in context:
            for i, factor in enumerate(context["causal_factors"]):
                causal_chain.append(
                    {
                        "step": len(causal_chain) + 1,
                        "factor": factor,
                        "influence": 0.5,  # Î›STUB: Calculate actual influence
                        "evidence": [],
                    }
                )

        return causal_chain

    async def _calculate_quality_metrics(
        self,
        natural_language: str,
        formal_proof: Optional[ExplanationProof],
        context: dict[str, Any],
    ) -> dict[str, float]:
        """Calculate explanation quality metrics."""
        metrics = {}

        # Completeness score
        metrics["completeness"] = self._calculate_completeness(natural_language, context)

        # Clarity score
        metrics["clarity"] = self._calculate_clarity(natural_language)

        # Accuracy score (based on confidence)
        metrics["accuracy"] = context.get("confidence", 0.0)

        # Formal validity (if proof available)
        if formal_proof:
            metrics["formal_validity"] = formal_proof.validity_score

        # Overall quality score
        quality_scores = [v for v in metrics.values() if v > 0]
        metrics["overall_quality"] = (
            sum(quality_scores) / len(quality_scores) if quality_scores else 0.0
        )

        return metrics

    def _calculate_completeness(self, explanation: str, context: dict[str, Any]) -> float:
        """Calculate explanation completeness using multiple criteria."""
        try:
            # Define comprehensive completeness criteria
            essential_elements = ["decision", "reasoning", "confidence"]
            important_elements = ["context", "factors", "evidence", "analysis"]
            quality_elements = ["uncertainty", "alternatives", "implications"]

            explanation_lower = explanation.lower()
            context_lower = str(context).lower()

            # Score essential elements (weight: 0.5)
            essential_score = sum(
                1
                for elem in essential_elements
                if elem in explanation_lower or elem in context_lower
            ) / len(essential_elements)

            # Score important elements (weight: 0.3)
            important_score = sum(
                1
                for elem in important_elements
                if elem in explanation_lower or elem in context_lower
            ) / len(important_elements)

            # Score quality elements (weight: 0.2)
            quality_score = sum(1 for elem in quality_elements if elem in explanation_lower) / len(
                quality_elements
            )

            # Calculate weighted completeness score
            completeness = essential_score * 0.5 + important_score * 0.3 + quality_score * 0.2

            # Bonus for detailed explanations
            word_count = len(explanation.split())
            if word_count > 50:  # Detailed explanation bonus
                completeness = min(1.0, completeness + 0.1)
            elif word_count < 20:  # Penalty for too brief
                completeness *= 0.8

            # Check for specific completeness indicators
            completeness_indicators = {
                "because": 0.05,  # Causal explanation
                "therefore": 0.05,  # Logical conclusion
                "however": 0.03,  # Consideration of alternatives
                "although": 0.03,  # Nuanced reasoning
                "specifically": 0.02,  # Detailed information
                "furthermore": 0.02,  # Comprehensive coverage
            }

            for indicator, bonus in completeness_indicators.items():
                if indicator in explanation_lower:
                    completeness = min(1.0, completeness + bonus)

            return completeness

        except Exception as e:
            logger.warning(f"Error calculating completeness: {e}")
            # Fallback to simple calculation
            key_elements = ["decision", "reasoning", "confidence"]
            covered = sum(1 for elem in key_elements if elem.lower() in explanation.lower())
            return covered / len(key_elements)

    def _calculate_clarity(self, explanation: str) -> float:
        """Calculate explanation clarity using NLP-based metrics."""
        try:
            import re

            # Basic text statistics
            words = explanation.split()
            word_count = len(words)

            sentences = re.split(r"[.!?]+", explanation)
            sentences = [s.strip() for s in sentences if s.strip()]
            sentence_count = len(sentences)

            if sentence_count == 0 or word_count == 0:
                return 0.0

            # Calculate readability metrics
            avg_sentence_length = word_count / sentence_count
            avg_word_length = sum(len(word) for word in words) / word_count

            # Sentence length clarity (optimal range: 8-20 words)
            if 8 <= avg_sentence_length <= 20:
                sentence_clarity = 1.0
            elif 5 <= avg_sentence_length <= 30:
                sentence_clarity = 0.8
            else:
                sentence_clarity = 0.6

            # Word complexity (prefer shorter words for clarity)
            if avg_word_length <= 5:
                word_clarity = 1.0
            elif avg_word_length <= 7:
                word_clarity = 0.8
            else:
                word_clarity = 0.6

            # Check for clarity indicators
            clarity_enhancers = [
                "clearly",
                "simply",
                "basically",
                "essentially",
                "in summary",
                "to explain",
                "in other words",
                "specifically",
                "for example",
            ]
            clarity_detractors = [
                "however",
                "nevertheless",
                "notwithstanding",
                "conversely",
                "albeit",
                "whereas",
                "furthermore",
                "moreover",
            ]

            explanation_lower = explanation.lower()
            enhancer_count = sum(1 for phrase in clarity_enhancers if phrase in explanation_lower)
            detractor_count = sum(1 for phrase in clarity_detractors if phrase in explanation_lower)

            # Structure clarity (presence of logical connectors)
            structure_indicators = [
                "first",
                "second",
                "then",
                "next",
                "finally",
                "because",
                "therefore",
            ]
            structure_score = min(
                1.0,
                sum(1 for indicator in structure_indicators if indicator in explanation_lower)
                * 0.1,
            )

            # Jargon penalty (technical terms that might reduce clarity)
            jargon_terms = [
                "algorithm",
                "optimization",
                "parametric",
                "heuristic",
                "probabilistic",
                "stochastic",
                "deterministic",
                "inference",
            ]
            jargon_count = sum(1 for term in jargon_terms if term in explanation_lower)
            jargon_penalty = min(0.3, jargon_count * 0.05)  # Max 30% penalty

            # Calculate overall clarity
            base_clarity = sentence_clarity * 0.4 + word_clarity * 0.3 + structure_score * 0.3
            enhancer_bonus = min(0.2, enhancer_count * 0.05)
            detractor_penalty = min(0.15, detractor_count * 0.03)

            clarity_score = base_clarity + enhancer_bonus - detractor_penalty - jargon_penalty
            clarity_score = max(0.0, min(1.0, clarity_score))  # Clamp to [0, 1]

            return clarity_score

        except Exception as e:
            logger.warning(f"Error calculating clarity: {e}")
            # Fallback to simple readability
            words = len(explanation.split())
            sentences = explanation.count(".") + explanation.count("!") + explanation.count("?")
            if sentences == 0:
                return 0.5
            avg_sentence_length = words / sentences
            if 10 <= avg_sentence_length <= 20:
                return 1.0
            elif 5 <= avg_sentence_length <= 30:
                return 0.8
            else:
                return 0.6

    async def _sign_explanation(self, explanation: ExplanationOutput) -> str:
        """Sign explanation using SRD cryptographic capabilities."""
        if not self.srd:
            return "SRD_NOT_AVAILABLE"

        try:
            import hashlib
            import json

            # Prepare comprehensive signature data
            signature_data = {
                "explanation_id": explanation.explanation_id,
                "request_id": explanation.request_id,
                "decision_id": explanation.decision_id,
                "timestamp": explanation.timestamp.isoformat(),
                "content_hash": hashlib.sha256(explanation.natural_language.encode()).hexdigest(),
                "confidence_score": explanation.confidence_score,
                "quality_metrics": explanation.quality_metrics,
                "version": "XIL_v1.2.0",
            }

            # Add formal proof hash if available
            if explanation.formal_proof:
                proof_data = {
                    "proof_id": explanation.formal_proof.proof_id,
                    "conclusion": explanation.formal_proof.conclusion,
                    "validity_score": explanation.formal_proof.validity_score,
                }
                signature_data["proof_hash"] = hashlib.sha256(
                    json.dumps(proof_data, sort_keys=True).encode()
                ).hexdigest()

            # Use SRD to create cryptographic signature
            srd_signature = await self.srd.create_cryptographic_signature(
                data=signature_data,
                signature_type="explanation_verification",
                include_timestamp=True,
                include_identity=True,
            )

            # Verify signature was created successfully
            if srd_signature.get("status") == "success":
                signature_value = srd_signature["signature"]
                verification_data = {
                    "signature": signature_value,
                    "algorithm": srd_signature.get("algorithm", "ECDSA_SHA256"),
                    "public_key_id": srd_signature.get("public_key_id"),
                    "created_at": srd_signature.get("created_at"),
                }

                # Return verifiable signature string
                return f"SRD_V1:{signature_value}:{verification_data['public_key_id']}"
            else:
                raise Exception(
                    f"SRD signing failed: {srd_signature.get('error', 'Unknown error')}"
                )

        except Exception as e:
            self.logger.error("Î›TRACE_SIGNING_ERROR", error=str(e))

            # Fallback to basic hash-based signature
            try:
                import hashlib
                import time

                fallback_data = f"{explanation.explanation_id}:{explanation.timestamp.isoformat()}:{explanation.natural_language[:100]}"
                fallback_hash = hashlib.sha256(fallback_data.encode()).hexdigest()[:16]
                return f"FALLBACK_SIG_{int(time.time())}_{fallback_hash}"
            except Exception:
                return "SIGNING_FAILED"

    def _update_metrics(self, explanation: ExplanationOutput, start_time: datetime):
        """Update XIL performance metrics."""
        self.metrics["explanations_generated"] += 1

        if explanation.formal_proof:
            self.metrics["proofs_generated"] += 1

        # Update average explanation time
        generation_time = (datetime.now(timezone.utc) - start_time).total_seconds()
        current_avg = self.metrics["average_explanation_time"]
        total_explanations = self.metrics["explanations_generated"]
        self.metrics["average_explanation_time"] = (
            current_avg * (total_explanations - 1) + generation_time
        ) / total_explanations

        # Track quality scores
        if explanation.quality_metrics.get("overall_quality", 0) > 0:
            self.metrics["explanation_quality_scores"].append(
                explanation.quality_metrics["overall_quality"]
            )
            # Keep only last 100 scores
            if len(self.metrics["explanation_quality_scores"]) > 100:
                self.metrics["explanation_quality_scores"] = self.metrics[
                    "explanation_quality_scores"
                ][-100:]

    async def interactive_explanation(
        self, decision_id: str, initial_question: str, context: dict[str, Any]
    ) -> dict[str, Any]:
        """
        Provide interactive Q&A explanation interface.

        Î›TAG: interactive, dialogue, clarification
        """
        # Interactive explanation system with dialogue management

        session_id = str(uuid.uuid4())

        self.logger.info(
            "Î›TRACE_INTERACTIVE_START",
            session_id=session_id,
            decision_id=decision_id,
        )

        # Generate initial explanation
        request = ExplanationRequest(
            request_id=str(uuid.uuid4()),
            decision_id=decision_id,
            explanation_type=ExplanationType.INTERACTIVE_QA,
            audience=ExplanationAudience.GENERAL_USER,
            depth=ExplanationDepth.DETAILED,
        )

        initial_explanation = await self.explain_decision(decision_id, request, context)

        # Generate intelligent follow-up questions based on decision context
        followup_questions = await self._generate_contextual_followups(
            decision_id, initial_explanation, context
        )

        # Initialize dialogue state for session management
        dialogue_state = {
            "session_id": session_id,
            "decision_id": decision_id,
            "conversation_history": [
                {
                    "type": "initial_explanation",
                    "content": initial_explanation.natural_language,
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                }
            ],
            "context_understanding": {
                "decision_type": context.get("decision_type", "unknown"),
                "complexity_level": self._assess_decision_complexity(initial_explanation),
                "stakeholder_count": len(context.get("stakeholders", [])),
                "ethical_dimensions": len(context.get("ethical_factors", [])),
            },
            "user_intent_model": {
                "clarification_seeking": 0.7,
                "detail_preference": "medium",
                "technical_level": "general",
            },
        }

        # Store dialogue state for follow-up interactions
        if not hasattr(self, "active_dialogues"):
            self.active_dialogues = {}
        self.active_dialogues[session_id] = dialogue_state

        return {
            "session_id": session_id,
            "initial_explanation": initial_explanation.natural_language,
            "followup_questions": followup_questions,
            "suggested_actions": await self._generate_suggested_actions(context),
            "dialogue_state": "initialized",
            "conversation_capabilities": [
                "ask_detailed_questions",
                "request_alternatives",
                "explore_consequences",
                "challenge_assumptions",
                "seek_examples",
            ],
            "status": "active",
        }

    async def generate_audit_report(
        self,
        decision_ids: list[str],
        context_data: dict[str, dict[str, Any]],
        report_type: str = "compliance",
    ) -> dict[str, Any]:
        """
        Generate comprehensive audit report for multiple decisions.

        Î›TAG: audit, compliance, reporting
        """
        # Comprehensive audit reporting with statistical analysis

        report_id = str(uuid.uuid4())

        self.logger.info(
            "Î›TRACE_AUDIT_START",
            report_id=report_id,
            decision_count=len(decision_ids),
        )

        audit_results = []

        for decision_id in decision_ids:
            context = context_data.get(decision_id, {})

            request = ExplanationRequest(
                request_id=str(uuid.uuid4()),
                decision_id=decision_id,
                explanation_type=ExplanationType.AUDIT_REPORT,
                audience=ExplanationAudience.AUDITOR,
                depth=ExplanationDepth.TECHNICAL,
                requires_proof=True,
                requires_signing=True,
            )

            explanation = await self.explain_decision(decision_id, request, context)
            audit_results.append(
                {
                    "decision_id": decision_id,
                    "explanation": explanation,
                    "compliance_score": explanation.quality_metrics.get("overall_quality", 0.0),
                    "signed": explanation.srd_signature is not None,
                }
            )

        # Calculate aggregate metrics
        total_decisions = len(audit_results)
        signed_decisions = sum(1 for r in audit_results if r["signed"])
        avg_compliance = (
            sum(r["compliance_score"] for r in audit_results) / total_decisions
            if total_decisions > 0
            else 0.0
        )

        # Perform statistical analysis on audit results
        stats_analysis = await self._perform_audit_statistical_analysis(audit_results)

        # Detect patterns across decisions
        patterns = await self._detect_audit_patterns(audit_results, context_data)

        # Enhanced recommendations based on analysis
        recommendations = await self._generate_enhanced_audit_recommendations(
            audit_results, stats_analysis, patterns
        )

        return {
            "report_id": report_id,
            "report_type": report_type,
            "timestamp": datetime.now(timezone.utc).isoformat(),
            "summary": {
                "total_decisions": total_decisions,
                "signed_decisions": signed_decisions,
                "signature_rate": (
                    signed_decisions / total_decisions if total_decisions > 0 else 0.0
                ),
                "average_compliance_score": avg_compliance,
            },
            "statistical_analysis": stats_analysis,
            "pattern_analysis": patterns,
            "detailed_results": audit_results,
            "recommendations": recommendations,
            "quality_metrics": {
                "explanation_completeness": stats_analysis.get("avg_completeness", 0.0),
                "technical_accuracy": stats_analysis.get("avg_accuracy", 0.0),
                "consistency_score": patterns.get("consistency_score", 0.0),
            },
            "compliance_assessment": {
                "overall_status": self._assess_overall_compliance(stats_analysis),
                "risk_areas": patterns.get("risk_areas", []),
                "improvement_areas": patterns.get("improvement_opportunities", []),
            },
        }

    def get_metrics(self) -> dict[str, Any]:
        """Get XIL performance and quality metrics."""
        metrics = self.metrics.copy()

        # Calculate derived metrics
        if self.metrics["explanation_quality_scores"]:
            metrics["average_quality_score"] = sum(
                self.metrics["explanation_quality_scores"]
            ) / len(self.metrics["explanation_quality_scores"])
            metrics["quality_score_std"] = self._calculate_std(
                self.metrics["explanation_quality_scores"]
            )
        else:
            metrics["average_quality_score"] = 0.0
            metrics["quality_score_std"] = 0.0

        if self.metrics["explanations_generated"] > 0:
            metrics["proof_generation_rate"] = (
                self.metrics["proofs_generated"] / self.metrics["explanations_generated"]
            )
            metrics["signing_rate"] = (
                self.metrics["explanations_signed"] / self.metrics["explanations_generated"]
            )
        else:
            metrics["proof_generation_rate"] = 0.0
            metrics["signing_rate"] = 0.0

        return metrics

    def _calculate_std(self, values: list[float]) -> float:
        """Calculate standard deviation."""
        if len(values) < 2:
            return 0.0
        mean = sum(values) / len(values)
        variance = sum((x - mean) ** 2 for x in values) / (len(values) - 1)
        return variance**0.5

    def _extract_premises_from_context(self, context: dict[str, Any]) -> list[str]:
        """Extract logical premises from decision context."""
        premises = []

        # Extract from input data
        if "input_data" in context:
            premises.append("P1: Input data has been validated and processed")

        # Extract from reasoning steps
        if "reasoning_steps" in context:
            for i, step in enumerate(context["reasoning_steps"][:3], 2):
                premises.append(f"P{i}: {step.get('assumption', 'Processing step completed')}")

        # Extract from constraints
        if "constraints" in context:
            for i, constraint in enumerate(context["constraints"][:2], len(premises) + 1):
                premises.append(f"P{i}: {constraint}")

        # Default premises if none found
        if not premises:
            premises = [
                "P1: System is operating within normal parameters",
                "P2: Input data meets quality requirements",
                "P3: Decision algorithms are properly configured",
            ]

        return premises

    def _determine_inference_rules(self, context: dict[str, Any]) -> list[str]:
        """Determine appropriate inference rules based on context."""
        rules = ["Modus Ponens"]  # Always include basic rule

        # Add rules based on reasoning type
        reasoning_type = context.get("reasoning_type", "deductive")

        if reasoning_type == "deductive":
            rules.extend(["Universal Instantiation", "Hypothetical Syllogism"])
        elif reasoning_type == "inductive":
            rules.extend(["Statistical Inference", "Generalization"])
        elif reasoning_type == "abductive":
            rules.extend(["Inference to Best Explanation", "Causal Reasoning"])

        # Add domain-specific rules
        if "probabilistic" in str(context).lower():
            rules.append("Bayesian Inference")
        if "causal" in str(context).lower():
            rules.append("Causal Chain Reasoning")

        return list(set(rules))  # Remove duplicates

    def _build_reasoning_chain(self, context: dict[str, Any]) -> list[dict[str, Any]]:
        """Build a logical reasoning chain from context."""
        chain = []

        # Extract existing reasoning steps
        if "reasoning_steps" in context:
            for step in context["reasoning_steps"]:
                chain.append(
                    {
                        "conclusion": step.get("conclusion", "Intermediate result"),
                        "justification": step.get("justification", "Logical step"),
                        "rule": step.get("rule", "Modus Ponens"),
                        "confidence": step.get("confidence", 0.8),
                        "dependencies": step.get("dependencies", []),
                    }
                )

        # Build chain from decision factors
        elif "decision_factors" in context:
            for i, factor in enumerate(context["decision_factors"]):
                chain.append(
                    {
                        "conclusion": f"Factor {i + 1} supports the decision",
                        "justification": f"Analysis of {factor}",
                        "rule": "Evidential Support",
                        "confidence": 0.75,
                        "dependencies": [f"P{i + 1}"],
                    }
                )

        # Default reasoning chain
        if not chain:
            chain = [
                {
                    "conclusion": "Available evidence supports the decision",
                    "justification": "Synthesis of input data and system analysis",
                    "rule": "Evidential Synthesis",
                    "confidence": context.get("confidence", 0.7),
                    "dependencies": ["P1", "P2"],
                }
            ]

        return chain

    async def _verify_with_theorem_prover(
        self, steps: list[dict[str, Any]], conclusion: str
    ) -> float:
        """Attempt verification with external theorem prover."""
        try:
            # Try to connect to theorem prover service
            # This would integrate with tools like Lean, Coq, or Z3
            # For now, implement basic logical consistency checking

            # Check for logical consistency in steps
            confidence_scores = []

            for step in steps:
                step_confidence = step.get("confidence", 0.8)

                # Basic consistency checks
                if "contradiction" in step.get("statement", "").lower():
                    step_confidence *= 0.5
                if "assumption" in step.get("rule", "").lower():
                    step_confidence *= 0.9  # Assumptions are less certain

                confidence_scores.append(step_confidence)

            # Calculate overall proof strength
            if confidence_scores:
                # Use geometric mean for conservative estimate
                product = 1.0
                for score in confidence_scores:
                    product *= score
                proof_strength = product ** (1.0 / len(confidence_scores))
            else:
                proof_strength = 0.5

            return min(0.95, proof_strength)  # Cap at 95% for automated verification

        except Exception as e:
            logger.debug(f"Theorem prover verification failed: {e}")
            # Return conservative estimate based on step consistency
            return 0.7

    # Helper methods for audit analysis and reporting
    async def _perform_audit_statistical_analysis(
        self, audit_results: list[dict]
    ) -> dict[str, Any]:
        """Perform statistical analysis on audit results"""

        if not audit_results:
            return {"error": "No audit data available"}

        # Extract quality metrics
        compliance_scores = [r["compliance_score"] for r in audit_results]
        signing_rates = [1.0 if r["signed"] else 0.0 for r in audit_results]

        # Calculate explanation quality metrics
        completeness_scores = []
        accuracy_scores = []

        for result in audit_results:
            explanation = result.get("explanation")
            if explanation and hasattr(explanation, "quality_metrics"):
                metrics = explanation.quality_metrics
                completeness_scores.append(metrics.get("completeness", 0.0))
                accuracy_scores.append(metrics.get("accuracy", 0.0))

        stats = {
            "total_decisions": len(audit_results),
            "compliance_statistics": self._calculate_basic_stats(compliance_scores),
            "signing_statistics": {
                "signing_rate": sum(signing_rates) / len(signing_rates),
                "signed_count": sum(signing_rates),
                "unsigned_count": len(signing_rates) - sum(signing_rates),
            },
            "quality_statistics": {
                "avg_completeness": sum(completeness_scores) / len(completeness_scores)
                if completeness_scores
                else 0.0,
                "avg_accuracy": sum(accuracy_scores) / len(accuracy_scores)
                if accuracy_scores
                else 0.0,
                "completeness_distribution": self._calculate_basic_stats(completeness_scores)
                if completeness_scores
                else {},
                "accuracy_distribution": self._calculate_basic_stats(accuracy_scores)
                if accuracy_scores
                else {},
            },
        }

        return stats

    async def _detect_audit_patterns(
        self, audit_results: list[dict], context_data: dict[str, dict]
    ) -> dict[str, Any]:
        """Detect patterns in audit results"""

        patterns = {
            "consistency_score": 0.0,
            "risk_areas": [],
            "improvement_opportunities": [],
            "trends": {},
        }

        if not audit_results:
            return patterns

        # Analyze consistency in compliance scores
        compliance_scores = [r["compliance_score"] for r in audit_results]
        if compliance_scores:
            import statistics

            std_dev = statistics.stdev(compliance_scores) if len(compliance_scores) > 1 else 0
            patterns["consistency_score"] = max(
                0.0, 1.0 - std_dev
            )  # High consistency = low deviation

        # Identify risk areas
        low_compliance_decisions = [
            r["decision_id"] for r in audit_results if r["compliance_score"] < 0.6
        ]
        if low_compliance_decisions:
            patterns["risk_areas"].append(
                {
                    "type": "low_compliance",
                    "description": f"{len(low_compliance_decisions)} decisions have compliance scores below 0.6",
                    "affected_decisions": low_compliance_decisions[:5],  # Limit for brevity
                    "severity": "high"
                    if len(low_compliance_decisions) > len(audit_results) * 0.2
                    else "medium",
                }
            )

        # Identify unsigned decisions
        unsigned_decisions = [r["decision_id"] for r in audit_results if not r["signed"]]
        if unsigned_decisions:
            patterns["risk_areas"].append(
                {
                    "type": "unsigned_decisions",
                    "description": f"{len(unsigned_decisions)} decisions are not SRD-signed",
                    "affected_decisions": unsigned_decisions[:5],
                    "severity": "medium"
                    if len(unsigned_decisions) < len(audit_results) * 0.1
                    else "high",
                }
            )

        # Improvement opportunities
        avg_compliance = sum(compliance_scores) / len(compliance_scores)
        if avg_compliance < 0.8:
            patterns["improvement_opportunities"].append(
                {
                    "type": "overall_compliance",
                    "description": f"Average compliance score ({avg_compliance:.2f}) could be improved",
                    "priority": "high",
                    "suggested_actions": [
                        "Review decision-making processes",
                        "Enhance training for decision makers",
                        "Implement additional quality checks",
                    ],
                }
            )

        return patterns

    async def _generate_enhanced_audit_recommendations(
        self, audit_results: list[dict], stats: dict, patterns: dict
    ) -> list[dict[str, str]]:
        """Generate enhanced recommendations based on statistical analysis"""

        recommendations = []

        # Base recommendations
        recommendations.extend(
            [
                {
                    "category": "signing_compliance",
                    "recommendation": "Ensure all critical decisions are SRD-signed",
                    "priority": "high",
                    "rationale": "Digital signatures provide authenticity and non-repudiation",
                },
                {
                    "category": "quality_monitoring",
                    "recommendation": "Monitor compliance scores below 0.7",
                    "priority": "medium",
                    "rationale": "Low compliance scores indicate potential issues",
                },
            ]
        )

        # Statistical analysis based recommendations
        signing_rate = stats.get("signing_statistics", {}).get("signing_rate", 1.0)
        if signing_rate < 0.9:
            recommendations.append(
                {
                    "category": "process_improvement",
                    "recommendation": f"Improve signing rate from {signing_rate:.1%} to >90%",
                    "priority": "high",
                    "rationale": "Low signing rate indicates process gaps",
                }
            )

        # Pattern-based recommendations
        for risk_area in patterns.get("risk_areas", []):
            if risk_area["type"] == "low_compliance":
                recommendations.append(
                    {
                        "category": "compliance_improvement",
                        "recommendation": f"Address {len(risk_area['affected_decisions'])} low-compliance decisions",
                        "priority": risk_area["severity"],
                        "rationale": "Low compliance decisions pose regulatory and operational risks",
                    }
                )

        # Quality-based recommendations
        avg_completeness = stats.get("quality_statistics", {}).get("avg_completeness", 1.0)
        if avg_completeness < 0.8:
            recommendations.append(
                {
                    "category": "explanation_quality",
                    "recommendation": f"Improve explanation completeness from {avg_completeness:.1%}",
                    "priority": "medium",
                    "rationale": "Incomplete explanations reduce transparency and auditability",
                }
            )

        return recommendations

    def _assess_overall_compliance(self, stats: dict) -> str:
        """Assess overall compliance status based on statistics"""

        compliance_stats = stats.get("compliance_statistics", {})
        avg_compliance = compliance_stats.get("mean", 0.0)
        signing_rate = stats.get("signing_statistics", {}).get("signing_rate", 0.0)

        if avg_compliance > 0.8 and signing_rate > 0.9:
            return "compliant"
        elif avg_compliance > 0.6 and signing_rate > 0.7:
            return "partially_compliant"
        else:
            return "non_compliant"

    def _calculate_basic_stats(self, values: list[float]) -> dict[str, float]:
        """Calculate basic statistical measures for a list of values"""

        if not values:
            return {}

        import statistics

        return {
            "count": len(values),
            "mean": statistics.mean(values),
            "median": statistics.median(values),
            "std_dev": statistics.stdev(values) if len(values) > 1 else 0.0,
            "min": min(values),
            "max": max(values),
        }

    # Helper methods for dialogue management and interactive explanations
    async def _generate_contextual_followups(
        self, decision_id: str, explanation: Any, context: dict[str, Any]
    ) -> list[str]:
        """Generate intelligent follow-up questions based on decision context"""

        followups = [
            "Can you explain this in more detail?",
            "What were the key factors in this decision?",
            "How confident is the system in this decision?",
        ]

        # Context-aware question generation
        if context.get("ethical_factors"):
            followups.append("What ethical considerations were involved?")
            followups.append("How were competing ethical principles balanced?")

        if context.get("uncertainty_level", 0) > 0.5:
            followups.append("What are the main sources of uncertainty?")
            followups.append("What would change your confidence in this decision?")

        if context.get("stakeholders"):
            followups.append("How does this affect different stakeholders?")
            followups.append("Were all relevant perspectives considered?")

        if context.get("alternatives"):
            followups.append("What alternatives were considered?")
            followups.append("Why wasn't the second-best option chosen?")

        if context.get("timeline_pressure"):
            followups.append("How did time constraints affect this decision?")

        if context.get("resource_constraints"):
            followups.append("How did resource limitations influence the choice?")

        # Decision type specific questions
        decision_type = context.get("decision_type", "")
        if "financial" in decision_type.lower():
            followups.extend(
                [
                    "What are the financial implications?",
                    "How was risk assessed?",
                ]
            )
        elif "technical" in decision_type.lower():
            followups.extend(
                [
                    "What technical factors were most important?",
                    "Are there any technical risks?",
                ]
            )
        elif "policy" in decision_type.lower():
            followups.extend(
                [
                    "What policy implications should be considered?",
                    "How does this align with existing policies?",
                ]
            )

        return followups[:8]  # Limit to most relevant questions

    async def _generate_suggested_actions(self, context: dict[str, Any]) -> list[dict[str, str]]:
        """Generate suggested actions user can take based on context"""

        actions = [
            {
                "action": "request_detailed_analysis",
                "description": "Get a more detailed technical analysis",
                "icon": "ðŸ“Š",
            },
            {
                "action": "explore_alternatives",
                "description": "Explore alternative decisions and their outcomes",
                "icon": "ðŸ”„",
            },
            {
                "action": "stakeholder_impact",
                "description": "Analyze impact on different stakeholders",
                "icon": "ðŸ‘¥",
            },
        ]

        if context.get("can_modify"):
            actions.append(
                {
                    "action": "propose_modification",
                    "description": "Propose modifications to the decision",
                    "icon": "âœï¸",
                }
            )

        if context.get("reversible"):
            actions.append(
                {
                    "action": "simulate_reversal",
                    "description": "Simulate reversing this decision",
                    "icon": "â†©ï¸",
                }
            )

        return actions

    def _assess_decision_complexity(self, explanation: Any) -> str:
        """Assess the complexity level of a decision for dialogue adaptation"""

        if not hasattr(explanation, "natural_language"):
            return "medium"

        text = explanation.natural_language
        word_count = len(text.split())

        # Simple heuristic based on explanation length and complexity indicators
        complexity_indicators = [
            "multiple factors",
            "complex interaction",
            "trade-off",
            "uncertainty",
            "competing",
            "interdependent",
            "cascading",
            "non-linear",
        ]

        complexity_score = sum(
            1 for indicator in complexity_indicators if indicator in text.lower()
        )

        if word_count > 300 or complexity_score > 3:
            return "high"
        elif word_count > 150 or complexity_score > 1:
            return "medium"
        else:
            return "low"


"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘ ðŸ“‹ FOOTER - LUKHAS AI
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘ VALIDATION:
â•‘   - Tests: lukhas/tests/bridge/test_explainability_interface_layer.py
â•‘   - Coverage: 85%
â•‘   - Linting: pylint 8.5/10
â•‘
â•‘ MONITORING:
â•‘   - Metrics: explanation_generation_time, proof_validity_score, quality_metrics
â•‘   - Logs: Î›TRACE_EXPLANATION_*, Î›TRACE_SRD_*, Î›TRACE_INTEGRATION_*
â•‘   - Alerts: explanation_failure, proof_generation_error, signing_failure
â•‘
â•‘ COMPLIANCE:
â•‘   - Standards: Explainable AI Guidelines, Transparency Regulations
â•‘   - Ethics: Human-interpretable decisions, bias detection and mitigation
â•‘   - Safety: Cryptographic proof integrity, tamper-resistant explanations
â•‘
â•‘ REFERENCES:
â•‘   - Docs: docs/bridge/explainability_interface_layer.md
â•‘   - Issues: github.com/lukhas-ai/lukhas/issues?label=XIL
â•‘   - Wiki: /wiki/Explainability_Interface_Layer
â•‘
â•‘ COPYRIGHT & LICENSE:
â•‘   Copyright (c) 2025 LUKHAS AI. All rights reserved.
â•‘   Licensed under the LUKHAS AI Proprietary License.
â•‘   Unauthorized use, reproduction, or distribution is prohibited.
â•‘
â•‘ DISCLAIMER:
â•‘   This module is part of the LUKHAS AGI system. Use only as intended
â•‘   within the system architecture. Modifications may affect system
â•‘   stability and require approval from the LUKHAS Architecture Board.
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""

# CLAUDE CHANGELOG
# [CLAUDE_01] Applied standardized LUKHAS AI header and footer template to explainability_interface_layer.py module. Updated header with proper module metadata, description, and symbolic tags. Added module constants and preserved all existing Î›STUB methods and functionality. # CLAUDE_EDIT_v0.1
