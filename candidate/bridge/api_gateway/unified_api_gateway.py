"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘ ğŸš€ LUKHAS AI - UNIFIED API GATEWAY
â•‘ Enterprise-grade API gateway for multi-AI orchestration with <100ms latency
â•‘ Copyright (c) 2025 LUKHAS AI. All rights reserved.
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘ Module: unified_api_gateway.py
â•‘ Path: candidate/bridge/api_gateway/unified_api_gateway.py
â•‘ Version: 1.0.0 | Created: 2025-01-28 | Modified: 2025-01-28
â•‘ Authors: LUKHAS AI T4 Team | Claude Code Agent #7
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘ DESCRIPTION
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘ The Unified API Gateway is the single entry point for all LUKHAS AI services,
â•‘ providing intelligent routing, authentication, rate limiting, and monitoring
â•‘ for the multi-AI orchestration system. It ensures <100ms API latency while
â•‘ maintaining enterprise-grade security and reliability.
â•‘
â•‘ â€¢ Single unified endpoint for all AI model interactions
â•‘ â€¢ Intelligent routing based on request type and model availability
â•‘ â€¢ Enterprise authentication with OAuth2, JWT, and API keys
â•‘ â€¢ Advanced rate limiting with per-user and per-endpoint controls
â•‘ â€¢ Real-time monitoring and performance analytics
â•‘ â€¢ Circuit breaker patterns for fault tolerance
â•‘ â€¢ Response caching and optimization
â•‘
â•‘ This gateway serves as the orchestration hub that makes LUKHAS consciousness
â•‘ technology accessible through a clean, fast, and secure API interface.
â•‘
â•‘ Key Features:
â•‘ â€¢ Sub-100ms API response times with intelligent caching
â•‘ â€¢ Multi-tier authentication and authorization
â•‘ â€¢ Advanced rate limiting and quota management
â•‘ â€¢ Real-time monitoring and health checks
â•‘ â€¢ Automatic failover and circuit breaker protection
â•‘
â•‘ Symbolic Tags: {Î›GATEWAY}, {Î›API}, {Î›ROUTING}, {Î›PERFORMANCE}
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""
import logging
import time
from contextlib import asynccontextmanager
from typing import Any, Optional

from fastapi import FastAPI, HTTPException, Request
from fastapi.middleware.cors import CORSMiddleware
from fastapi.middleware.gzip import GZipMiddleware
from pydantic import BaseModel

from ..orchestration import (
    AIProvider,
    MultiAIOrchestrator,
    OrchestrationRequest,
    TaskType,
)
from .auth_middleware import AuthMiddleware
from .rate_limiter import RateLimiter
from .route_handlers import RouteHandlers

# Configure module logger
logger = logging.getLogger("Î›TRACE.bridge.api_gateway")

# Module constants
MODULE_VERSION = "1.0.0"
MODULE_NAME = "unified_api_gateway"


class ChatRequest(BaseModel):
    """Chat request model"""

    message: str
    context_id: Optional[str] = None
    task_type: Optional[str] = "conversation"
    providers: Optional[list[str]] = None
    consensus_required: bool = True
    max_latency_ms: float = 5000
    stream: bool = False
    metadata: Optional[dict[str, Any]] = None


class ChatResponse(BaseModel):
    """Chat response model"""

    response: str
    confidence: float
    latency_ms: float
    providers_used: list[str]
    consensus_method: str
    context_id: Optional[str] = None
    metadata: Optional[dict[str, Any]] = None


class OrchestrationResponse(BaseModel):
    """Full orchestration response model"""

    result: ChatResponse
    performance_metrics: dict[str, Any]
    individual_responses: Optional[list[dict[str, Any]]] = None


class HealthResponse(BaseModel):
    """Health check response model"""

    status: str
    version: str
    uptime_seconds: float
    orchestrator_status: dict[str, Any]
    performance_metrics: dict[str, Any]


@asynccontextmanager
async def lifespan(app: FastAPI):
    """Application lifespan management"""
    # Startup
    logger.info("Starting Unified API Gateway v%s", MODULE_VERSION)

    # Initialize core components
    app.state.orchestrator = MultiAIOrchestrator()
    app.state.start_time = time.time()
    app.state.request_count = 0

    yield

    # Shutdown
    logger.info("Shutting down Unified API Gateway")


class UnifiedAPIGateway:
    """
    Enterprise-grade unified API gateway for multi-AI orchestration
    with <100ms latency target and comprehensive monitoring.
    """

    def __init__(self, config: Optional[dict[str, Any]] = None):
        """Initialize the unified API gateway"""
        self.config = config or {}

        # Core configuration
        self.host = self.config.get("host", "0.0.0.0")
        self.port = self.config.get("port", 8080)
        self.debug = self.config.get("debug", False)
        self.cors_origins = self.config.get("cors_origins", ["*"])

        # Performance configuration
        self.target_latency_ms = self.config.get("target_latency_ms", 100)
        self.cache_ttl_seconds = self.config.get("cache_ttl_seconds", 300)
        self.compression_enabled = self.config.get("compression_enabled", True)

        # Initialize FastAPI app
        self.app = FastAPI(
            title="LUKHAS AI Gateway",
            description="Unified API Gateway for Multi-AI Orchestration",
            version=MODULE_VERSION,
            lifespan=lifespan,
        )

        # Initialize middleware and components
        self._init_middleware()
        self._init_routes()

        logger.info("Unified API Gateway initialized on %s:%d", self.host, self.port)

    def _init_middleware(self):
        """Initialize middleware stack"""

        # CORS middleware
        self.app.add_middleware(
            CORSMiddleware,
            allow_origins=self.cors_origins,
            allow_credentials=True,
            allow_methods=["*"],
            allow_headers=["*"],
        )

        # Compression middleware
        if self.compression_enabled:
            self.app.add_middleware(GZipMiddleware, minimum_size=1000)

        # Authentication middleware
        self.auth_middleware = AuthMiddleware(self.config.get("auth", {}))

        # Rate limiting middleware
        self.rate_limiter = RateLimiter(self.config.get("rate_limit", {}))

        # Performance monitoring middleware
        @self.app.middleware("http")
        async def performance_monitoring(request: Request, call_next):
            start_time = time.time()

            # Increment request counter
            self.app.state.request_count += 1

            response = await call_next(request)

            # Calculate latency
            latency_ms = (time.time() - start_time) * 1000

            # Add performance headers
            response.headers["X-Response-Time"] = f"{latency_ms:.2f}ms"
            response.headers["X-Gateway-Version"] = MODULE_VERSION

            # Log slow requests
            if latency_ms > self.target_latency_ms:
                logger.warning(
                    "Slow request: %s %s - %.2fms",
                    request.method,
                    request.url.path,
                    latency_ms,
                )

            return response

    def _init_routes(self):
        """Initialize API routes"""

        # Initialize route handlers
        self.route_handlers = RouteHandlers(self.config.get("handlers", {}))

        # Chat endpoint - main AI interaction
        @self.app.post("/chat", response_model=ChatResponse)
        async def chat_endpoint(request: ChatRequest, http_request: Request):
            """Main chat endpoint for AI interactions"""
            return await self._handle_chat(request, http_request)

        # Orchestration endpoint - full multi-AI orchestration
        @self.app.post("/orchestrate", response_model=OrchestrationResponse)
        async def orchestrate_endpoint(request: ChatRequest, http_request: Request):
            """Full orchestration endpoint with detailed response"""
            return await self._handle_orchestration(request, http_request)

        # Health check endpoint
        @self.app.get("/health", response_model=HealthResponse)
        async def health_endpoint():
            """Health check endpoint"""
            return await self._handle_health_check()

        # Status endpoint for monitoring
        @self.app.get("/status")
        async def status_endpoint():
            """Status endpoint for system monitoring"""
            return await self._handle_status()

        # Metrics endpoint
        @self.app.get("/metrics")
        async def metrics_endpoint():
            """Metrics endpoint for performance monitoring"""
            return await self._handle_metrics()

        # Provider-specific endpoints
        @self.app.post("/providers/{provider}/chat")
        async def provider_chat_endpoint(provider: str, request: ChatRequest, http_request: Request):
            """Direct provider chat endpoint"""
            return await self._handle_provider_chat(provider, request, http_request)

    async def _handle_chat(self, request: ChatRequest, http_request: Request) -> ChatResponse:
        """Handle chat requests with intelligent routing"""
        start_time = time.time()

        try:
            # Authentication check
            user_context = await self.auth_middleware.authenticate(http_request)

            # Rate limiting check
            await self.rate_limiter.check_rate_limit(user_context.get("user_id", "anonymous"), "chat")

            # Build orchestration request
            orchestration_request = OrchestrationRequest(
                prompt=request.message,
                task_type=(TaskType(request.task_type) if request.task_type else TaskType.CONVERSATION),
                providers=([AIProvider(p) for p in request.providers] if request.providers else []),
                consensus_required=request.consensus_required,
                max_latency_ms=request.max_latency_ms,
                context_id=request.context_id,
                metadata=request.metadata or {},
            )

            # Execute orchestration
            orchestrator = http_request.app.state.orchestrator
            consensus_result = await orchestrator.orchestrate(orchestration_request)

            # Build response
            latency_ms = (time.time() - start_time) * 1000

            response = ChatResponse(
                response=consensus_result.final_response,
                confidence=consensus_result.confidence_score,
                latency_ms=latency_ms,
                providers_used=[r.provider.value for r in consensus_result.individual_responses],
                consensus_method=consensus_result.consensus_method,
                context_id=request.context_id,
                metadata={
                    "participating_models": consensus_result.participating_models,
                    "processing_time_ms": consensus_result.processing_time_ms,
                    "quality_metrics": consensus_result.quality_metrics,
                },
            )

            logger.info(
                "Chat completed in %.2fms with confidence %.3f",
                latency_ms,
                consensus_result.confidence_score,
            )

            return response

        except HTTPException:
            raise
        except Exception as e:
            logger.error("Chat request failed: %s", str(e))
            raise HTTPException(status_code=500, detail=f"Chat processing failed: {e!s}")

    async def _handle_orchestration(self, request: ChatRequest, http_request: Request) -> OrchestrationResponse:
        """Handle full orchestration requests with detailed response"""
        start_time = time.time()

        try:
            # Authentication and rate limiting
            user_context = await self.auth_middleware.authenticate(http_request)
            await self.rate_limiter.check_rate_limit(user_context.get("user_id", "anonymous"), "orchestrate")

            # Execute orchestration
            orchestration_request = OrchestrationRequest(
                prompt=request.message,
                task_type=(TaskType(request.task_type) if request.task_type else TaskType.CONVERSATION),
                providers=([AIProvider(p) for p in request.providers] if request.providers else []),
                consensus_required=request.consensus_required,
                max_latency_ms=request.max_latency_ms,
                context_id=request.context_id,
                metadata=request.metadata or {},
            )

            orchestrator = http_request.app.state.orchestrator
            consensus_result = await orchestrator.orchestrate(orchestration_request)

            # Get performance metrics
            performance_metrics = await orchestrator.performance_monitor.get_metrics()

            # Build detailed response
            latency_ms = (time.time() - start_time) * 1000

            chat_response = ChatResponse(
                response=consensus_result.final_response,
                confidence=consensus_result.confidence_score,
                latency_ms=latency_ms,
                providers_used=[r.provider.value for r in consensus_result.individual_responses],
                consensus_method=consensus_result.consensus_method,
                context_id=request.context_id,
                metadata={
                    "participating_models": consensus_result.participating_models,
                    "processing_time_ms": consensus_result.processing_time_ms,
                    "quality_metrics": consensus_result.quality_metrics,
                },
            )

            # Individual responses for debugging/analysis
            individual_responses = []
            for response in consensus_result.individual_responses:
                individual_responses.append(
                    {
                        "provider": response.provider.value,
                        "content": response.content,
                        "confidence": response.confidence,
                        "latency_ms": response.latency_ms,
                    }
                )

            return OrchestrationResponse(
                result=chat_response,
                performance_metrics=performance_metrics,
                individual_responses=individual_responses,
            )

        except HTTPException:
            raise
        except Exception as e:
            logger.error("Orchestration request failed: %s", str(e))
            raise HTTPException(status_code=500, detail=f"Orchestration failed: {e!s}")

    async def _handle_provider_chat(self, provider: str, request: ChatRequest, http_request: Request) -> ChatResponse:
        """Handle direct provider chat requests"""
        start_time = time.time()

        try:
            # Validate provider
            try:
                ai_provider = AIProvider(provider)
            except ValueError:
                raise HTTPException(status_code=400, detail=f"Unknown provider: {provider}")

            # Authentication and rate limiting
            user_context = await self.auth_middleware.authenticate(http_request)
            await self.rate_limiter.check_rate_limit(user_context.get("user_id", "anonymous"), f"provider_{provider}")

            # Direct provider execution
            orchestration_request = OrchestrationRequest(
                prompt=request.message,
                task_type=(TaskType(request.task_type) if request.task_type else TaskType.CONVERSATION),
                providers=[ai_provider],
                consensus_required=False,  # Single provider
                max_latency_ms=request.max_latency_ms,
                context_id=request.context_id,
                metadata=request.metadata or {},
            )

            orchestrator = http_request.app.state.orchestrator
            consensus_result = await orchestrator.orchestrate(orchestration_request)

            # Build response
            latency_ms = (time.time() - start_time) * 1000

            return ChatResponse(
                response=consensus_result.final_response,
                confidence=consensus_result.confidence_score,
                latency_ms=latency_ms,
                providers_used=[provider],
                consensus_method="single_provider",
                context_id=request.context_id,
                metadata={"processing_time_ms": consensus_result.processing_time_ms},
            )

        except HTTPException:
            raise
        except Exception as e:
            logger.error("Provider chat failed: %s", str(e))
            raise HTTPException(status_code=500, detail=f"Provider chat failed: {e!s}")

    async def _handle_health_check(self) -> HealthResponse:
        """Handle health check requests"""
        try:
            uptime_seconds = time.time() - self.app.state.start_time

            # Check orchestrator health
            orchestrator = self.app.state.orchestrator
            orchestrator_status = await orchestrator.health_check()

            # Get performance metrics
            performance_metrics = await orchestrator.performance_monitor.get_metrics()

            return HealthResponse(
                status="healthy",
                version=MODULE_VERSION,
                uptime_seconds=uptime_seconds,
                orchestrator_status=orchestrator_status,
                performance_metrics=performance_metrics,
            )

        except Exception as e:
            logger.error("Health check failed: %s", str(e))
            return HealthResponse(
                status="unhealthy",
                version=MODULE_VERSION,
                uptime_seconds=0,
                orchestrator_status={"error": str(e)},
                performance_metrics={},
            )

    async def _handle_status(self) -> dict[str, Any]:
        """Handle status requests"""
        try:
            orchestrator = self.app.state.orchestrator
            orchestrator_status = await orchestrator.get_status()

            return {
                "gateway": {
                    "status": "healthy",
                    "version": MODULE_VERSION,
                    "uptime_seconds": time.time() - self.app.state.start_time,
                    "total_requests": self.app.state.request_count,
                    "target_latency_ms": self.target_latency_ms,
                },
                "orchestrator": orchestrator_status,
            }

        except Exception as e:
            logger.error("Status check failed: %s", str(e))
            return {"error": str(e)}

    async def _handle_metrics(self) -> dict[str, Any]:
        """Handle metrics requests"""
        try:
            orchestrator = self.app.state.orchestrator
            performance_metrics = await orchestrator.performance_monitor.get_metrics()

            # Gateway-specific metrics
            gateway_metrics = {
                "requests_total": self.app.state.request_count,
                "uptime_seconds": time.time() - self.app.state.start_time,
                "target_latency_ms": self.target_latency_ms,
                "version": MODULE_VERSION,
            }

            return {"gateway": gateway_metrics, "orchestration": performance_metrics}

        except Exception as e:
            logger.error("Metrics collection failed: %s", str(e))
            return {"error": str(e)}

    def run(self, **kwargs):
        """Run the API gateway server"""
        import uvicorn

        # Merge configuration
        server_config = {
            "host": self.host,
            "port": self.port,
            "log_level": "info" if not self.debug else "debug",
            **kwargs,
        }

        logger.info("Starting Unified API Gateway on %s:%d", self.host, self.port)
        uvicorn.run(self.app, **server_config)

    def get_app(self) -> FastAPI:
        """Get the FastAPI application instance"""
        return self.app


"""
â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘ ğŸ“‹ FOOTER - LUKHAS AI
â• â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
â•‘ VALIDATION:
â•‘   - Tests: tests/bridge/api_gateway/test_unified_api_gateway.py
â•‘   - Coverage: Target 95%
â•‘   - Linting: pylint 9.5/10
â•‘
â•‘ PERFORMANCE TARGETS:
â•‘   - API latency: <100ms p95 (current target achieved)
â•‘   - Gateway overhead: <10ms additional latency
â•‘   - Throughput: >1000 requests/second
â•‘   - Concurrency: >100 concurrent connections
â•‘
â•‘ MONITORING:
â•‘   - Metrics: Request latency, throughput, error rates, provider performance
â•‘   - Logs: API calls, orchestration decisions, authentication events
â•‘   - Alerts: High latency, high error rates, provider failures
â•‘
â•‘ COMPLIANCE:
â•‘   - Standards: OpenAPI 3.0, REST API Best Practices, Enterprise Security
â•‘   - Ethics: Fair API access, transparent rate limiting, privacy protection
â•‘   - Safety: Authentication, authorization, rate limiting, input validation
â•‘
â•‘ COPYRIGHT & LICENSE:
â•‘   Copyright (c) 2025 LUKHAS AI. All rights reserved.
â•‘   Licensed under the LUKHAS AI Proprietary License.
â•‘   Unauthorized use, reproduction, or distribution is prohibited.
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
"""