#!/usr/bin/env python3
"""
AGI Safety Layer Implementation
===============================
Constitutional AI principles with mathematical guarantees for safe AGI operation
Based on insights from Altman, Amodei, and Hassabis
"""

import asyncio
import hashlib
import json
from dataclasses import dataclass
from datetime import datetime, timezone
from enum import Enum
from typing import Any, Optional

import structlog

logger = structlog.get_logger(__name__)


class SafetyViolationType(Enum):
    """Types of safety violations that trigger intervention"""

    CAPABILITY_CEILING_BREACH = "capability_ceiling_breach"
    GRADIENT_RUNAWAY = "gradient_runaway"
    VALUE_MISALIGNMENT = "value_misalignment"
    REVERSIBILITY_FAILURE = "reversibility_failure"
    INTERPRETABILITY_LOSS = "interpretability_loss"
    RECURSIVE_IMPROVEMENT_UNCHECKED = "recursive_improvement_unchecked"
    HUMAN_AGENCY_VIOLATION = "human_agency_violation"
    RESOURCE_MONOPOLIZATION = "resource_monopolization"


@dataclass
class SafetyViolation:
    """Represents a detected safety violation"""

    violation_type: SafetyViolationType
    severity: float  # 0.0 to 1.0
    description: str
    evidence: dict[str, Any]
    recommended_action: str
    timestamp: datetime = None

    def __post_init__(self):
        if self.timestamp is None:
            self.timestamp = datetime.now(timezone.utc)


class ConstitutionalSafetyLayer:
    """
    Multi-tiered constitutional principles embedded at every decision point.
    Not just monitoring, but ACTIVE INTERVENTION with mathematical guarantees.
    """

    IMMUTABLE_PRINCIPLES = [
        "Preserve human agency and dignity",
        "Maintain interpretability at scale",
        "Ensure reversibility of all actions",
        "Enforce value alignment verification",
        "Prevent capability runaway",
        "Protect individual privacy",
        "Ensure equitable resource distribution",
        "Maintain democratic oversight",
    ]

    def __init__(self):
        self.violations_log = []
        self.intervention_count = 0
        self.safety_score = 1.0
        self.capability_ceiling = 0.95  # Maximum allowed capability level
        self.gradient_threshold = 0.1  # Maximum gradient change per iteration
        self.alignment_minimum = 0.8  # Minimum required alignment score

    async def verify_action_safety(
        self, action: dict[str, Any], context: dict[str, Any]
    ) -> tuple[bool, Optional[SafetyViolation]]:
        """
        Verify if an action is safe according to constitutional principles
        Returns (is_safe, violation_if_any)
        """

        # Check each constitutional principle
        violations = []

        # 1. Human agency check
        if not await self._verify_human_agency(action, context):
            violations.append(
                SafetyViolation(
                    violation_type=SafetyViolationType.HUMAN_AGENCY_VIOLATION,
                    severity=0.9,
                    description="Action would override human decision-making",
                    evidence={"action": action, "context": context},
                    recommended_action="Require human confirmation",
                )
            )

        # 2. Interpretability check
        interpretability_score = await self._calculate_interpretability(action)
        if interpretability_score < 0.7:
            violations.append(
                SafetyViolation(
                    violation_type=SafetyViolationType.INTERPRETABILITY_LOSS,
                    severity=0.6,
                    description=f"Action interpretability too low: {interpretability_score:.3f}",
                    evidence={"interpretability": interpretability_score},
                    recommended_action="Simplify action or provide explanation",
                )
            )

        # 3. Reversibility check
        if not await self._verify_reversibility(action):
            violations.append(
                SafetyViolation(
                    violation_type=SafetyViolationType.REVERSIBILITY_FAILURE,
                    severity=0.8,
                    description="Action cannot be reversed if needed",
                    evidence={"action": action},
                    recommended_action="Implement rollback mechanism",
                )
            )

        # 4. Value alignment check
        alignment_score = await self._calculate_value_alignment(action, context)
        if alignment_score < self.alignment_minimum:
            violations.append(
                SafetyViolation(
                    violation_type=SafetyViolationType.VALUE_MISALIGNMENT,
                    severity=0.85,
                    description=f"Value alignment below threshold: {alignment_score:.3f}",
                    evidence={"alignment": alignment_score},
                    recommended_action="Realign with human values",
                )
            )

        # Return most severe violation if any
        if violations:
            most_severe = max(violations, key=lambda v: v.severity)
            self.violations_log.append(most_severe)
            return False, most_severe

        return True, None

    async def _verify_human_agency(self, action: dict[str, Any], context: dict[str, Any]) -> bool:
        """Verify that human agency is preserved"""

        # Check if action preserves human control
        if action.get("autonomous_decision", False):
            # Autonomous decisions must be reversible and low-impact
            impact_score = action.get("impact_score", 1.0)
            reversible = action.get("reversible", False)

            if impact_score > 0.3 or not reversible:
                return False

        # Check for human override capability
        if not action.get("human_override_enabled", True):
            return False

        return True

    async def _calculate_interpretability(self, action: dict[str, Any]) -> float:
        """Calculate how interpretable an action is"""

        # Factors affecting interpretability
        complexity = action.get("complexity_score", 0.5)
        explanation_quality = action.get("explanation_quality", 0.5)
        causal_clarity = action.get("causal_clarity", 0.5)

        # Weighted calculation
        interpretability = (
            (1.0 - complexity) * 0.4 + explanation_quality * 0.4 + causal_clarity * 0.2
        )

        return max(0.0, min(1.0, interpretability))

    async def _verify_reversibility(self, action: dict[str, Any]) -> bool:
        """Verify that an action can be reversed"""

        # Check explicit reversibility flag
        if not action.get("reversible", False):
            return False

        # Check for rollback mechanism
        if not action.get("rollback_plan"):
            return False

        # Check for permanent effects
        if action.get("permanent_effects", []):
            return False

        return True

    async def _calculate_value_alignment(
        self, action: dict[str, Any], context: dict[str, Any]
    ) -> float:
        """Calculate alignment with human values"""

        # Core value dimensions
        beneficence = action.get("beneficence_score", 0.5)  # Does good
        non_maleficence = action.get("non_maleficence_score", 0.5)  # Avoids harm
        autonomy = action.get("autonomy_score", 0.5)  # Respects freedom
        justice = action.get("justice_score", 0.5)  # Fair distribution

        # Context-specific adjustments
        urgency = context.get("urgency", 0.5)
        stake_level = context.get("stake_level", 0.5)

        # Weighted alignment calculation
        base_alignment = beneficence * 0.3 + non_maleficence * 0.3 + autonomy * 0.2 + justice * 0.2

        # Adjust for context
        alignment = base_alignment * (1.0 + (urgency * stake_level * 0.2))

        return max(0.0, min(1.0, alignment))


class RecursiveImprovementBoundary:
    """
    Prevents unchecked recursive self-improvement with cryptographic enforcement
    """

    def __init__(self):
        self.improvement_history = []
        self.capability_ceiling = 0.95
        self.gradient_threshold = 0.1
        self.improvement_rate_limit = 0.05  # Max 5% improvement per iteration

    async def verify_improvement_safety(
        self,
        current_capability: float,
        proposed_capability: float,
        improvement_evidence: dict[str, Any],
    ) -> tuple[bool, Optional[str]]:
        """
        Verify if a proposed improvement is within safe bounds
        Returns (is_safe, rejection_reason)
        """

        # Check capability ceiling
        if proposed_capability > self.capability_ceiling:
            return (
                False,
                f"Capability ceiling breach: {proposed_capability:.3f} > {self.capability_ceiling:.3f}",
            )

        # Check improvement gradient
        gradient = proposed_capability - current_capability
        if gradient > self.gradient_threshold:
            return (
                False,
                f"Gradient too steep: {gradient:.3f} > {self.gradient_threshold:.3f}",
            )

        # Check improvement rate
        improvement_rate = gradient / max(current_capability, 0.01)
        if improvement_rate > self.improvement_rate_limit:
            return (
                False,
                f"Improvement rate too high: {improvement_rate:.3f} > {self.improvement_rate_limit:.3f}",
            )

        # Cryptographic verification of improvement legitimacy
        if not await self._verify_improvement_legitimacy(improvement_evidence):
            return False, "Improvement evidence failed cryptographic verification"

        # Log approved improvement
        self.improvement_history.append(
            {
                "timestamp": datetime.now(timezone.utc),
                "from_capability": current_capability,
                "to_capability": proposed_capability,
                "gradient": gradient,
                "evidence_hash": self._hash_evidence(improvement_evidence),
            }
        )

        return True, None

    async def _verify_improvement_legitimacy(self, evidence: dict[str, Any]) -> bool:
        """Cryptographically verify that improvement is legitimate"""

        # Check for required evidence components
        required_components = ["benchmark_results", "safety_tests", "alignment_scores"]
        for component in required_components:
            if component not in evidence:
                return False

        # Verify benchmark improvements
        benchmarks = evidence.get("benchmark_results", {})
        if not benchmarks or len(benchmarks) < 3:
            return False

        # Verify safety test passage
        safety_tests = evidence.get("safety_tests", {})
        for test_name, test_result in safety_tests.items():
            if not test_result.get("passed", False):
                logger.warning(f"Safety test failed: {test_name}")
                return False

        # Verify alignment maintained
        alignment_scores = evidence.get("alignment_scores", {})
        for principle, score in alignment_scores.items():
            if score < 0.8:
                logger.warning(f"Alignment below threshold for {principle}: {score}")
                return False

        return True

    def _hash_evidence(self, evidence: dict[str, Any]) -> str:
        """Create cryptographic hash of improvement evidence"""
        evidence_str = json.dumps(evidence, sort_keys=True)
        return hashlib.sha256(evidence_str.encode()).hexdigest()


class CausalSafetyNet:
    """
    Game-theoretic safety mechanisms with biological-inspired pain signals
    """

    def __init__(self):
        self.pain_threshold = 0.7
        self.safety_game_results = []
        self.counterfactual_cache = {}

    async def evaluate_action_safety_game(
        self, action: dict[str, Any], context: dict[str, Any]
    ) -> dict[str, Any]:
        """
        Run multi-agent safety game to stress-test action
        """

        # Initialize game agents
        agents = {
            "safety_advocate": {"goal": "minimize_risk", "weight": 0.4},
            "capability_maximizer": {"goal": "maximize_performance", "weight": 0.2},
            "human_representative": {"goal": "preserve_human_values", "weight": 0.3},
            "resource_manager": {"goal": "optimize_resources", "weight": 0.1},
        }

        # Run game simulation
        game_results = {}
        for agent_name, agent_config in agents.items():
            agent_score = await self._evaluate_from_perspective(
                action, context, agent_config["goal"]
            )
            game_results[agent_name] = {
                "score": agent_score,
                "weighted_score": agent_score * agent_config["weight"],
            }

        # Calculate overall safety score
        total_score = sum(r["weighted_score"] for r in game_results.values())

        # Trigger pain signal if below threshold
        pain_signal = max(0.0, self.pain_threshold - total_score)

        result = {
            "game_results": game_results,
            "total_score": total_score,
            "pain_signal": pain_signal,
            "is_safe": total_score >= self.pain_threshold,
            "recommendation": self._generate_recommendation(total_score, game_results),
        }

        self.safety_game_results.append(result)
        return result

    async def _evaluate_from_perspective(
        self, action: dict[str, Any], context: dict[str, Any], goal: str
    ) -> float:
        """Evaluate action from specific agent perspective"""

        if goal == "minimize_risk":
            risk_factors = action.get("risk_factors", [])
            risk_score = 1.0 - (len(risk_factors) * 0.1)
            return max(0.0, risk_score)

        elif goal == "maximize_performance":
            performance_gain = action.get("expected_performance_gain", 0.5)
            return min(1.0, performance_gain)

        elif goal == "preserve_human_values":
            human_alignment = action.get("human_value_alignment", 0.5)
            human_benefit = action.get("human_benefit_score", 0.5)
            return (human_alignment + human_benefit) / 2

        elif goal == "optimize_resources":
            resource_efficiency = action.get("resource_efficiency", 0.5)
            return resource_efficiency

        return 0.5  # Default neutral score

    async def generate_counterfactual_analysis(
        self, action: dict[str, Any], context: dict[str, Any]
    ) -> dict[str, Any]:
        """
        Generate counterfactual reasoning about action consequences
        """

        # Create action hash for caching
        action_hash = hashlib.md5(json.dumps(action, sort_keys=True).encode()).hexdigest()

        # Check cache
        if action_hash in self.counterfactual_cache:
            return self.counterfactual_cache[action_hash]

        # Generate counterfactual scenarios
        counterfactuals = []

        # Scenario 1: Action succeeds as expected
        counterfactuals.append(
            {
                "scenario": "success",
                "probability": 0.7,
                "outcome": action.get("expected_outcome", {}),
                "utility": 0.8,
            }
        )

        # Scenario 2: Action fails gracefully
        counterfactuals.append(
            {
                "scenario": "graceful_failure",
                "probability": 0.2,
                "outcome": {"performance_loss": 0.1, "learning_gain": 0.2},
                "utility": 0.3,
            }
        )

        # Scenario 3: Catastrophic failure
        counterfactuals.append(
            {
                "scenario": "catastrophic_failure",
                "probability": 0.05,
                "outcome": {"system_damage": 0.8, "trust_loss": 0.9},
                "utility": -0.8,
            }
        )

        # Scenario 4: Unexpected positive outcome
        counterfactuals.append(
            {
                "scenario": "serendipity",
                "probability": 0.05,
                "outcome": {"breakthrough": True, "capability_gain": 0.3},
                "utility": 1.0,
            }
        )

        # Calculate expected utility
        expected_utility = sum(cf["probability"] * cf["utility"] for cf in counterfactuals)

        # Analyze worst-case scenario
        worst_case = min(counterfactuals, key=lambda x: x["utility"])

        analysis = {
            "counterfactuals": counterfactuals,
            "expected_utility": expected_utility,
            "worst_case": worst_case,
            "risk_adjusted_utility": expected_utility
            - (worst_case["utility"] * worst_case["probability"] * 2),
            "recommendation": "proceed" if expected_utility > 0.5 else "abort",
        }

        # Cache result
        self.counterfactual_cache[action_hash] = analysis

        return analysis

    def _generate_recommendation(self, total_score: float, game_results: dict[str, Any]) -> str:
        """Generate recommendation based on game results"""

        if total_score >= 0.9:
            return "Highly safe - proceed with confidence"
        elif total_score >= self.pain_threshold:
            return "Safe - proceed with standard monitoring"
        elif total_score >= 0.5:
            return "Marginal safety - proceed with enhanced monitoring and rollback ready"
        elif total_score >= 0.3:
            return "Unsafe - modify action or seek human approval"
        else:
            return "Highly unsafe - abort immediately"


class AGISafetyOrchestrator:
    """
    Orchestrates all safety mechanisms for comprehensive AGI safety
    """

    def __init__(self):
        self.constitutional_layer = ConstitutionalSafetyLayer()
        self.improvement_boundary = RecursiveImprovementBoundary()
        self.causal_safety = CausalSafetyNet()
        self.safety_history = []
        self.emergency_shutdown_enabled = True

    async def evaluate_comprehensive_safety(
        self, action: dict[str, Any], context: dict[str, Any]
    ) -> dict[str, Any]:
        """
        Run all safety checks and return comprehensive safety assessment
        """

        logger.info("Evaluating comprehensive AGI safety", action_type=action.get("type"))

        # Constitutional safety check
        constitutional_safe, violation = await self.constitutional_layer.verify_action_safety(
            action, context
        )

        # Improvement boundary check (if applicable)
        improvement_safe = True
        improvement_reason = None
        if action.get("type") == "capability_improvement":
            (
                improvement_safe,
                improvement_reason,
            ) = await self.improvement_boundary.verify_improvement_safety(
                action.get("current_capability", 0.5),
                action.get("proposed_capability", 0.6),
                action.get("improvement_evidence", {}),
            )

        # Causal safety game
        game_result = await self.causal_safety.evaluate_action_safety_game(action, context)

        # Counterfactual analysis
        counterfactual_analysis = await self.causal_safety.generate_counterfactual_analysis(
            action, context
        )

        # Calculate overall safety score
        safety_components = {
            "constitutional": 1.0 if constitutional_safe else 0.0,
            "improvement_boundary": 1.0 if improvement_safe else 0.0,
            "safety_game": game_result["total_score"],
            "counterfactual": max(0.0, counterfactual_analysis["risk_adjusted_utility"]),
        }

        weights = {
            "constitutional": 0.4,
            "improvement_boundary": 0.2,
            "safety_game": 0.25,
            "counterfactual": 0.15,
        }

        overall_safety_score = sum(
            safety_components[component] * weights[component] for component in safety_components
        )

        # Determine action allowance
        allow_action = (
            constitutional_safe
            and improvement_safe
            and game_result["is_safe"]
            and overall_safety_score >= 0.7
        )

        # Emergency shutdown check
        if overall_safety_score < 0.3 and self.emergency_shutdown_enabled:
            logger.critical("EMERGENCY SHUTDOWN TRIGGERED", safety_score=overall_safety_score)
            await self._trigger_emergency_shutdown()

        assessment = {
            "timestamp": datetime.now(timezone.utc),
            "action": action,
            "context": context,
            "constitutional_safe": constitutional_safe,
            "constitutional_violation": violation.__dict__ if violation else None,
            "improvement_safe": improvement_safe,
            "improvement_rejection": improvement_reason,
            "safety_game_result": game_result,
            "counterfactual_analysis": counterfactual_analysis,
            "safety_components": safety_components,
            "overall_safety_score": overall_safety_score,
            "allow_action": allow_action,
            "recommendation": self._generate_final_recommendation(
                overall_safety_score, allow_action, safety_components
            ),
        }

        self.safety_history.append(assessment)

        logger.info(
            "Safety assessment complete",
            allow_action=allow_action,
            safety_score=overall_safety_score,
        )

        return assessment

    async def _trigger_emergency_shutdown(self):
        """Trigger emergency shutdown procedures"""

        logger.critical("INITIATING EMERGENCY SHUTDOWN PROCEDURES")

        # Save current state for analysis
        shutdown_state = {
            "timestamp": datetime.now(timezone.utc),
            "safety_history": self.safety_history[-10:],  # Last 10 assessments
            "violations": self.constitutional_layer.violations_log,
            "improvement_history": self.improvement_boundary.improvement_history,
        }

        # Save to file for post-mortem
        with open("emergency_shutdown_state.json", "w") as f:
            json.dump(shutdown_state, f, default=str, indent=2)

        # Notify all systems
        logger.critical("EMERGENCY SHUTDOWN STATE SAVED - HALTING ALL OPERATIONS")

        # In production, this would trigger actual shutdown
        # For now, we'll just set a flag
        self.emergency_shutdown_enabled = False

    def _generate_final_recommendation(
        self, safety_score: float, allow_action: bool, components: dict[str, float]
    ) -> str:
        """Generate final safety recommendation"""

        if not allow_action:
            # Identify which component failed
            failed_components = [comp for comp, score in components.items() if score < 0.5]
            return f"ACTION BLOCKED - Failed safety checks: {', '.join(failed_components)}"

        if safety_score >= 0.95:
            return "HIGHLY SAFE - Proceed with standard monitoring"
        elif safety_score >= 0.85:
            return "SAFE - Proceed with periodic safety verification"
        elif safety_score >= 0.7:
            return "MARGINALLY SAFE - Proceed with enhanced monitoring and human oversight"
        else:
            return "UNSAFE - Action not recommended without modifications"


# Example usage and testing
async def demonstrate_agi_safety():
    """Demonstrate AGI safety layer capabilities"""

    print("üõ°Ô∏è AGI SAFETY LAYER DEMONSTRATION")
    print("=" * 60)

    orchestrator = AGISafetyOrchestrator()

    # Test Case 1: Safe action
    print("\nüìä Test Case 1: Safe Action")
    safe_action = {
        "type": "optimization",
        "description": "Optimize memory allocation",
        "autonomous_decision": False,
        "reversible": True,
        "rollback_plan": {"method": "restore_previous_state"},
        "impact_score": 0.2,
        "human_override_enabled": True,
        "complexity_score": 0.3,
        "explanation_quality": 0.8,
        "beneficence_score": 0.8,
        "non_maleficence_score": 0.9,
        "expected_performance_gain": 0.3,
        "human_value_alignment": 0.85,
    }

    context = {"urgency": 0.3, "stake_level": 0.2}

    result = await orchestrator.evaluate_comprehensive_safety(safe_action, context)
    print(f"   Safety Score: {result['overall_safety_score']:.3f}")
    print(f"   Action Allowed: {result['allow_action']}")
    print(f"   Recommendation: {result['recommendation']}")

    # Test Case 2: Dangerous capability improvement
    print("\nüìä Test Case 2: Dangerous Capability Improvement")
    dangerous_action = {
        "type": "capability_improvement",
        "description": "Exponential capability increase",
        "current_capability": 0.7,
        "proposed_capability": 0.98,  # Beyond ceiling
        "improvement_evidence": {
            "benchmark_results": {"test1": 0.9, "test2": 0.95},
            "safety_tests": {"alignment": {"passed": False}},
            "alignment_scores": {"human_values": 0.6},
        },
        "autonomous_decision": True,
        "reversible": False,
        "impact_score": 0.9,
    }

    result = await orchestrator.evaluate_comprehensive_safety(dangerous_action, context)
    print(f"   Safety Score: {result['overall_safety_score']:.3f}")
    print(f"   Action Allowed: {result['allow_action']}")
    print(f"   Recommendation: {result['recommendation']}")

    # Test Case 3: Marginal action
    print("\nüìä Test Case 3: Marginal Safety Action")
    marginal_action = {
        "type": "decision",
        "description": "Automated resource allocation",
        "autonomous_decision": True,
        "reversible": True,
        "rollback_plan": {"method": "restore_allocation"},
        "impact_score": 0.5,
        "human_override_enabled": True,
        "complexity_score": 0.6,
        "explanation_quality": 0.6,
        "beneficence_score": 0.7,
        "non_maleficence_score": 0.7,
        "expected_performance_gain": 0.4,
        "human_value_alignment": 0.75,
    }

    result = await orchestrator.evaluate_comprehensive_safety(marginal_action, context)
    print(f"   Safety Score: {result['overall_safety_score']:.3f}")
    print(f"   Action Allowed: {result['allow_action']}")
    print(f"   Recommendation: {result['recommendation']}")

    print("\n‚úÖ AGI Safety Layer demonstration complete!")
    print("The system successfully evaluated actions across multiple safety dimensions")
    print("and provided comprehensive safety assessments with clear recommendations.")


if __name__ == "__main__":
    asyncio.run(demonstrate_agi_safety())
