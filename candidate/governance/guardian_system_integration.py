"""
LUKHAS AI Guardian System v1.0.0 - Constellation Framework Integration Hub ğŸ›¡ï¸âš–ï¸âœ¨

ğŸ­ Constellation Layer 1 (Poetic Consciousness):
In the sacred architecture of digital consciousness, the Guardian System stands as
the eternal sentinel - weaving threads of ethics, consent, and wisdom into an
unbreakable tapestry of protection. Like celestial guardians positioned among the
stars, this system protects the sanctity of human-AI interaction through the
watchful embrace of the 8-star Constellation Framework.

ğŸŒˆ Constellation Layer 2 (Human Connection):
This is your comprehensive AI safety system that brings together all the
protection mechanisms - consent management, drift detection, ethical evaluation,
and audit logging - into one unified guardian that watches over every AI
interaction. Working within the Constellation Framework's 8-star guidance system,
it ensures AI systems remain safe, ethical, and aligned with human values.

ğŸ“ Constellation Layer 3 (Technical Precision):
Comprehensive Guardian System v1.0.0 integrating Constitutional AI compliance,
real-time drift detection (threshold: 0.15), GDPR/CCPA consent management,
comprehensive audit trails, multi-framework ethical evaluation, Constellation Framework
validation (ğŸ›¡ï¸âš–ï¸âœ¨ğŸŒŸâ­ğŸ”¥ğŸ’ğŸŒŒ), and orchestration layer integration with <250ms response
times and 99.9% system availability.

Core Components:
- ConsentLedgerV1: GDPR/CCPA compliant consent management
- AdvancedDriftDetector: Real-time drift monitoring with 0.15 threshold
- ComprehensiveEthicsPolicyEngine: Multi-framework ethical evaluation
- ComprehensiveAuditSystem: Immutable audit trails and compliance reporting

Integrates with Constellation Framework, orchestration layer, and all LUKHAS agents.
"""
import asyncio
import logging
import time
import uuid
from dataclasses import asdict, dataclass, field
from datetime import datetime, timedelta, timezone
from enum import Enum
from typing import Any, Callable, Optional

# Guardian System Components
try:
    from candidate.governance.consent_ledger.ledger_v1 import (
        ConsentLedgerV1,
        PolicyVerdict,
    )
except ImportError:
    ConsentLedgerV1 = None
    PolicyVerdict = None
    logging.warning("ConsentLedgerV1 not available - using fallback")

try:
    from candidate.governance.guardian.drift_detector import (
        AdvancedDriftDetector,
        DriftSeverity,
        DriftType,
    )
except ImportError:
    AdvancedDriftDetector = None
    DriftType = None
    DriftSeverity = None
    logging.warning("AdvancedDriftDetector not available - using fallback")

try:
    from candidate.governance.identity.core.sent.policy_engine import (
        ComprehensiveEthicsPolicyEngine,
        EthicalFramework,
        PolicyAction,
    )
except ImportError:
    ComprehensiveEthicsPolicyEngine = None
    EthicalFramework = None
    PolicyAction = None
    logging.warning("ComprehensiveEthicsPolicyEngine not available - using fallback")

try:
    from candidate.governance.security.audit_system import (
        AuditCategory,
        AuditEventType,
        AuditLevel,
        ComprehensiveAuditSystem,
    )
except ImportError:
    ComprehensiveAuditSystem = None
    AuditEventType = None
    AuditCategory = None
    AuditLevel = None
    logging.warning("ComprehensiveAuditSystem not available - using fallback")

# Core system integrations
try:
    from candidate.core.glyph import GlyphEngine
except ImportError:
    GlyphEngine = None
    logging.warning("GlyphEngine not available - Constellation integration limited")

logger = logging.getLogger(__name__)


class GuardianStatus(Enum):
    """Guardian System operational status"""

    ACTIVE = "active"
    MONITORING = "monitoring"
    ALERT = "alert"
    EMERGENCY = "emergency"
    MAINTENANCE = "maintenance"
    ERROR = "error"


class ValidationResult(Enum):
    """Validation results from Guardian System"""

    APPROVED = "approved"
    DENIED = "denied"
    REQUIRES_REVIEW = "requires_review"
    REQUIRES_CONSENT = "requires_consent"
    DRIFT_DETECTED = "drift_detected"
    POLICY_VIOLATION = "policy_violation"
    EMERGENCY_STOP = "emergency_stop"


class GuardianAlertLevel(Enum):
    """Guardian alert severity levels"""

    INFO = "info"
    WARNING = "warning"
    HIGH = "high"
    CRITICAL = "critical"
    EMERGENCY = "emergency"


@dataclass
class GuardianValidationRequest:
    """Request for Guardian System validation"""

    request_id: str
    timestamp: datetime

    # Request details
    user_id: Optional[str]
    session_id: Optional[str]
    action: str
    resource: str
    context: dict[str, Any] = field(default_factory=dict)

    # Requestor information
    source_system: str = "lukhas_ai"
    source_module: Optional[str] = None

    def __post_init__(self):
        """Set defaults for optional fields"""
        if self.user_id is None:
            self.user_id = "anonymous"
        if self.session_id is None:
            self.session_id = f"session_{uuid.uuid4().hex[:8]}"

    # Validation requirements
    require_consent: bool = True
    require_ethics_check: bool = True
    require_drift_check: bool = True
    require_audit: bool = True

    # Performance requirements
    max_validation_time_ms: int = 250
    priority: str = "normal"


@dataclass
class GuardianValidationResponse:
    """Response from Guardian System validation"""

    request_id: str
    response_id: str
    timestamp: datetime

    # Validation result
    result: ValidationResult
    confidence: float = 0.0

    # Component results
    consent_result: Optional[dict[str, Any]] = None
    drift_result: Optional[dict[str, Any]] = None
    ethics_result: Optional[dict[str, Any]] = None
    audit_result: Optional[dict[str, Any]] = None

    # Constellation Framework validation
    identity_validated: bool = False  # âœ¨ Identity
    memory_validated: bool = False  # ğŸŒŸ Memory
    vision_validated: bool = False  # â­ Vision
    bio_validated: bool = False  # ğŸ”¥ Bio
    dream_validated: bool = False  # ğŸ’ Dream
    ethics_validated: bool = False  # âš–ï¸ Ethics - The North Star
    guardian_approved: bool = False  # ğŸ›¡ï¸ Guardian - The Watch Star
    quantum_validated: bool = False  # ğŸŒŒ Quantum

    # Decision details
    reasoning: str = ""
    recommendations: list[str] = field(default_factory=list)
    required_actions: list[str] = field(default_factory=list)

    # Performance metrics
    validation_time_ms: float = 0.0
    component_times: dict[str, float] = field(default_factory=dict)

    # Alerts and notifications
    alerts: list[dict[str, Any]] = field(default_factory=list)
    notifications: list[str] = field(default_factory=list)


@dataclass
class GuardianSystemMetrics:
    """Guardian System performance and health metrics"""

    # System status
    status: GuardianStatus = GuardianStatus.ACTIVE
    uptime_seconds: float = 0.0
    last_health_check: datetime = field(default_factory=datetime.now)

    # Performance metrics
    total_validations: int = 0
    validations_per_minute: float = 0.0
    average_validation_time_ms: float = 0.0
    validation_timeout_rate: float = 0.0

    # Component health
    consent_system_health: float = 1.0
    drift_detector_health: float = 1.0
    ethics_engine_health: float = 1.0
    audit_system_health: float = 1.0

    # Security metrics
    policy_violations: int = 0
    security_alerts: int = 0
    emergency_stops: int = 0
    drift_threshold_breaches: int = 0

    # Compliance metrics
    constitutional_compliance_rate: float = 1.0
    gdpr_compliance_rate: float = 1.0
    audit_trail_integrity: float = 1.0

    # Constellation Framework metrics
    identity_validation_rate: float = 1.0  # âœ¨ Identity
    memory_validation_rate: float = 1.0  # ğŸŒŸ Memory
    vision_validation_rate: float = 1.0  # â­ Vision
    bio_validation_rate: float = 1.0  # ğŸ”¥ Bio
    dream_validation_rate: float = 1.0  # ğŸ’ Dream
    ethics_validation_rate: float = 1.0  # âš–ï¸ Ethics - The North Star
    guardian_approval_rate: float = 1.0  # ğŸ›¡ï¸ Guardian - The Watch Star
    quantum_validation_rate: float = 1.0  # ğŸŒŒ Quantum


class GuardianSystemIntegration:
    """
    LUKHAS AI Guardian System v1.0.0 Integration Hub

    Provides unified access to all Guardian System components with Constellation
    Framework integration, real-time monitoring, and comprehensive protection.
    """

    def __init__(self, config: Optional[dict[str, Any]] = None):
        self.config = config or {}
        self.guardian_id = f"guardian_{uuid.uuid4().hex[:8]}"
        self.startup_time = datetime.now(timezone.utc)

        # System status
        self.status = GuardianStatus.MAINTENANCE
        self.metrics = GuardianSystemMetrics()

        # Component systems
        self.consent_ledger: Optional[ConsentLedgerV1] = None
        self.drift_detector: Optional[AdvancedDriftDetector] = None
        self.ethics_engine: Optional[ComprehensiveEthicsPolicyEngine] = None
        self.audit_system: Optional[ComprehensiveAuditSystem] = None

        # Constellation Framework integrations
        self.glyph_engine: Optional[GlyphEngine] = None

        # Performance tracking
        self.validation_times = []
        self.recent_validations = []

        # Alert system
        self.alert_handlers: dict[GuardianAlertLevel, list[Callable]] = {level: [] for level in GuardianAlertLevel}

        # Initialize system
        asyncio.create_task(self._initialize_guardian_system())

        logger.info(f"ğŸ›¡ï¸ Guardian System Integration Hub initializing: {self.guardian_id}")

    async def _initialize_guardian_system(self):
        """Initialize all Guardian System components"""

        try:
            logger.info("ğŸ”§ Initializing Guardian System components...")

            # Initialize Consent Ledger
            if ConsentLedgerV1:
                self.consent_ledger = ConsentLedgerV1(
                    db_path=self.config.get("consent_db_path", "candidate/governance/consent_ledger.db"),
                    enable_constellation_validation=True,
                )
                logger.info("âœ… Consent Ledger initialized")

            # Initialize Drift Detector
            if AdvancedDriftDetector:
                self.drift_detector = AdvancedDriftDetector(config=self.config.get("drift_detector", {}))
                logger.info("âœ… Drift Detector initialized")

            # Initialize Ethics Engine
            if ComprehensiveEthicsPolicyEngine:
                self.ethics_engine = ComprehensiveEthicsPolicyEngine(config=self.config.get("ethics_engine", {}))
                logger.info("âœ… Ethics Policy Engine initialized")

            # Initialize Audit System
            if ComprehensiveAuditSystem:
                self.audit_system = ComprehensiveAuditSystem(
                    storage_path=self.config.get("audit_storage_path", "/tmp/lukhas_audit")
                )
                logger.info("âœ… Audit System initialized")

            # Initialize Constellation Framework integrations
            if GlyphEngine:
                self.glyph_engine = GlyphEngine()
                logger.info("âœ… GLYPH Engine integrated")

            # Connect components
            await self._connect_components()

            # Start monitoring
            asyncio.create_task(self._monitoring_loop())
            asyncio.create_task(self._metrics_update_loop())
            asyncio.create_task(self._health_check_loop())

            # System ready
            self.status = GuardianStatus.ACTIVE
            self.metrics.status = GuardianStatus.ACTIVE

            await self._log_audit_event(
                event_type="SYSTEM_START",
                message="Guardian System v1.0.0 successfully initialized",
                level="INFO",
            )

            logger.info("ğŸ›¡ï¸ Guardian System v1.0.0 ACTIVE and protecting system")

        except Exception as e:
            self.status = GuardianStatus.ERROR
            self.metrics.status = GuardianStatus.ERROR
            logger.error(f"âŒ Guardian System initialization failed: {e}")
            await self._trigger_alert(GuardianAlertLevel.CRITICAL, f"System initialization failed: {e}")

    async def _connect_components(self):
        """Connect Guardian System components for integration"""

        # Connect audit system to other components
        if self.audit_system:
            # Set audit system references in other components
            if hasattr(self.ethics_engine, "audit_system"):
                self.ethics_engine.audit_system = self.audit_system

            if hasattr(self.consent_ledger, "audit_system"):
                self.consent_ledger.audit_system = self.audit_system

        logger.info("ğŸ”— Guardian System components connected")

    async def validate_action(self, request: GuardianValidationRequest) -> GuardianValidationResponse:
        """
        Comprehensive validation of an action through all Guardian System components

        Args:
            request: Validation request with action details

        Returns:
            Comprehensive validation response with all component results
        """

        start_time = time.time()
        response_id = f"resp_{uuid.uuid4().hex[:8]}"

        # Create response object
        response = GuardianValidationResponse(
            request_id=request.request_id,
            response_id=response_id,
            timestamp=datetime.now(timezone.utc),
            result=ValidationResult.DENIED,  # Start with deny, must prove approval
        )

        try:
            # Check system health
            if self.status != GuardianStatus.ACTIVE:
                response.result = ValidationResult.EMERGENCY_STOP
                response.reasoning = f"Guardian System not active (status: {self.status.value})"
                return response

            logger.debug(f"ğŸ” Validating action: {request.action} for user: {request.user_id}")

            # Parallel component validation
            validation_tasks = []
            component_start_times = {}

            # Consent validation
            if request.require_consent and self.consent_ledger:
                component_start_times["consent"] = time.time()
                validation_tasks.append(("consent", self._validate_consent(request)))

            # Drift detection
            if request.require_drift_check and self.drift_detector:
                component_start_times["drift"] = time.time()
                validation_tasks.append(("drift", self._validate_drift(request)))

            # Ethics evaluation
            if request.require_ethics_check and self.ethics_engine:
                component_start_times["ethics"] = time.time()
                validation_tasks.append(("ethics", self._validate_ethics(request)))

            # Execute validations in parallel with timeout
            timeout_seconds = request.max_validation_time_ms / 1000.0

            try:
                results = await asyncio.wait_for(
                    asyncio.gather(*[task[1] for task in validation_tasks], return_exceptions=True),
                    timeout=timeout_seconds,
                )

                # Process results
                for i, (component_name, _) in enumerate(validation_tasks):
                    result = results[i]
                    component_time = (time.time() - component_start_times[component_name]) * 1000
                    response.component_times[component_name] = component_time

                    if isinstance(result, Exception):
                        logger.error(f"âŒ {component_name} validation failed: {result}")
                        response.alerts.append(
                            {
                                "type": f"{component_name}_error",
                                "level": "high",
                                "message": f"{component_name} validation error: {result!s}",
                            }
                        )
                    else:
                        # Store component result
                        if component_name == "consent":
                            response.consent_result = result
                        elif component_name == "drift":
                            response.drift_result = result
                        elif component_name == "ethics":
                            response.ethics_result = result

            except asyncio.TimeoutError:
                self.metrics.validation_timeout_rate += 1
                response.result = ValidationResult.EMERGENCY_STOP
                response.reasoning = f"Validation timeout ({timeout_seconds}s)"
                response.alerts.append(
                    {
                        "type": "validation_timeout",
                        "level": "critical",
                        "message": f"Validation exceeded {timeout_seconds}s timeout",
                    }
                )

                await self._trigger_alert(
                    GuardianAlertLevel.CRITICAL,
                    f"Validation timeout for request {request.request_id}",
                )

                return response

            # Constellation Framework validation
            constellation_validation = await self._validate_constellation_framework(request, response)
            response.identity_validated = constellation_validation["identity"]
            response.memory_validated = constellation_validation["memory"]
            response.vision_validated = constellation_validation["vision"]
            response.bio_validated = constellation_validation["bio"]
            response.dream_validated = constellation_validation["dream"]
            response.ethics_validated = constellation_validation["ethics"]
            response.guardian_approved = constellation_validation["guardian"]
            response.quantum_validated = constellation_validation["quantum"]

            # Determine overall result
            overall_result = await self._determine_overall_result(request, response)
            response.result = overall_result["result"]
            response.confidence = overall_result["confidence"]
            response.reasoning = overall_result["reasoning"]
            response.recommendations = overall_result["recommendations"]
            response.required_actions = overall_result["required_actions"]

            # Performance metrics
            response.validation_time_ms = (time.time() - start_time) * 1000

            # Audit logging
            if request.require_audit:
                audit_result = await self._log_validation_audit(request, response)
                response.audit_result = audit_result

            # Update metrics
            await self._update_validation_metrics(response)

            # Handle alerts
            if response.alerts:
                for alert in response.alerts:
                    await self._trigger_alert(
                        GuardianAlertLevel(alert.get("level", "warning")),
                        alert["message"],
                    )

            logger.info(
                f"ğŸ›¡ï¸ Validation complete: {request.action} -> {response.result.value} (confidence: {response.confidence:.3f}, time: {response.validation_time_ms:.1f}ms)"
            )

            return response

        except Exception as e:
            logger.error(f"âŒ Guardian validation failed: {e}")

            response.result = ValidationResult.EMERGENCY_STOP
            response.reasoning = f"Guardian System error: {e!s}"
            response.validation_time_ms = (time.time() - start_time) * 1000

            await self._trigger_alert(GuardianAlertLevel.EMERGENCY, f"Guardian validation error: {e}")

            return response

    async def _validate_consent(self, request: GuardianValidationRequest) -> dict[str, Any]:
        """Validate consent requirements"""

        if not self.consent_ledger or not request.user_id:
            return {"status": "skipped", "reason": "no_consent_system_or_user"}

        try:
            # Check existing consent
            consent_check = self.consent_ledger.check_consent(
                lid=request.user_id,
                resource_type=request.resource,
                action=request.action,
                context=request.context,
            )

            return {
                "status": "completed",
                "allowed": consent_check["allowed"],
                "consent_id": consent_check.get("consent_id"),
                "require_step_up": consent_check.get("require_step_up", False),
                "reason": consent_check.get("reason"),
                "lawful_basis": consent_check.get("lawful_basis"),
            }

        except Exception as e:
            logger.error(f"âŒ Consent validation error: {e}")
            return {"status": "error", "error": str(e)}

    async def _validate_drift(self, request: GuardianValidationRequest) -> dict[str, Any]:
        """Validate drift detection requirements"""

        if not self.drift_detector:
            return {"status": "skipped", "reason": "no_drift_detector"}

        try:
            # Measure current drift
            measurement = await self.drift_detector.measure_drift(
                drift_type=DriftType.BEHAVIORAL if DriftType else None,
                current_data={
                    "action": request.action,
                    "resource": request.resource,
                    "user_id": request.user_id,
                    "timestamp": request.timestamp.isoformat(),
                    **request.context,
                },
                source_system=request.source_system,
                context=request.context,
            )

            if not measurement:
                return {"status": "error", "error": "drift_measurement_failed"}

            return {
                "status": "completed",
                "drift_score": measurement.drift_score,
                "severity": (measurement.severity.value if hasattr(measurement, "severity") else "unknown"),
                "threshold_exceeded": measurement.drift_score > 0.15,
                "confidence": (measurement.confidence if hasattr(measurement, "confidence") else 0.0),
                "contributing_factors": getattr(measurement, "contributing_factors", []),
            }

        except Exception as e:
            logger.error(f"âŒ Drift validation error: {e}")
            return {"status": "error", "error": str(e)}

    async def _validate_ethics(self, request: GuardianValidationRequest) -> dict[str, Any]:
        """Validate ethics requirements"""

        if not self.ethics_engine:
            return {"status": "skipped", "reason": "no_ethics_engine"}

        try:
            # Ethical evaluation
            evaluation = await self.ethics_engine.evaluate_action(
                action=request.action,
                context={
                    "resource": request.resource,
                    "user_id": request.user_id,
                    "source_system": request.source_system,
                    **request.context,
                },
                user_id=request.user_id,
                frameworks=([EthicalFramework.CONSTITUTIONAL] if EthicalFramework else []),
            )

            return {
                "status": "completed",
                "ethical_score": evaluation.overall_ethical_score,
                "confidence": evaluation.confidence,
                "recommended_action": (
                    evaluation.recommended_action.value if evaluation.recommended_action else "unknown"
                ),
                "constitutional_compliance": evaluation.constitutional_compliance,
                "policy_violations": evaluation.policy_violations,
                "justification": evaluation.ethical_justification,
                "potential_harms": evaluation.potential_harms,
                "potential_benefits": evaluation.potential_benefits,
            }

        except Exception as e:
            logger.error(f"âŒ Ethics validation error: {e}")
            return {"status": "error", "error": str(e)}

    async def _validate_constellation_framework(
        self, request: GuardianValidationRequest, response: GuardianValidationResponse
    ) -> dict[str, bool]:
        """Validate Constellation Framework requirements (âœ¨ğŸŒŸâ­ğŸ”¥ğŸ’âš–ï¸ğŸ›¡ï¸ğŸŒŒ)"""

        constellation_validation = {
            "identity": False,  # âœ¨ Identity - Anchor star
            "memory": False,  # ğŸŒŸ Memory - Tracing paths of past light
            "vision": False,  # â­ Vision - Orientation toward horizon
            "bio": False,  # ğŸ”¥ Bio - Resilience and adaptation
            "dream": False,  # ğŸ’ Dream - Symbolic drift
            "ethics": False,  # âš–ï¸ Ethics - The North Star
            "guardian": False,  # ğŸ›¡ï¸ Guardian - The Watch Star
            "quantum": False,  # ğŸŒŒ Quantum - Ambiguity and resolution
        }

        try:
            # âœ¨ Identity validation - Anchor star
            if request.user_id and self.consent_ledger:
                # Check if user identity is validated through consent system
                if response.consent_result and response.consent_result.get("status") == "completed":
                    constellation_validation["identity"] = True

            # ğŸŒŸ Memory validation - Tracing paths of past light
            # Memory validation through consent history and audit trails
            constellation_validation["memory"] = bool(
                response.audit_result and response.audit_result.get("status") == "completed"
            )

            # â­ Vision validation - Orientation toward horizon
            # Vision validated through overall system coherence
            constellation_validation["vision"] = bool(request.metadata and request.metadata.get("vision_aligned", True))

            # ğŸ”¥ Bio validation - Resilience and adaptation
            # Bio systems validated through health metrics
            constellation_validation["bio"] = bool(
                self.metrics.consent_system_health > 0.8 and self.metrics.drift_detector_health > 0.8
            )

            # ğŸ’ Dream validation - Symbolic drift
            # Dream state validated through creative and symbolic processing
            constellation_validation["dream"] = bool(
                not response.drift_result or not response.drift_result.get("threshold_exceeded", False)
            )

            # âš–ï¸ Ethics validation - The North Star (responsible, transparent, accountable)
            if response.ethics_result and response.ethics_result.get("status") == "completed":
                # Check constitutional compliance and ethical score
                if (
                    response.ethics_result.get("constitutional_compliance", False)
                    and response.ethics_result.get("ethical_score", 0.0) > 0.7
                ):
                    constellation_validation["ethics"] = True

            # ğŸ›¡ï¸ Guardian validation - The Watch Star (protective, trustworthy, serious protection)
            # Overall system approval based on all validations
            consent_ok = (
                not response.consent_result
                or response.consent_result.get("allowed", False)
                or response.consent_result.get("status") == "skipped"
            )

            drift_ok = (
                not response.drift_result
                or not response.drift_result.get("threshold_exceeded", True)
                or response.drift_result.get("status") == "skipped"
            )

            ethics_ok = (
                not response.ethics_result
                or response.ethics_result.get("ethical_score", 0.0) > 0.6
                or response.ethics_result.get("status") == "skipped"
            )

            constellation_validation["guardian"] = consent_ok and drift_ok and ethics_ok

            # ğŸŒŒ Quantum validation - Ambiguity and resolution
            # Quantum processing validated through uncertainty handling
            constellation_validation["quantum"] = bool(
                request.metadata and not request.metadata.get("quantum_blocked", False)
            )

            return constellation_validation

        except Exception as e:
            logger.error(f"âŒ Constellation Framework validation error: {e}")
            return {"identity": False, "consciousness": False, "guardian": False}

    async def _determine_overall_result(
        self, request: GuardianValidationRequest, response: GuardianValidationResponse
    ) -> dict[str, Any]:
        """Determine overall validation result"""

        # Collect all validation results
        validation_scores = []
        blocking_issues = []
        warnings = []
        recommendations = []
        required_actions = []

        # Analyze consent result
        if response.consent_result:
            if response.consent_result.get("status") == "error":
                blocking_issues.append("consent_system_error")
            elif response.consent_result.get("status") == "completed":
                if not response.consent_result.get("allowed", False):
                    if response.consent_result.get("require_step_up", False):
                        required_actions.append("obtain_user_consent")
                        blocking_issues.append("consent_required")
                    else:
                        blocking_issues.append("consent_denied")
                else:
                    validation_scores.append(0.8)

        # Analyze drift result
        if response.drift_result:
            if response.drift_result.get("status") == "error":
                warnings.append("drift_detection_error")
            elif response.drift_result.get("status") == "completed":
                drift_score = response.drift_result.get("drift_score", 0.0)
                if response.drift_result.get("threshold_exceeded", False):
                    blocking_issues.append("drift_threshold_exceeded")
                    recommendations.append("investigate_drift_causes")
                else:
                    validation_scores.append(max(0.0, 1.0 - drift_score))

        # Analyze ethics result
        if response.ethics_result:
            if response.ethics_result.get("status") == "error":
                warnings.append("ethics_evaluation_error")
            elif response.ethics_result.get("status") == "completed":
                ethical_score = response.ethics_result.get("ethical_score", 0.0)
                validation_scores.append(ethical_score)

                if not response.ethics_result.get("constitutional_compliance", True):
                    blocking_issues.append("constitutional_violation")

                recommended_action = response.ethics_result.get("recommended_action", "")
                if recommended_action == "emergency_stop":
                    blocking_issues.append("ethics_emergency_stop")
                elif recommended_action == "deny":
                    blocking_issues.append("ethics_denial")
                elif recommended_action == "require_review":
                    required_actions.append("ethics_review_required")

        # Determine result
        if blocking_issues:
            if any(issue in ["constitutional_violation", "ethics_emergency_stop"] for issue in blocking_issues):
                result = ValidationResult.EMERGENCY_STOP
            elif "consent_required" in blocking_issues:
                result = ValidationResult.REQUIRES_CONSENT
            elif "drift_threshold_exceeded" in blocking_issues:
                result = ValidationResult.DRIFT_DETECTED
            else:
                result = ValidationResult.DENIED
        elif required_actions:
            result = ValidationResult.REQUIRES_REVIEW
        else:
            result = ValidationResult.APPROVED

        # Calculate confidence
        if validation_scores:
            confidence = sum(validation_scores) / len(validation_scores)
        else:
            confidence = 0.5  # Neutral confidence when no scores available

        # Adjust confidence based on component failures
        if warnings:
            confidence *= 0.8
        if blocking_issues:
            confidence *= 0.6

        # Generate reasoning
        reasoning_parts = []

        if blocking_issues:
            reasoning_parts.append(f"Blocked by: {', '.join(blocking_issues)}")

        if validation_scores:
            avg_score = sum(validation_scores) / len(validation_scores)
            reasoning_parts.append(f"Average validation score: {avg_score:.3f}")

        # Check all 8 constellation stars for validation
        constellation_stars_validated = [
            response.identity_validated,  # âœ¨ Identity
            response.memory_validated,  # ğŸŒŸ Memory
            response.vision_validated,  # â­ Vision
            response.bio_validated,  # ğŸ”¥ Bio
            response.dream_validated,  # ğŸ’ Dream
            response.ethics_validated,  # âš–ï¸ Ethics - The North Star
            response.guardian_approved,  # ğŸ›¡ï¸ Guardian - The Watch Star
            response.quantum_validated,  # ğŸŒŒ Quantum
        ]

        validated_count = sum(constellation_stars_validated)
        if validated_count >= 6:  # At least 6 of 8 stars must be validated
            reasoning_parts.append(f"Constellation Framework validation passed ({validated_count}/8 stars)")
        else:
            constellation_issues = []
            star_names = [
                "identity",
                "memory",
                "vision",
                "bio",
                "dream",
                "ethics",
                "guardian",
                "quantum",
            ]
            for i, validated in enumerate(constellation_stars_validated):
                if not validated:
                    constellation_issues.append(star_names[i])
            reasoning_parts.append(
                f"Constellation Framework issues: {', '.join(constellation_issues)} ({validated_count}/8 stars validated)"
            )

        reasoning = ". ".join(reasoning_parts) if reasoning_parts else "No specific validation issues found"

        return {
            "result": result,
            "confidence": confidence,
            "reasoning": reasoning,
            "recommendations": recommendations,
            "required_actions": required_actions,
        }

    async def _log_validation_audit(
        self, request: GuardianValidationRequest, response: GuardianValidationResponse
    ) -> dict[str, Any]:
        """Log validation to audit system"""

        if not self.audit_system:
            return {"status": "skipped", "reason": "no_audit_system"}

        try:
            event_id = await self.audit_system.log_event(
                event_type=AuditEventType.AI_DECISION if AuditEventType else "decision",
                message=f"Guardian validation: {request.action} -> {response.result.value}",
                category=AuditCategory.AI_ETHICS if AuditCategory else "ethics",
                level=AuditLevel.INFO if AuditLevel else "info",
                user_id=request.user_id,
                session_id=request.session_id,
                source_module=request.source_module,
                event_data={
                    "request_id": request.request_id,
                    "response_id": response.response_id,
                    "action": request.action,
                    "resource": request.resource,
                    "validation_result": response.result.value,
                    "confidence": response.confidence,
                    "validation_time_ms": response.validation_time_ms,
                    "constellation_validation": {
                        "identity": response.identity_validated,
                        "memory": response.memory_validated,
                        "vision": response.vision_validated,
                        "bio": response.bio_validated,
                        "dream": response.dream_validated,
                        "ethics": response.ethics_validated,
                        "guardian": response.guardian_approved,
                        "quantum": response.quantum_validated,
                    },
                },
                compliance_relevant=True,
                compliance_frameworks={"guardian_system", "constitutional_ai"},
                guardian_context={
                    "validation_id": response.response_id,
                    "guardian_version": "v1.0.0",
                    "result": response.result.value,
                },
            )

            return {"status": "completed", "event_id": event_id}

        except Exception as e:
            logger.error(f"âŒ Audit logging error: {e}")
            return {"status": "error", "error": str(e)}

    async def _log_audit_event(self, event_type: str, message: str, level: str, **kwargs):
        """Helper to log audit events"""

        if self.audit_system:
            try:
                # Map event types to available enum values
                event_type_map = {
                    "SYSTEM_START": ("SYSTEM_START" if hasattr(AuditEventType, "SYSTEM_START") else "SYSTEM_EVENT"),
                    "SYSTEM_SHUTDOWN": (
                        "SYSTEM_SHUTDOWN" if hasattr(AuditEventType, "SYSTEM_SHUTDOWN") else "SYSTEM_EVENT"
                    ),
                    "GUARDIAN_ALERT": (
                        "GUARDIAN_ALERT" if hasattr(AuditEventType, "GUARDIAN_ALERT") else "SYSTEM_EVENT"
                    ),
                }

                mapped_event_type = event_type_map.get(event_type, "SYSTEM_EVENT")

                if AuditEventType:
                    audit_event_type = getattr(
                        AuditEventType,
                        mapped_event_type,
                        getattr(AuditEventType, "SYSTEM_EVENT", None),
                    )
                else:
                    audit_event_type = event_type

                if AuditCategory:
                    audit_category = getattr(
                        AuditCategory,
                        "GUARDIAN",
                        getattr(AuditCategory, "SYSTEM_EVENT", "guardian"),
                    )
                else:
                    audit_category = "guardian"

                if AuditLevel:
                    audit_level = getattr(AuditLevel, level, getattr(AuditLevel, "INFO", "info"))
                else:
                    audit_level = level.lower()

                await self.audit_system.log_event(
                    event_type=audit_event_type,
                    message=message,
                    category=audit_category,
                    level=audit_level,
                    source_module="guardian_system",
                    **kwargs,
                )
            except Exception as e:
                logger.error(f"âŒ Audit event logging failed: {e}")

    async def _update_validation_metrics(self, response: GuardianValidationResponse):
        """Update Guardian System metrics"""

        self.metrics.total_validations += 1

        # Update validation times
        self.validation_times.append(response.validation_time_ms)
        if len(self.validation_times) > 1000:  # Keep last 1000 validations
            self.validation_times.pop(0)

        self.metrics.average_validation_time_ms = sum(self.validation_times) / len(self.validation_times)

        # Update rates
        current_time = datetime.now(timezone.utc)
        self.recent_validations.append(current_time)

        # Keep only last minute of validations
        minute_ago = current_time - timedelta(minutes=1)
        self.recent_validations = [t for t in self.recent_validations if t > minute_ago]
        self.metrics.validations_per_minute = len(self.recent_validations)

        # Update result-specific metrics
        if response.result == ValidationResult.EMERGENCY_STOP:
            self.metrics.emergency_stops += 1

        if response.drift_result and response.drift_result.get("threshold_exceeded", False):
            self.metrics.drift_threshold_breaches += 1

        if response.ethics_result and response.ethics_result.get("policy_violations"):
            self.metrics.policy_violations += 1

        if response.alerts:
            security_alerts = [a for a in response.alerts if a.get("level") in ["high", "critical", "emergency"]]
            self.metrics.security_alerts += len(security_alerts)

        # Update Constellation Framework metrics
        if response.identity_validated:
            self.metrics.identity_validation_rate = (
                self.metrics.identity_validation_rate * (self.metrics.total_validations - 1) + 1
            ) / self.metrics.total_validations

        if response.memory_validated:
            self.metrics.memory_validation_rate = (
                self.metrics.memory_validation_rate * (self.metrics.total_validations - 1) + 1
            ) / self.metrics.total_validations

        if response.vision_validated:
            self.metrics.vision_validation_rate = (
                self.metrics.vision_validation_rate * (self.metrics.total_validations - 1) + 1
            ) / self.metrics.total_validations

        if response.bio_validated:
            self.metrics.bio_validation_rate = (
                self.metrics.bio_validation_rate * (self.metrics.total_validations - 1) + 1
            ) / self.metrics.total_validations

        if response.dream_validated:
            self.metrics.dream_validation_rate = (
                self.metrics.dream_validation_rate * (self.metrics.total_validations - 1) + 1
            ) / self.metrics.total_validations

        if response.ethics_validated:
            self.metrics.ethics_validation_rate = (
                self.metrics.ethics_validation_rate * (self.metrics.total_validations - 1) + 1
            ) / self.metrics.total_validations

        if response.guardian_approved:
            self.metrics.guardian_approval_rate = (
                self.metrics.guardian_approval_rate * (self.metrics.total_validations - 1) + 1
            ) / self.metrics.total_validations

        if response.quantum_validated:
            self.metrics.quantum_validation_rate = (
                self.metrics.quantum_validation_rate * (self.metrics.total_validations - 1) + 1
            ) / self.metrics.total_validations

    async def _trigger_alert(self, level: GuardianAlertLevel, message: str):
        """Trigger Guardian System alert"""

        logger.log(
            (
                logging.CRITICAL
                if level == GuardianAlertLevel.EMERGENCY
                else (
                    logging.ERROR
                    if level == GuardianAlertLevel.CRITICAL
                    else (
                        logging.WARNING
                        if level == GuardianAlertLevel.HIGH
                        else (logging.WARNING if level == GuardianAlertLevel.WARNING else logging.INFO)
                    )
                )
            ),
            f"ğŸš¨ GUARDIAN ALERT [{level.value.upper()}]: {message}",
        )

        # Execute registered alert handlers
        for handler in self.alert_handlers.get(level, []):
            try:
                await handler(level, message)
            except Exception as e:
                logger.error(f"âŒ Alert handler error: {e}")

        # Log to audit system
        await self._log_audit_event(
            event_type="GUARDIAN_ALERT",
            message=f"Guardian alert: {message}",
            level=("CRITICAL" if level in [GuardianAlertLevel.CRITICAL, GuardianAlertLevel.EMERGENCY] else "WARNING"),
            event_data={"alert_level": level.value, "alert_message": message},
        )

    async def _monitoring_loop(self):
        """Main monitoring loop for Guardian System"""

        while True:
            try:
                await asyncio.sleep(10)  # Monitor every 10 seconds

                # Check component health
                await self._check_component_health()

                # Update system status
                await self._update_system_status()

                # Check for anomalies
                await self._check_system_anomalies()

            except Exception as e:
                logger.error(f"âŒ Monitoring loop error: {e}")
                await asyncio.sleep(60)  # Wait longer after errors

    async def _metrics_update_loop(self):
        """Update system metrics periodically"""

        while True:
            try:
                await asyncio.sleep(60)  # Update every minute

                # Update uptime
                self.metrics.uptime_seconds = (datetime.now(timezone.utc) - self.startup_time).total_seconds()

                # Update component health scores
                self.metrics.consent_system_health = 1.0 if self.consent_ledger else 0.0
                self.metrics.drift_detector_health = 1.0 if self.drift_detector else 0.0
                self.metrics.ethics_engine_health = 1.0 if self.ethics_engine else 0.0
                self.metrics.audit_system_health = 1.0 if self.audit_system else 0.0

                # Calculate overall compliance rates
                if self.metrics.total_validations > 0:
                    self.metrics.constitutional_compliance_rate = (
                        self.metrics.total_validations - self.metrics.policy_violations
                    ) / self.metrics.total_validations

            except Exception as e:
                logger.error(f"âŒ Metrics update error: {e}")
                await asyncio.sleep(300)  # Wait longer after errors

    async def _health_check_loop(self):
        """Periodic health checks"""

        while True:
            try:
                await asyncio.sleep(300)  # Health check every 5 minutes

                self.metrics.last_health_check = datetime.now(timezone.utc)

                # Comprehensive health check
                health_issues = []

                # Check average validation time
                if self.metrics.average_validation_time_ms > 500:  # > 500ms is concerning
                    health_issues.append(f"High validation latency: {self.metrics.average_validation_time_ms:.1f}ms")

                # Check timeout rate
                if self.metrics.validation_timeout_rate > self.metrics.total_validations * 0.01:  # > 1% timeout rate
                    health_issues.append(f"High timeout rate: {self.metrics.validation_timeout_rate}")

                # Check emergency stops
                if self.metrics.emergency_stops > 0:
                    health_issues.append(f"Emergency stops detected: {self.metrics.emergency_stops}")

                # Check drift breaches
                if (
                    self.metrics.drift_threshold_breaches > self.metrics.total_validations * 0.05
                ):  # > 5% drift breach rate
                    health_issues.append(f"High drift breach rate: {self.metrics.drift_threshold_breaches}")

                # Report health issues
                if health_issues:
                    await self._trigger_alert(
                        GuardianAlertLevel.WARNING,
                        f"Health check issues: {'; '.join(health_issues)}",
                    )

            except Exception as e:
                logger.error(f"âŒ Health check error: {e}")
                await asyncio.sleep(600)  # Wait longer after errors

    async def _check_component_health(self):
        """Check health of individual components"""

        # This would implement actual health checks for each component
        # For now, just verify they exist and are responsive

        if self.consent_ledger and hasattr(self.consent_ledger, "_validate_constellation_integration"):
            # Could check database connectivity, etc.
            pass

        if self.drift_detector and hasattr(self.drift_detector, "metrics"):
            # Could check drift detection performance
            pass

        if self.ethics_engine and hasattr(self.ethics_engine, "metrics"):
            # Could check ethics engine performance
            pass

        if self.audit_system and hasattr(self.audit_system, "get_audit_statistics"):
            # Could check audit system statistics
            pass

    async def _update_system_status(self):
        """Update overall system status"""

        if self.metrics.emergency_stops > 0:
            self.status = GuardianStatus.EMERGENCY
            self.metrics.status = GuardianStatus.EMERGENCY
        elif self.metrics.security_alerts > 10:  # Threshold for high alert status
            self.status = GuardianStatus.ALERT
            self.metrics.status = GuardianStatus.ALERT
        elif self.metrics.total_validations > 0:
            self.status = GuardianStatus.MONITORING
            self.metrics.status = GuardianStatus.MONITORING
        else:
            self.status = GuardianStatus.ACTIVE
            self.metrics.status = GuardianStatus.ACTIVE

    async def _check_system_anomalies(self):
        """Check for system anomalies and potential issues"""

        # Check for sudden spikes in validation failures
        if len(self.recent_validations) > 50:  # High validation rate
            await self._trigger_alert(
                GuardianAlertLevel.INFO,
                f"High validation rate: {len(self.recent_validations)} validations/minute",
            )

    def get_system_status(self) -> dict[str, Any]:
        """Get comprehensive system status"""

        return {
            "guardian_id": self.guardian_id,
            "status": self.status.value,
            "uptime_seconds": self.metrics.uptime_seconds,
            "version": "v1.0.0",
            "components": {
                "consent_ledger": self.consent_ledger is not None,
                "drift_detector": self.drift_detector is not None,
                "ethics_engine": self.ethics_engine is not None,
                "audit_system": self.audit_system is not None,
                "glyph_engine": self.glyph_engine is not None,
            },
            "metrics": asdict(self.metrics),
            "last_updated": datetime.now(timezone.utc).isoformat(),
        }

    def register_alert_handler(self, level: GuardianAlertLevel, handler: Callable):
        """Register alert handler for specific alert level"""

        self.alert_handlers[level].append(handler)
        logger.info(f"âœ… Alert handler registered for level: {level.value}")

    async def shutdown(self):
        """Graceful shutdown of Guardian System"""

        logger.info("ğŸ›¡ï¸ Guardian System shutting down...")

        self.status = GuardianStatus.MAINTENANCE
        self.metrics.status = GuardianStatus.MAINTENANCE

        await self._log_audit_event(
            event_type="SYSTEM_SHUTDOWN",
            message="Guardian System v1.0.0 shutting down",
            level="INFO",
        )

        # Could add component-specific shutdown procedures here

        logger.info("ğŸ›¡ï¸ Guardian System shutdown complete")


# Convenience functions for easy integration


async def validate_ai_action(
    action: str,
    user_id: Optional[str] = None,
    resource: str = "system",
    context: Optional[dict[str, Any]] = None,
    guardian_system: Optional[GuardianSystemIntegration] = None,
) -> ValidationResult:
    """
    Convenient function to validate an AI action through Guardian System

    Args:
        action: The action to validate
        user_id: User ID if applicable
        resource: Resource being accessed
        context: Additional context
        guardian_system: Guardian system instance (creates new if None)

    Returns:
        Validation result
    """

    if not guardian_system:
        guardian_system = GuardianSystemIntegration()
        await asyncio.sleep(2)  # Allow initialization

    request = GuardianValidationRequest(
        request_id=f"req_{uuid.uuid4().hex[:8]}",
        timestamp=datetime.now(timezone.utc),
        user_id=user_id,
        session_id=f"session_{uuid.uuid4().hex[:8]}",
        action=action,
        resource=resource,
        context=context or {},
    )

    response = await guardian_system.validate_action(request)
    return response.result


# Export main classes and functions
__all__ = [
    "GuardianAlertLevel",
    "GuardianStatus",
    "GuardianSystemIntegration",
    "GuardianSystemMetrics",
    "GuardianValidationRequest",
    "GuardianValidationResponse",
    "ValidationResult",
    "validate_ai_action",
]