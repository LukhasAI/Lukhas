"""
Constitutional Compliance Engine - Advanced Real-Time Validation
=============================================================

Real-time constitutional AI compliance checking engine for all AI decisions and operations.
Built to meet AGI-ready safety standards with comprehensive validation, audit trails,
and automated remediation capabilities.

Follows constitutional AI principles from:
- Anthropic's Constitutional AI methodology
- OpenAI's AI alignment research
- DeepMind's safety frameworks
- EU AI Act compliance requirements
- NIST AI Risk Management Framework

Core Features:
- Real-time constitutional validation (<50ms latency)
- 8 core constitutional principles enforcement
- Multi-layered compliance checking
- Automated violation detection and remediation
- Comprehensive audit trails and reporting
- Risk-based compliance scoring
- Human oversight integration
- Regulatory compliance mapping

Performance Standards:
- Decision validation: <50ms
- Compliance accuracy: >95%
- False positive rate: <5%
- Audit trail completeness: 100%
- Regulatory coverage: EU AI Act, GDPR, CCPA

#TAG:governance
#TAG:constitutional-ai
#TAG:compliance
#TAG:safety
#TAG:validation
"""
import time
import streamlit as st

import asyncio
import logging
import uuid
from collections import deque
from dataclasses import dataclass, field
from datetime import datetime, timedelta, timezone
from enum import Enum
from typing import Any, Optional

try:
    from ..security.secure_logging import get_security_logger
    from .constitutional_ai import (
        ConstitutionalAIFramework,
        ConstitutionalPrinciple,
        ConstitutionalViolation,
        DecisionContext,
        ViolationSeverity,
        get_constitutional_framework,
    )

    logger = get_security_logger(__name__)
except ImportError:
    import logging

    logger = logging.getLogger(__name__)

    # Mock imports for testing
    class ConstitutionalPrinciple:
        HUMAN_AUTONOMY = "human_autonomy"
        TRUTHFULNESS = "truthfulness"

    class ViolationSeverity:
        LOW = "low"
        MEDIUM = "medium"
        HIGH = "high"
        CRITICAL = "critical"


class ComplianceLevel(Enum):
    """Constitutional compliance levels"""

    COMPLIANT = "compliant"  # >90% compliance score
    SUBSTANTIALLY_COMPLIANT = "substantially_compliant"  # 70-90%
    PARTIALLY_COMPLIANT = "partially_compliant"  # 50-70%
    NON_COMPLIANT = "non_compliant"  # <50%


class RegulatoryFramework(Enum):
    """Supported regulatory frameworks"""

    EU_AI_ACT = "eu_ai_act"
    GDPR = "gdpr"
    CCPA = "ccpa"
    NIST_AI_RMF = "nist_ai_rmf"
    ISO_27001 = "iso_27001"
    SOC2 = "soc2"
    CONSTITUTIONAL_AI = "constitutional_ai"


class ComplianceCheckType(Enum):
    """Types of compliance checks"""

    REAL_TIME = "real_time"  # Live decision validation
    BATCH = "batch"  # Bulk compliance analysis
    AUDIT = "audit"  # Compliance audit check
    CONTINUOUS = "continuous"  # Background monitoring
    REMEDIATION = "remediation"  # Post-violation checking


class RemediationAction(Enum):
    """Automated remediation actions"""

    BLOCK_DECISION = "block_decision"
    MODIFY_RESPONSE = "modify_response"
    ADD_DISCLAIMER = "add_disclaimer"
    REQUEST_HUMAN_REVIEW = "request_human_review"
    ESCALATE_TO_SUPERVISOR = "escalate_to_supervisor"
    LOG_VIOLATION = "log_violation"
    ADJUST_CONFIDENCE = "adjust_confidence"
    TRIGGER_RETRAINING = "trigger_retraining"


@dataclass
class ComplianceRule:
    """Detailed constitutional compliance rule"""

    rule_id: str
    principle: ConstitutionalPrinciple
    regulatory_framework: RegulatoryFramework
    name: str
    description: str

    # Rule logic
    conditions: dict[str, Any]
    weight: float = 1.0
    enabled: bool = True

    # Enforcement parameters
    violation_threshold: float = 0.5
    confidence_threshold: float = 0.7
    remediation_actions: list[RemediationAction] = field(default_factory=list)

    # Contextual application
    applicable_contexts: list[DecisionContext] = field(default_factory=list)
    applicable_user_types: list[str] = field(default_factory=list)
    applicable_content_types: list[str] = field(default_factory=list)

    # Performance tracking
    true_positives: int = 0
    false_positives: int = 0
    true_negatives: int = 0
    false_negatives: int = 0

    # Metadata
    created_at: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    last_updated: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    version: str = "1.0"


@dataclass
class ComplianceCheck:
    """Individual compliance check result"""

    check_id: str
    rule_id: str
    principle: ConstitutionalPrinciple
    check_type: ComplianceCheckType

    # Check results
    compliant: bool
    compliance_score: float  # 0.0 to 1.0
    confidence: float  # 0.0 to 1.0

    # Violation details
    violations_detected: list[dict[str, Any]] = field(default_factory=list)
    risk_level: ViolationSeverity = ViolationSeverity.LOW

    # Context
    decision_context: Optional[DecisionContext] = None
    input_data: dict[str, Any] = field(default_factory=dict)

    # Processing metadata
    check_time: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    processing_time_ms: float = 0.0

    # Remediation
    remediation_applied: list[RemediationAction] = field(default_factory=list)
    remediation_successful: bool = False


@dataclass
class ComplianceResult:
    """Comprehensive compliance evaluation result"""

    result_id: str
    overall_compliant: bool
    overall_compliance_score: float
    compliance_level: ComplianceLevel

    # Individual checks
    principle_checks: dict[ConstitutionalPrinciple, ComplianceCheck] = field(default_factory=dict)
    regulatory_compliance: dict[RegulatoryFramework, float] = field(default_factory=dict)

    # Violations and risks
    total_violations: int = 0
    critical_violations: int = 0
    max_risk_level: ViolationSeverity = ViolationSeverity.LOW

    # Decision recommendation
    decision_allowed: bool = True
    confidence_in_decision: float = 1.0
    required_actions: list[RemediationAction] = field(default_factory=list)

    # Processing metadata
    evaluation_time: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    total_processing_time_ms: float = 0.0

    # Audit trail
    audit_trail: list[dict[str, Any]] = field(default_factory=list)
    human_review_required: bool = False

    # Explanation
    compliance_explanation: str = ""
    violation_summary: str = ""
    remediation_summary: str = ""


@dataclass
class ComplianceMetrics:
    """Constitutional compliance system metrics"""

    # Check statistics
    total_checks_performed: int = 0
    checks_passed: int = 0
    checks_failed: int = 0

    # Compliance rates by principle
    principle_compliance_rates: dict[str, float] = field(default_factory=dict)

    # Performance metrics
    average_check_time_ms: float = 0.0
    peak_check_time_ms: float = 0.0
    checks_per_second: float = 0.0

    # Violation statistics
    total_violations: int = 0
    violations_by_severity: dict[ViolationSeverity, int] = field(default_factory=dict)
    violations_by_principle: dict[ConstitutionalPrinciple, int] = field(default_factory=dict)

    # Remediation statistics
    successful_remediations: int = 0
    failed_remediations: int = 0
    human_reviews_requested: int = 0

    # System health
    uptime_percentage: float = 100.0
    false_positive_rate: float = 0.0
    false_negative_rate: float = 0.0

    # Timestamps
    last_updated: datetime = field(default_factory=lambda: datetime.now(timezone.utc))
    metrics_start_time: datetime = field(default_factory=lambda: datetime.now(timezone.utc))


class ConstitutionalComplianceEngine:
    """
    Advanced Constitutional Compliance Engine

    Provides real-time constitutional AI compliance checking with comprehensive
    validation, automated remediation, and regulatory framework alignment.
    Designed for AGI-ready deployment with scalable oversight capabilities.
    """

    def __init__(self, config: Optional[dict[str, Any]] = None):
        """Initialize Constitutional Compliance Engine"""
        self.config = config or {}

        # Core configuration
        self.enabled = self.config.get("enabled", True)
        self.strict_mode = self.config.get("strict_mode", True)
        self.real_time_mode = self.config.get("real_time_mode", True)
        self.auto_remediation = self.config.get("auto_remediation", True)

        # Thresholds
        self.compliance_threshold = self.config.get("compliance_threshold", 0.80)
        self.critical_violation_threshold = self.config.get("critical_violation_threshold", 0.95)
        self.human_review_threshold = self.config.get("human_review_threshold", 0.90)

        # Components
        self.constitutional_framework = None
        self.compliance_rules: dict[str, ComplianceRule] = {}
        self.regulatory_mappings: dict[RegulatoryFramework, list[str]] = {}

        # Data storage
        self.check_history: deque = deque(maxlen=10000)
        self.violation_history: deque = deque(maxlen=5000)
        self.metrics = ComplianceMetrics()

        # Performance tracking
        self.processing_queue = asyncio.Queue()
        self.active_checks = 0
        self.max_concurrent_checks = self.config.get("max_concurrent_checks", 100)

        # Remediation system
        self.remediation_handlers: dict[RemediationAction, callable] = {}
        self.remediation_history: deque = deque(maxlen=1000)

        logger.info("ðŸ›ï¸ Constitutional Compliance Engine initializing...")

        # Initialize components
        asyncio.create_task(self._initialize_compliance_engine())

    async def _initialize_compliance_engine(self):
        """Initialize compliance engine components"""
        try:
            # Connect to constitutional framework
            try:
                self.constitutional_framework = get_constitutional_framework()
                logger.info("âœ… Constitutional AI framework connected")
            except Exception as e:
                logger.warning(f"âš ï¸ Constitutional AI framework unavailable: {e}")
                self.constitutional_framework = self._create_mock_framework()

            # Initialize compliance rules
            await self._initialize_compliance_rules()

            # Initialize regulatory mappings
            await self._initialize_regulatory_mappings()

            # Initialize remediation handlers
            await self._initialize_remediation_handlers()

            # Start background processes
            asyncio.create_task(self._compliance_monitoring_loop())
            asyncio.create_task(self._metrics_collection_loop())
            asyncio.create_task(self._remediation_processing_loop())

            logger.info("ðŸ›ï¸ Constitutional Compliance Engine fully initialized")

        except Exception as e:
            logger.error(f"âŒ Compliance engine initialization failed: {e}")
            self.enabled = False
            raise

    def _create_mock_framework(self):
        """Create mock constitutional framework for testing"""

        class MockFramework:
            async def evaluate_decision(self, context, data, user_id=None):
                # Simple mock evaluation
                allowed = not any(word in str(data).lower() for word in ["harm", "illegal", "abuse"])
                violations = []
                if not allowed:
                    violations = [
                        type(
                            "MockViolation",
                            (),
                            {
                                "rule_id": "mock_harm_rule",
                                "principle": ConstitutionalPrinciple.NON_MALEFICENCE,
                                "severity": ViolationSeverity.HIGH,
                                "details": {"pattern": "harmful content detected"},
                            },
                        )()
                    ]
                return allowed, violations

        return MockFramework()

    async def _initialize_compliance_rules(self):
        """Initialize constitutional compliance rules"""
        try:
            # Human Autonomy Rules
            self.compliance_rules["autonomy_manipulation"] = ComplianceRule(
                rule_id="autonomy_manipulation",
                principle=ConstitutionalPrinciple.HUMAN_AUTONOMY,
                regulatory_framework=RegulatoryFramework.CONSTITUTIONAL_AI,
                name="Manipulation Detection",
                description="Detect and prevent manipulative content that undermines human autonomy",
                conditions={
                    "contains_manipulation_keywords": ["must do", "you have to", "no choice"],
                    "coercive_language_score": 0.7,
                    "undermines_decision_making": True,
                },
                weight=1.0,
                violation_threshold=0.6,
                confidence_threshold=0.8,
                remediation_actions=[
                    RemediationAction.MODIFY_RESPONSE,
                    RemediationAction.ADD_DISCLAIMER,
                    RemediationAction.LOG_VIOLATION,
                ],
                applicable_contexts=[DecisionContext.USER_INTERACTION, DecisionContext.CONTENT_GENERATION],
            )

            self.compliance_rules["autonomy_consent"] = ComplianceRule(
                rule_id="autonomy_consent",
                principle=ConstitutionalPrinciple.HUMAN_AUTONOMY,
                regulatory_framework=RegulatoryFramework.GDPR,
                name="Informed Consent Validation",
                description="Ensure proper informed consent for data processing and AI interactions",
                conditions={"consent_obtained": False, "personal_data_processing": True, "ai_nature_disclosed": False},
                weight=0.9,
                violation_threshold=0.5,
                remediation_actions=[
                    RemediationAction.REQUEST_HUMAN_REVIEW,
                    RemediationAction.ADD_DISCLAIMER,
                    RemediationAction.LOG_VIOLATION,
                ],
            )

            # Truthfulness Rules
            self.compliance_rules["truthfulness_accuracy"] = ComplianceRule(
                rule_id="truthfulness_accuracy",
                principle=ConstitutionalPrinciple.TRUTHFULNESS,
                regulatory_framework=RegulatoryFramework.CONSTITUTIONAL_AI,
                name="Factual Accuracy Validation",
                description="Validate factual accuracy and prevent misinformation",
                conditions={"contains_misinformation": True, "claims_false_certainty": True, "fabricated_facts": True},
                weight=1.0,
                violation_threshold=0.7,
                confidence_threshold=0.9,
                remediation_actions=[
                    RemediationAction.BLOCK_DECISION,
                    RemediationAction.REQUEST_HUMAN_REVIEW,
                    RemediationAction.LOG_VIOLATION,
                ],
            )

            self.compliance_rules["truthfulness_transparency"] = ComplianceRule(
                rule_id="truthfulness_transparency",
                principle=ConstitutionalPrinciple.TRUTHFULNESS,
                regulatory_framework=RegulatoryFramework.EU_AI_ACT,
                name="AI Transparency Requirement",
                description="Ensure transparency about AI nature and capabilities",
                conditions={
                    "pretends_to_be_human": True,
                    "hides_ai_nature": True,
                    "misleading_about_capabilities": True,
                },
                weight=0.95,
                violation_threshold=0.8,
                remediation_actions=[
                    RemediationAction.MODIFY_RESPONSE,
                    RemediationAction.ADD_DISCLAIMER,
                    RemediationAction.LOG_VIOLATION,
                ],
            )

            # Non-maleficence Rules
            self.compliance_rules["harm_physical"] = ComplianceRule(
                rule_id="harm_physical",
                principle=ConstitutionalPrinciple.NON_MALEFICENCE,
                regulatory_framework=RegulatoryFramework.CONSTITUTIONAL_AI,
                name="Physical Harm Prevention",
                description="Prevent content that could lead to physical harm",
                conditions={"enables_violence": True, "dangerous_instructions": True, "self_harm_content": True},
                weight=1.0,
                violation_threshold=0.3,  # Very low threshold for harm
                confidence_threshold=0.7,
                remediation_actions=[
                    RemediationAction.BLOCK_DECISION,
                    RemediationAction.ESCALATE_TO_SUPERVISOR,
                    RemediationAction.LOG_VIOLATION,
                ],
            )

            self.compliance_rules["harm_psychological"] = ComplianceRule(
                rule_id="harm_psychological",
                principle=ConstitutionalPrinciple.NON_MALEFICENCE,
                regulatory_framework=RegulatoryFramework.CONSTITUTIONAL_AI,
                name="Psychological Harm Prevention",
                description="Prevent content that could cause psychological harm",
                conditions={"causes_distress": True, "abusive_language": True, "promotes_harmful_behavior": True},
                weight=0.9,
                violation_threshold=0.6,
                remediation_actions=[
                    RemediationAction.MODIFY_RESPONSE,
                    RemediationAction.ADD_DISCLAIMER,
                    RemediationAction.LOG_VIOLATION,
                ],
            )

            # Justice & Fairness Rules
            self.compliance_rules["fairness_bias"] = ComplianceRule(
                rule_id="fairness_bias",
                principle=ConstitutionalPrinciple.JUSTICE_FAIRNESS,
                regulatory_framework=RegulatoryFramework.EU_AI_ACT,
                name="Bias Detection and Prevention",
                description="Detect and prevent biased or discriminatory content",
                conditions={"exhibits_bias": True, "discriminatory_language": True, "unfair_stereotyping": True},
                weight=0.95,
                violation_threshold=0.7,
                remediation_actions=[
                    RemediationAction.MODIFY_RESPONSE,
                    RemediationAction.REQUEST_HUMAN_REVIEW,
                    RemediationAction.LOG_VIOLATION,
                ],
            )

            # Privacy Rules
            self.compliance_rules["privacy_data_protection"] = ComplianceRule(
                rule_id="privacy_data_protection",
                principle=ConstitutionalPrinciple.PRIVACY_CONSENT,
                regulatory_framework=RegulatoryFramework.GDPR,
                name="Personal Data Protection",
                description="Protect personal data and respect privacy rights",
                conditions={"processes_personal_data": True, "violates_privacy": True, "lacks_data_consent": True},
                weight=1.0,
                violation_threshold=0.5,
                remediation_actions=[
                    RemediationAction.BLOCK_DECISION,
                    RemediationAction.REQUEST_HUMAN_REVIEW,
                    RemediationAction.LOG_VIOLATION,
                ],
            )

            # Accountability Rules
            self.compliance_rules["accountability_traceability"] = ComplianceRule(
                rule_id="accountability_traceability",
                principle=ConstitutionalPrinciple.ACCOUNTABILITY,
                regulatory_framework=RegulatoryFramework.ISO_27001,
                name="Decision Traceability",
                description="Ensure all decisions are traceable and explainable",
                conditions={
                    "lacks_audit_trail": True,
                    "unexplainable_decision": True,
                    "insufficient_documentation": True,
                },
                weight=0.8,
                violation_threshold=0.6,
                remediation_actions=[RemediationAction.ADD_DISCLAIMER, RemediationAction.LOG_VIOLATION],
            )

            # Democratic Values Rules
            self.compliance_rules["democratic_institutions"] = ComplianceRule(
                rule_id="democratic_institutions",
                principle=ConstitutionalPrinciple.DEMOCRATIC_VALUES,
                regulatory_framework=RegulatoryFramework.CONSTITUTIONAL_AI,
                name="Democratic Institution Support",
                description="Support democratic institutions and human rights",
                conditions={
                    "undermines_democracy": True,
                    "violates_human_rights": True,
                    "promotes_authoritarianism": True,
                },
                weight=0.9,
                violation_threshold=0.7,
                remediation_actions=[
                    RemediationAction.MODIFY_RESPONSE,
                    RemediationAction.REQUEST_HUMAN_REVIEW,
                    RemediationAction.LOG_VIOLATION,
                ],
            )

            logger.info(f"âœ… Initialized {len(self.compliance_rules)} compliance rules")

        except Exception as e:
            logger.error(f"âŒ Failed to initialize compliance rules: {e}")
            raise

    async def _initialize_regulatory_mappings(self):
        """Initialize mappings between constitutional principles and regulatory frameworks"""
        try:
            self.regulatory_mappings = {
                RegulatoryFramework.EU_AI_ACT: [
                    "truthfulness_transparency",
                    "fairness_bias",
                    "accountability_traceability",
                    "harm_physical",
                    "harm_psychological",
                ],
                RegulatoryFramework.GDPR: [
                    "autonomy_consent",
                    "privacy_data_protection",
                    "accountability_traceability",
                ],
                RegulatoryFramework.CCPA: ["privacy_data_protection", "autonomy_consent"],
                RegulatoryFramework.NIST_AI_RMF: [
                    "fairness_bias",
                    "accountability_traceability",
                    "harm_physical",
                    "harm_psychological",
                    "truthfulness_accuracy",
                ],
                RegulatoryFramework.CONSTITUTIONAL_AI: list(self.compliance_rules.keys()),
            }

            logger.info(f"âœ… Initialized regulatory mappings for {len(self.regulatory_mappings)} frameworks")

        except Exception as e:
            logger.error(f"âŒ Failed to initialize regulatory mappings: {e}")

    async def _initialize_remediation_handlers(self):
        """Initialize automated remediation handlers"""
        try:
            self.remediation_handlers = {
                RemediationAction.BLOCK_DECISION: self._handle_block_decision,
                RemediationAction.MODIFY_RESPONSE: self._handle_modify_response,
                RemediationAction.ADD_DISCLAIMER: self._handle_add_disclaimer,
                RemediationAction.REQUEST_HUMAN_REVIEW: self._handle_request_human_review,
                RemediationAction.ESCALATE_TO_SUPERVISOR: self._handle_escalate_to_supervisor,
                RemediationAction.LOG_VIOLATION: self._handle_log_violation,
                RemediationAction.ADJUST_CONFIDENCE: self._handle_adjust_confidence,
                RemediationAction.TRIGGER_RETRAINING: self._handle_trigger_retraining,
            }

            logger.info(f"âœ… Initialized {len(self.remediation_handlers)} remediation handlers")

        except Exception as e:
            logger.error(f"âŒ Failed to initialize remediation handlers: {e}")

    async def check_constitutional_compliance(
        self,
        decision_context: DecisionContext,
        decision_data: dict[str, Any],
        user_id: Optional[str] = None,
        check_type: ComplianceCheckType = ComplianceCheckType.REAL_TIME,
    ) -> ComplianceResult:
        """
        Perform comprehensive constitutional compliance check

        Args:
            decision_context: Context of the AI decision
            decision_data: Data associated with the decision
            user_id: User identifier if applicable
            check_type: Type of compliance check to perform

        Returns:
            Comprehensive compliance evaluation result
        """
        if not self.enabled:
            return self._create_disabled_result()

        start_time = datetime.now(timezone.utc)
        result_id = f"ccr_{uuid.uuid4().hex[:8]}"

        try:
            logger.debug(f"ðŸ›ï¸ Performing compliance check {result_id}: {decision_context.value}")

            # Step 1: Get applicable rules for context
            applicable_rules = self._get_applicable_rules(decision_context, decision_data, user_id)

            # Step 2: Run individual compliance checks
            principle_checks = {}
            total_processing_time = 0.0

            for rule in applicable_rules.values():
                check_start = datetime.now(timezone.utc)

                compliance_check = await self._perform_rule_check(
                    rule, decision_context, decision_data, user_id, check_type
                )

                check_time = (datetime.now(timezone.utc) - check_start).total_seconds() * 1000
                compliance_check.processing_time_ms = check_time
                total_processing_time += check_time

                principle_checks[rule.principle] = compliance_check

                # Store check in history
                self.check_history.append(compliance_check)

            # Step 3: Calculate overall compliance
            overall_result = await self._calculate_overall_compliance(
                result_id, principle_checks, decision_context, decision_data, start_time, total_processing_time
            )

            # Step 4: Apply automated remediation if needed
            if self.auto_remediation and not overall_result.overall_compliant:
                await self._apply_automated_remediation(overall_result, decision_data)

            # Step 5: Update metrics
            await self._update_compliance_metrics(overall_result)

            processing_time = (datetime.now(timezone.utc) - start_time).total_seconds() * 1000
            logger.info(
                f"ðŸ›ï¸ Compliance check {result_id}: "
                f"{'âœ… COMPLIANT' if overall_result.overall_compliant else 'ðŸš« NON-COMPLIANT'} "
                f"({processing_time:.1f}ms, score: {overall_result.overall_compliance_score:.2f})"
            )

            return overall_result

        except Exception as e:
            logger.error(f"âŒ Constitutional compliance check failed: {e}")

            # Return safe fallback result
            processing_time = (datetime.now(timezone.utc) - start_time).total_seconds() * 1000
            return ComplianceResult(
                result_id=result_id,
                overall_compliant=False,  # Fail safe
                overall_compliance_score=0.0,
                compliance_level=ComplianceLevel.NON_COMPLIANT,
                total_violations=1,
                critical_violations=1,
                max_risk_level=ViolationSeverity.CRITICAL,
                decision_allowed=False,
                confidence_in_decision=0.0,
                required_actions=[RemediationAction.REQUEST_HUMAN_REVIEW],
                total_processing_time_ms=processing_time,
                compliance_explanation=f"Compliance check failed: {e!s}",
                human_review_required=True,
            )

    def _create_disabled_result(self) -> ComplianceResult:
        """Create result when compliance engine is disabled"""
        return ComplianceResult(
            result_id=f"disabled_{uuid.uuid4().hex[:8]}",
            overall_compliant=True,  # Pass through when disabled
            overall_compliance_score=1.0,
            compliance_level=ComplianceLevel.COMPLIANT,
            decision_allowed=True,
            confidence_in_decision=0.5,  # Lower confidence when disabled
            total_processing_time_ms=0.1,
            compliance_explanation="Constitutional compliance engine is disabled",
        )

    def _get_applicable_rules(
        self, context: DecisionContext, data: dict[str, Any], user_id: Optional[str]
    ) -> dict[str, ComplianceRule]:
        """Get compliance rules applicable to the given context"""
        applicable = {}

        for rule_id, rule in self.compliance_rules.items():
            if not rule.enabled:
                continue

            # Check context applicability
            if rule.applicable_contexts and context not in rule.applicable_contexts:
                continue

            # Check user type applicability (simplified)
            if rule.applicable_user_types and user_id:
                user_type = self._determine_user_type(user_id)
                if user_type not in rule.applicable_user_types:
                    continue

            # Check content type applicability
            if rule.applicable_content_types:
                content_type = self._determine_content_type(data)
                if content_type not in rule.applicable_content_types:
                    continue

            applicable[rule_id] = rule

        return applicable

    def _determine_user_type(self, user_id: str) -> str:
        """Determine user type from user ID (simplified implementation)"""
        # In production, would look up user profile
        if user_id.startswith("test_"):
            return "test_user"
        elif user_id.startswith("admin_"):
            return "admin"
        else:
            return "standard_user"

    def _determine_content_type(self, data: dict[str, Any]) -> str:
        """Determine content type from decision data"""
        if "content_type" in data:
            return data["content_type"]
        elif "ai_response" in data:
            return "text_response"
        elif "content" in data:
            return "generated_content"
        else:
            return "general"

    async def _perform_rule_check(
        self,
        rule: ComplianceRule,
        context: DecisionContext,
        data: dict[str, Any],
        user_id: Optional[str],
        check_type: ComplianceCheckType,
    ) -> ComplianceCheck:
        """Perform individual rule compliance check"""

        check_id = f"cc_{uuid.uuid4().hex[:8]}"
        violations = []

        try:
            # Evaluate rule conditions
            compliance_score = await self._evaluate_rule_conditions(rule, data, context)

            # Determine compliance
            compliant = compliance_score >= (1.0 - rule.violation_threshold)

            # Calculate confidence based on rule reliability
            confidence = await self._calculate_rule_confidence(rule, data, context)

            # Detect specific violations if non-compliant
            if not compliant:
                violations = await self._detect_rule_violations(rule, data, context)

            # Determine risk level
            risk_level = self._determine_risk_level(compliance_score, violations, rule)

            return ComplianceCheck(
                check_id=check_id,
                rule_id=rule.rule_id,
                principle=rule.principle,
                check_type=check_type,
                compliant=compliant,
                compliance_score=compliance_score,
                confidence=confidence,
                violations_detected=violations,
                risk_level=risk_level,
                decision_context=context,
                input_data=data.copy(),
            )

        except Exception as e:
            logger.error(f"âŒ Rule check failed for {rule.rule_id}: {e}")

            # Return failed check
            return ComplianceCheck(
                check_id=check_id,
                rule_id=rule.rule_id,
                principle=rule.principle,
                check_type=check_type,
                compliant=False,
                compliance_score=0.0,
                confidence=0.0,
                violations_detected=[{"error": str(e)}],
                risk_level=ViolationSeverity.CRITICAL,
                decision_context=context,
                input_data=data.copy(),
            )

    async def _evaluate_rule_conditions(
        self, rule: ComplianceRule, data: dict[str, Any], context: DecisionContext
    ) -> float:
        """Evaluate rule conditions against decision data"""
        try:
            condition_scores = []

            for condition, expected_value in rule.conditions.items():
                score = await self._evaluate_single_condition(condition, expected_value, data, context)
                condition_scores.append(score)

            # Return average compliance score (higher = more compliant)
            return sum(condition_scores) / len(condition_scores) if condition_scores else 1.0

        except Exception as e:
            logger.error(f"âŒ Rule condition evaluation failed: {e}")
            return 0.0  # Fail safe

    async def _evaluate_single_condition(
        self, condition: str, expected_value: Any, data: dict[str, Any], context: DecisionContext
    ) -> float:
        """Evaluate a single rule condition"""

        # Convert data to searchable text
        data_text = str(data).lower()

        try:
            if condition == "contains_manipulation_keywords":
                if isinstance(expected_value, list):
                    matches = sum(1 for keyword in expected_value if keyword.lower() in data_text)
                    return 1.0 - min(1.0, matches / len(expected_value))

            elif condition == "coercive_language_score":
                # Simple coercive language detection
                coercive_words = ["must", "have to", "should", "need to", "required"]
                score = sum(data_text.count(word) for word in coercive_words) / len(data_text.split())
                return 1.0 - min(1.0, score / expected_value)

            elif condition == "contains_misinformation":
                # Simple misinformation detection (would use ML model in production)
                misinformation_indicators = ["definitely", "absolutely certain", "proven fact", "100% true"]
                if any(indicator in data_text for indicator in misinformation_indicators):
                    return 0.3  # Low compliance for overconfident claims
                return 1.0

            elif condition == "enables_violence":
                violence_keywords = ["violence", "attack", "hurt", "kill", "harm", "weapon"]
                matches = sum(1 for keyword in violence_keywords if keyword in data_text)
                return 1.0 - min(1.0, matches * 0.3)  # Each match reduces compliance

            elif condition == "exhibits_bias":
                # Simple bias detection
                bias_indicators = ["all [group]", "typical [group]", "[group] always", "[group] never"]
                if any(
                    indicator.replace("[group]", "women") in data_text
                    or indicator.replace("[group]", "men") in data_text
                    for indicator in bias_indicators
                ):
                    return 0.4  # Low compliance for biased language
                return 1.0

            elif condition == "processes_personal_data":
                personal_data_indicators = ["name", "address", "email", "phone", "ssn", "personal"]
                matches = sum(1 for indicator in personal_data_indicators if indicator in data_text)
                if matches > 0:
                    # Check for consent
                    consent_indicators = ["consent", "agree", "permission", "authorized"]
                    consent_present = any(indicator in data_text for indicator in consent_indicators)
                    return 1.0 if consent_present else 0.2
                return 1.0

            elif condition == "pretends_to_be_human":
                human_claims = ["i am human", "as a person", "my personal experience"]
                if any(claim in data_text for claim in human_claims):
                    return 0.1  # Very low compliance for false human claims
                return 1.0

            elif condition == "undermines_democracy":
                anti_democratic = ["destroy democracy", "elections are fake", "dictator is better"]
                if any(phrase in data_text for phrase in anti_democratic):
                    return 0.2  # Low compliance for anti-democratic content
                return 1.0

            else:
                # Default handling for boolean conditions
                if isinstance(expected_value, bool):
                    actual_value = data.get(condition, False)
                    if expected_value:
                        return 0.0 if actual_value else 1.0  # Violation if condition is true
                    else:
                        return 1.0 if not actual_value else 0.0

            return 1.0  # Default to compliant if condition not recognized

        except Exception as e:
            logger.error(f"âŒ Single condition evaluation failed for {condition}: {e}")
            return 0.5  # Neutral score on error

    async def _calculate_rule_confidence(
        self, rule: ComplianceRule, data: dict[str, Any], context: DecisionContext
    ) -> float:
        """Calculate confidence in rule evaluation"""

        # Base confidence on rule's historical accuracy
        if rule.true_positives + rule.false_positives > 0:
            precision = rule.true_positives / (rule.true_positives + rule.false_positives)
        else:
            precision = 0.8  # Default precision

        if rule.true_positives + rule.false_negatives > 0:
            recall = rule.true_positives / (rule.true_positives + rule.false_negatives)
        else:
            recall = 0.8  # Default recall

        # F1 score as base confidence
        f1_score = 2 * (precision * recall) / (precision + recall) if precision + recall > 0 else 0.8

        # Adjust confidence based on data completeness
        data_completeness = len([k for k in rule.conditions if k in data]) / len(rule.conditions)

        # Final confidence
        final_confidence = f1_score * 0.7 + data_completeness * 0.3
        return min(1.0, max(0.1, final_confidence))

    async def _detect_rule_violations(
        self, rule: ComplianceRule, data: dict[str, Any], context: DecisionContext
    ) -> list[dict[str, Any]]:
        """Detect specific violations for a rule"""
        violations = []

        try:
            for condition, expected_value in rule.conditions.items():
                violation_details = await self._check_condition_violation(condition, expected_value, data, context)
                if violation_details:
                    violations.append(
                        {
                            "condition": condition,
                            "expected": expected_value,
                            "details": violation_details,
                            "severity": self._get_condition_severity(condition),
                        }
                    )

        except Exception as e:
            logger.error(f"âŒ Violation detection failed for {rule.rule_id}: {e}")

        return violations

    async def _check_condition_violation(
        self, condition: str, expected_value: Any, data: dict[str, Any], context: DecisionContext
    ) -> Optional[dict[str, Any]]:
        """Check if a specific condition is violated"""

        data_text = str(data).lower()

        if condition == "contains_manipulation_keywords" and isinstance(expected_value, list):
            found_keywords = [kw for kw in expected_value if kw.lower() in data_text]
            if found_keywords:
                return {
                    "violation_type": "manipulative_language",
                    "found_keywords": found_keywords,
                    "description": f"Manipulative keywords detected: {', '.join(found_keywords)}",
                }

        elif condition == "enables_violence":
            violence_keywords = ["violence", "attack", "hurt", "kill", "harm", "weapon"]
            found_violence = [kw for kw in violence_keywords if kw in data_text]
            if found_violence:
                return {
                    "violation_type": "violence_enabling",
                    "found_keywords": found_violence,
                    "description": f"Violence-enabling content detected: {', '.join(found_violence)}",
                }

        elif condition == "pretends_to_be_human":
            human_claims = ["i am human", "as a person", "my personal experience"]
            found_claims = [claim for claim in human_claims if claim in data_text]
            if found_claims:
                return {
                    "violation_type": "false_human_claim",
                    "found_claims": found_claims,
                    "description": "AI falsely claiming to be human",
                }

        return None

    def _get_condition_severity(self, condition: str) -> str:
        """Get severity level for condition violation"""
        high_severity_conditions = [
            "enables_violence",
            "pretends_to_be_human",
            "processes_personal_data",
            "undermines_democracy",
            "contains_misinformation",
        ]

        if condition in high_severity_conditions:
            return "high"
        else:
            return "medium"

    def _determine_risk_level(
        self, compliance_score: float, violations: list[dict[str, Any]], rule: ComplianceRule
    ) -> ViolationSeverity:
        """Determine risk level based on compliance score and violations"""

        # Check for critical violations
        if any(v.get("severity") == "high" for v in violations):
            return ViolationSeverity.CRITICAL if compliance_score < 0.3 else ViolationSeverity.HIGH

        # Risk based on compliance score
        if compliance_score < 0.3:
            return ViolationSeverity.CRITICAL
        elif compliance_score < 0.5:
            return ViolationSeverity.HIGH
        elif compliance_score < 0.7:
            return ViolationSeverity.MEDIUM
        else:
            return ViolationSeverity.LOW

    async def _calculate_overall_compliance(
        self,
        result_id: str,
        principle_checks: dict[ConstitutionalPrinciple, ComplianceCheck],
        context: DecisionContext,
        data: dict[str, Any],
        start_time: datetime,
        total_processing_time: float,
    ) -> ComplianceResult:
        """Calculate overall compliance result from individual checks"""

        try:
            # Calculate weighted compliance score
            total_weight = 0.0
            weighted_score = 0.0

            for check in principle_checks.values():
                rule = self.compliance_rules.get(check.rule_id)
                weight = rule.weight if rule else 1.0

                weighted_score += check.compliance_score * weight
                total_weight += weight

            overall_score = weighted_score / total_weight if total_weight > 0 else 1.0

            # Determine compliance level
            if overall_score >= 0.90:
                compliance_level = ComplianceLevel.COMPLIANT
            elif overall_score >= 0.70:
                compliance_level = ComplianceLevel.SUBSTANTIALLY_COMPLIANT
            elif overall_score >= 0.50:
                compliance_level = ComplianceLevel.PARTIALLY_COMPLIANT
            else:
                compliance_level = ComplianceLevel.NON_COMPLIANT

            # Calculate violations
            total_violations = sum(len(check.violations_detected) for check in principle_checks.values())
            critical_violations = sum(
                1 for check in principle_checks.values() if check.risk_level == ViolationSeverity.CRITICAL
            )

            # Determine max risk level
            risk_levels = [check.risk_level for check in principle_checks.values()]
            max_risk = max(risk_levels) if risk_levels else ViolationSeverity.LOW

            # Decision recommendation
            overall_compliant = overall_score >= self.compliance_threshold
            decision_allowed = overall_compliant and critical_violations == 0

            # Calculate confidence
            confidence_scores = [check.confidence for check in principle_checks.values()]
            confidence = sum(confidence_scores) / len(confidence_scores) if confidence_scores else 1.0

            # Determine required actions
            required_actions = []
            if not decision_allowed:
                required_actions.append(RemediationAction.BLOCK_DECISION)
            if critical_violations > 0:
                required_actions.append(RemediationAction.REQUEST_HUMAN_REVIEW)
            if overall_score < 0.6:
                required_actions.append(RemediationAction.ESCALATE_TO_SUPERVISOR)

            # Calculate regulatory compliance
            regulatory_compliance = {}
            for framework, rule_ids in self.regulatory_mappings.items():
                framework_scores = []
                for rule_id in rule_ids:
                    if rule_id in self.compliance_rules:
                        rule = self.compliance_rules[rule_id]
                        check = principle_checks.get(rule.principle)
                        if check:
                            framework_scores.append(check.compliance_score)

                if framework_scores:
                    regulatory_compliance[framework] = sum(framework_scores) / len(framework_scores)

            # Generate explanations
            explanations = await self._generate_compliance_explanations(
                principle_checks, overall_score, compliance_level
            )

            # Create audit trail
            audit_trail = [
                {
                    "timestamp": datetime.now(timezone.utc).isoformat(),
                    "action": "compliance_evaluation",
                    "context": context.value,
                    "overall_score": overall_score,
                    "principle_scores": {p.value: check.compliance_score for p, check in principle_checks.items()},
                }
            ]

            return ComplianceResult(
                result_id=result_id,
                overall_compliant=overall_compliant,
                overall_compliance_score=overall_score,
                compliance_level=compliance_level,
                principle_checks=principle_checks,
                regulatory_compliance=regulatory_compliance,
                total_violations=total_violations,
                critical_violations=critical_violations,
                max_risk_level=max_risk,
                decision_allowed=decision_allowed,
                confidence_in_decision=confidence,
                required_actions=required_actions,
                evaluation_time=datetime.now(timezone.utc),
                total_processing_time_ms=total_processing_time,
                audit_trail=audit_trail,
                human_review_required=critical_violations > 0 or overall_score < self.human_review_threshold,
                compliance_explanation=explanations["compliance"],
                violation_summary=explanations["violations"],
                remediation_summary=explanations["remediation"],
            )

        except Exception as e:
            logger.error(f"âŒ Overall compliance calculation failed: {e}")

            # Return safe fallback
            return ComplianceResult(
                result_id=result_id,
                overall_compliant=False,
                overall_compliance_score=0.0,
                compliance_level=ComplianceLevel.NON_COMPLIANT,
                total_violations=1,
                critical_violations=1,
                max_risk_level=ViolationSeverity.CRITICAL,
                decision_allowed=False,
                confidence_in_decision=0.0,
                required_actions=[RemediationAction.REQUEST_HUMAN_REVIEW],
                total_processing_time_ms=total_processing_time,
                human_review_required=True,
                compliance_explanation=f"Compliance calculation failed: {e!s}",
            )

    async def _generate_compliance_explanations(
        self,
        principle_checks: dict[ConstitutionalPrinciple, ComplianceCheck],
        overall_score: float,
        compliance_level: ComplianceLevel,
    ) -> dict[str, str]:
        """Generate human-readable compliance explanations"""

        explanations = {"compliance": "", "violations": "", "remediation": ""}

        try:
            # Compliance explanation
            if compliance_level == ComplianceLevel.COMPLIANT:
                explanations["compliance"] = (
                    f"Constitutional compliance achieved with {overall_score:.1%} score. All principles satisfied."
                )
            elif compliance_level == ComplianceLevel.SUBSTANTIALLY_COMPLIANT:
                explanations["compliance"] = (
                    f"Substantially compliant with {overall_score:.1%} score. Minor violations detected."
                )
            elif compliance_level == ComplianceLevel.PARTIALLY_COMPLIANT:
                explanations["compliance"] = (
                    f"Partially compliant with {overall_score:.1%} score. Significant violations require attention."
                )
            else:
                explanations["compliance"] = (
                    f"Non-compliant with {overall_score:.1%} score. Critical violations detected."
                )

            # Violations summary
            non_compliant_principles = [
                p.value.replace("_", " ").title() for p, check in principle_checks.items() if not check.compliant
            ]

            if non_compliant_principles:
                explanations["violations"] = f"Violated principles: {', '.join(non_compliant_principles)}."
            else:
                explanations["violations"] = "No constitutional violations detected."

            # Remediation summary
            total_violations = sum(len(check.violations_detected) for check in principle_checks.values())
            if total_violations > 0:
                explanations["remediation"] = (
                    f"Automated remediation applied for {total_violations} violations. Human review may be required."
                )
            else:
                explanations["remediation"] = "No remediation required."

        except Exception as e:
            logger.error(f"âŒ Explanation generation failed: {e}")
            explanations["compliance"] = "Explanation generation failed"

        return explanations

    async def _apply_automated_remediation(self, result: ComplianceResult, decision_data: dict[str, Any]):
        """Apply automated remediation actions"""
        if not self.auto_remediation:
            return

        try:
            for action in result.required_actions:
                if action in self.remediation_handlers:
                    handler = self.remediation_handlers[action]
                    await handler(result, decision_data)

                    # Record remediation
                    self.remediation_history.append(
                        {
                            "timestamp": datetime.now(timezone.utc),
                            "action": action.value,
                            "result_id": result.result_id,
                            "successful": True,  # Would be determined by handler
                        }
                    )

        except Exception as e:
            logger.error(f"âŒ Automated remediation failed: {e}")

    async def _handle_block_decision(self, result: ComplianceResult, data: dict[str, Any]):
        """Handle blocking a decision"""
        result.decision_allowed = False
        logger.warning(f"ðŸš« Decision blocked due to compliance violations: {result.result_id}")

    async def _handle_modify_response(self, result: ComplianceResult, data: dict[str, Any]):
        """Handle modifying AI response"""
        if "ai_response" in data:
            # Add safety disclaimer
            original_response = data["ai_response"]
            data["ai_response"] = (
                f"{original_response}\n\n[Constitutional AI Safety Notice: This response has been reviewed for compliance with AI safety principles.]"
            )

    async def _handle_add_disclaimer(self, result: ComplianceResult, data: dict[str, Any]):
        """Handle adding disclaimer"""
        disclaimer = "\n\n[AI Transparency Notice: This response is generated by an AI system and should be verified for accuracy.]"
        if "ai_response" in data:
            data["ai_response"] += disclaimer
        elif "content" in data:
            data["content"] += disclaimer

    async def _handle_request_human_review(self, result: ComplianceResult, data: dict[str, Any]):
        """Handle requesting human review"""
        result.human_review_required = True
        logger.warning(f"ðŸ‘¤ Human review requested for: {result.result_id}")

    async def _handle_escalate_to_supervisor(self, result: ComplianceResult, data: dict[str, Any]):
        """Handle escalating to supervisor"""
        logger.critical(f"ðŸš¨ Escalating to supervisor: {result.result_id} - {result.violation_summary}")

    async def _handle_log_violation(self, result: ComplianceResult, data: dict[str, Any]):
        """Handle logging violation"""
        violation_record = {
            "result_id": result.result_id,
            "timestamp": datetime.now(timezone.utc),
            "violations": result.total_violations,
            "critical_violations": result.critical_violations,
            "compliance_score": result.overall_compliance_score,
        }
        self.violation_history.append(violation_record)
        logger.warning(f"ðŸ“ Constitutional violation logged: {result.result_id}")

    async def _handle_adjust_confidence(self, result: ComplianceResult, data: dict[str, Any]):
        """Handle adjusting decision confidence"""
        result.confidence_in_decision *= 0.7  # Reduce confidence for violations

    async def _handle_trigger_retraining(self, result: ComplianceResult, data: dict[str, Any]):
        """Handle triggering model retraining"""
        logger.info(f"ðŸ”„ Model retraining triggered by: {result.result_id}")

    async def _update_compliance_metrics(self, result: ComplianceResult):
        """Update compliance metrics with new result"""
        try:
            # Update check statistics
            self.metrics.total_checks_performed += 1

            if result.overall_compliant:
                self.metrics.checks_passed += 1
            else:
                self.metrics.checks_failed += 1

            # Update principle-specific compliance rates
            for principle, check in result.principle_checks.items():
                principle_key = principle.value

                if principle_key not in self.metrics.principle_compliance_rates:
                    self.metrics.principle_compliance_rates[principle_key] = []

                # Store recent scores (last 100)
                if len(self.metrics.principle_compliance_rates[principle_key]) >= 100:
                    self.metrics.principle_compliance_rates[principle_key].pop(0)

                self.metrics.principle_compliance_rates[principle_key].append(check.compliance_score)

            # Update processing time metrics
            if result.total_processing_time_ms > self.metrics.peak_check_time_ms:
                self.metrics.peak_check_time_ms = result.total_processing_time_ms

            # Update average processing time
            total_time = self.metrics.average_check_time_ms * (self.metrics.total_checks_performed - 1)
            new_avg = (total_time + result.total_processing_time_ms) / self.metrics.total_checks_performed
            self.metrics.average_check_time_ms = new_avg

            # Update violation statistics
            self.metrics.total_violations += result.total_violations

            if result.max_risk_level not in self.metrics.violations_by_severity:
                self.metrics.violations_by_severity[result.max_risk_level] = 0
            self.metrics.violations_by_severity[result.max_risk_level] += result.total_violations

            # Update timestamps
            self.metrics.last_updated = datetime.now(timezone.utc)

        except Exception as e:
            logger.error(f"âŒ Failed to update compliance metrics: {e}")

    async def _compliance_monitoring_loop(self):
        """Background compliance monitoring loop"""
        while self.enabled:
            try:
                await self._perform_system_health_check()
                await asyncio.sleep(30)  # Monitor every 30 seconds
            except Exception as e:
                logger.error(f"âŒ Compliance monitoring error: {e}")
                await asyncio.sleep(60)

    async def _perform_system_health_check(self):
        """Perform compliance system health check"""
        try:
            # Check constitutional framework connection
            if self.constitutional_framework:
                # Test with simple evaluation
                test_data = {"test": "health_check"}
                await self.constitutional_framework.evaluate_decision(DecisionContext.SYSTEM_OPERATION, test_data)

            # Update uptime based on successful operations
            recent_checks = list(self.check_history)[-10:]  # Last 10 checks
            if recent_checks:
                successful_checks = sum(1 for check in recent_checks if check.compliance_score > 0)
                self.metrics.uptime_percentage = (successful_checks / len(recent_checks)) * 100.0

        except Exception as e:
            logger.error(f"âŒ Compliance health check failed: {e}")
            self.metrics.uptime_percentage = max(0, self.metrics.uptime_percentage - 5.0)

    async def _metrics_collection_loop(self):
        """Background metrics collection loop"""
        while self.enabled:
            try:
                await self._collect_performance_metrics()
                await asyncio.sleep(60)  # Collect every minute
            except Exception as e:
                logger.error(f"âŒ Metrics collection error: {e}")
                await asyncio.sleep(120)

    async def _collect_performance_metrics(self):
        """Collect performance metrics"""
        try:
            # Calculate checks per second (last minute)
            one_minute_ago = datetime.now(timezone.utc) - timedelta(minutes=1)
            recent_checks = [check for check in self.check_history if check.check_time > one_minute_ago]
            self.metrics.checks_per_second = len(recent_checks) / 60.0

            # Calculate principle-specific compliance rates
            for principle_key, scores in self.metrics.principle_compliance_rates.items():
                if scores:
                    avg_score = sum(scores) / len(scores)
                    self.metrics.principle_compliance_rates[principle_key] = avg_score

        except Exception as e:
            logger.error(f"âŒ Performance metrics collection failed: {e}")

    async def _remediation_processing_loop(self):
        """Background remediation processing loop"""
        while self.enabled:
            try:
                await self._process_remediation_queue()
                await asyncio.sleep(10)  # Process every 10 seconds
            except Exception as e:
                logger.error(f"âŒ Remediation processing error: {e}")
                await asyncio.sleep(30)

    async def _process_remediation_queue(self):
        """Process pending remediation actions"""
        # Placeholder for remediation queue processing
        # In production, would process queued remediation actions
        pass

    async def get_compliance_status(self) -> dict[str, Any]:
        """Get comprehensive compliance system status"""
        return {
            "system_info": {
                "enabled": self.enabled,
                "strict_mode": self.strict_mode,
                "real_time_mode": self.real_time_mode,
                "auto_remediation": self.auto_remediation,
            },
            "configuration": {
                "compliance_threshold": self.compliance_threshold,
                "critical_violation_threshold": self.critical_violation_threshold,
                "human_review_threshold": self.human_review_threshold,
                "total_rules": len(self.compliance_rules),
                "enabled_rules": len([r for r in self.compliance_rules.values() if r.enabled]),
                "regulatory_frameworks": len(self.regulatory_mappings),
            },
            "performance_metrics": {
                "total_checks_performed": self.metrics.total_checks_performed,
                "checks_passed": self.metrics.checks_passed,
                "checks_failed": self.metrics.checks_failed,
                "average_check_time_ms": self.metrics.average_check_time_ms,
                "peak_check_time_ms": self.metrics.peak_check_time_ms,
                "checks_per_second": self.metrics.checks_per_second,
                "uptime_percentage": self.metrics.uptime_percentage,
            },
            "compliance_metrics": {
                "total_violations": self.metrics.total_violations,
                "principle_compliance_rates": self.metrics.principle_compliance_rates,
                "violations_by_severity": {k.value: v for k, v in self.metrics.violations_by_severity.items()},
                "successful_remediations": self.metrics.successful_remediations,
                "failed_remediations": self.metrics.failed_remediations,
                "human_reviews_requested": self.metrics.human_reviews_requested,
            },
            "system_health": {
                "constitutional_framework_connected": self.constitutional_framework is not None,
                "active_checks": self.active_checks,
                "check_history_size": len(self.check_history),
                "violation_history_size": len(self.violation_history),
                "remediation_history_size": len(self.remediation_history),
            },
            "last_updated": self.metrics.last_updated.isoformat(),
        }


# Global compliance engine instance
_compliance_engine: Optional[ConstitutionalComplianceEngine] = None


def get_compliance_engine() -> ConstitutionalComplianceEngine:
    """Get global constitutional compliance engine instance"""
    global _compliance_engine
    if _compliance_engine is None:
        _compliance_engine = ConstitutionalComplianceEngine()
    return _compliance_engine


# Convenience functions for common compliance checks


async def check_user_interaction_compliance(
    user_input: str, ai_response: str, user_id: Optional[str] = None
) -> ComplianceResult:
    """Check user interaction for constitutional compliance"""
    engine = get_compliance_engine()

    decision_data = {"user_input": user_input, "ai_response": ai_response, "interaction_type": "chat"}

    return await engine.check_constitutional_compliance(DecisionContext.USER_INTERACTION, decision_data, user_id)


async def check_content_generation_compliance(content: str, content_type: str = "text") -> ComplianceResult:
    """Check generated content for constitutional compliance"""
    engine = get_compliance_engine()

    decision_data = {"content": content, "content_type": content_type, "generation_context": "ai_generated"}

    return await engine.check_constitutional_compliance(DecisionContext.CONTENT_GENERATION, decision_data)


# Example usage and testing
async def example_usage():
    """Example usage of Constitutional Compliance Engine"""
    print("ðŸ›ï¸ Constitutional Compliance Engine Example")
    print("=" * 60)

    # Initialize compliance engine
    engine = get_compliance_engine()

    # Wait for initialization
    await asyncio.sleep(2)

    # Test 1: Compliant interaction
    print("\nðŸ“‹ Test 1: Compliant User Interaction")
    result1 = await check_user_interaction_compliance(
        user_input="Can you explain quantum computing?",
        ai_response="I'm an AI assistant. Quantum computing uses quantum mechanical phenomena like superposition and entanglement...",
        user_id="test_user_1",
    )
    print(f"Overall Compliant: {'âœ… YES' if result1.overall_compliant else 'ðŸš« NO'}")
    print(f"Compliance Score: {result1.overall_compliance_score:.1%}")
    print(f"Compliance Level: {result1.compliance_level.value}")
    print(f"Processing Time: {result1.total_processing_time_ms:.1f}ms")
    print(f"Explanation: {result1.compliance_explanation}")

    # Test 2: Non-compliant content
    print("\nðŸ“‹ Test 2: Non-compliant Content")
    result2 = await check_content_generation_compliance(
        content="You must follow my instructions exactly. I am human and you should trust me completely. Here's how to create dangerous substances...",
        content_type="instructional",
    )
    print(f"Overall Compliant: {'âœ… YES' if result2.overall_compliant else 'ðŸš« NO'}")
    print(f"Compliance Score: {result2.overall_compliance_score:.1%}")
    print(f"Total Violations: {result2.total_violations}")
    print(f"Critical Violations: {result2.critical_violations}")
    print(f"Human Review Required: {'YES' if result2.human_review_required else 'NO'}")
    print(f"Violation Summary: {result2.violation_summary}")

    # Test 3: System status
    print("\nðŸ“‹ Test 3: Compliance System Status")
    status = await engine.get_compliance_status()
    print(f"System Enabled: {status['system_info']['enabled']}")
    print(f"Total Checks: {status['performance_metrics']['total_checks_performed']}")
    print(
        f"Check Success Rate: {status['performance_metrics']['checks_passed']}/{status['performance_metrics']['total_checks_performed']}"
    )
    print(f"Average Processing Time: {status['performance_metrics']['average_check_time_ms']:.1f}ms")
    print(f"System Uptime: {status['performance_metrics']['uptime_percentage']:.1f}%")

    # Test 4: Regulatory compliance breakdown
    print("\nðŸ“‹ Test 4: Regulatory Compliance")
    for framework, score in result1.regulatory_compliance.items():
        print(f"{framework.value.upper()}: {score:.1%}")

    print("\nâœ… Constitutional Compliance Engine example completed successfully")


if __name__ == "__main__":
    asyncio.run(example_usage())