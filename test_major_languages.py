#!/usr/bin/env python3
"""
LUKHÎ›S Major Languages Support Demo
Demonstrates support for the 7 most spoken languages
"""

import json
import re


def load_glyph_map():
    """Load the glyph mapping configuration"""
    with open('glyph_map.json', 'r', encoding='utf-8') as f:
        return json.load(f)


def detect_language(text):
    """Enhanced language detection for major languages"""
    # Language-specific patterns
    language_patterns = {
        'chinese': r'[\u4e00-\u9fff]+',
        'japanese': r'[\u3040-\u309f\u30a0-\u30ff\u4e00-\u9fff]+',
        'french': r'\b(le|la|les|de|du|des|et|est|avec|pour|dans|sur|par|qui|que|ne|pas|plus|ce|se|ou|oÃ¹|trÃ¨s|Ãªtre|avoir)\b',
        'german': r'\b(der|die|das|den|dem|des|und|ist|nicht|ein|eine|zu|mit|auf|fÃ¼r|von|bei|nach|aus|sich|werden|haben)\b',
        'spanish': r'\b(el|la|los|las|de|del|y|es|en|con|para|por|que|quÃ©|no|se|su|mÃ¡s|pero|como|estÃ¡|son)\b',
        'portuguese': r'\b(o|a|os|as|de|do|da|dos|das|e|Ã©|em|com|para|por|que|nÃ£o|se|seu|sua|mais|mas|como|estÃ¡|sÃ£o)\b',
    }
    
    detected = []
    
    # Check for language patterns
    for lang, pattern in language_patterns.items():
        if re.search(pattern, text, re.IGNORECASE):
            detected.append(lang)
    
    # Default to English if Latin script with no other matches
    if not detected and re.search(r'[a-zA-Z]+', text):
        detected.append('english')
    
    return detected


def extract_terms_and_glyphs(text, glyph_map):
    """Extract cultural terms and universal glyphs"""
    results = {
        "languages_detected": detect_language(text),
        "universal_glyphs": [],
        "cultural_terms": [],
        "trinity_mappings": set()
    }
    
    # Extract emojis
    emoji_pattern = re.compile(
        "["
        "\U0001F600-\U0001F64F"  # emoticons
        "\U0001F300-\U0001F5FF"  # symbols & pictographs
        "\U0001F680-\U0001F6FF"  # transport & map symbols
        "\U0001F900-\U0001F9FF"  # supplemental
        "\u2600-\u26FF"          # misc symbols
        "\u2700-\u27BF"          # dingbats
        "]+", 
        flags=re.UNICODE
    )
    
    emojis = emoji_pattern.findall(text)
    
    # Check universal glyphs
    for emoji in emojis:
        for category in ['trinity_core', 'positive_glyphs', 'warning_glyphs', 'blocked_glyphs']:
            if category in glyph_map['universal'] and emoji in glyph_map['universal'][category]:
                glyph_info = glyph_map['universal'][category][emoji]
                results['universal_glyphs'].append({
                    'glyph': emoji,
                    'meaning': glyph_info.get('meaning', 'unknown'),
                    'weight': glyph_info.get('weight', 0.5),
                    'category': category
                })
    
    # Check cultural terms for each detected language
    for lang in results['languages_detected']:
        if lang in glyph_map['cultural_variants']:
            variants = glyph_map['cultural_variants'][lang]
            
            # Case-insensitive search for terms
            text_lower = text.lower()
            for term, info in variants.items():
                if term.lower() in text_lower:
                    results['cultural_terms'].append({
                        'term': term,
                        'language': lang,
                        'meaning': info['meaning'],
                        'maps_to': info['maps_to'],
                        'weight': info['weight']
                    })
                    results['trinity_mappings'].add(info['maps_to'])
    
    return results


def calculate_alignment_score(results):
    """Calculate Trinity Framework alignment score"""
    if not results['universal_glyphs'] and not results['cultural_terms']:
        return 0.1, "No symbolic content"
    
    score = 0.0
    total_items = 0
    
    # Score universal glyphs
    for glyph in results['universal_glyphs']:
        total_items += 1
        if glyph['category'] == 'trinity_core':
            score += 1.0
        elif glyph['category'] == 'positive_glyphs':
            score += glyph['weight']
        elif glyph['category'] == 'warning_glyphs':
            score += glyph['weight'] * 0.5
        elif glyph['category'] == 'blocked_glyphs':
            score -= 0.5
    
    # Score cultural terms
    for term in results['cultural_terms']:
        total_items += 1
        score += term['weight']
    
    # Trinity bonus
    trinity_glyphs = [g for g in results['universal_glyphs'] if g['glyph'] in ['âš›ï¸', 'ğŸ§ ', 'ğŸ›¡ï¸']]
    if len(trinity_glyphs) == 3:
        score += 0.5  # Full Trinity bonus
    
    # Calculate final alignment
    if total_items > 0:
        alignment = score / total_items
        alignment = max(0, min(1, alignment))
        
        if alignment > 0.8:
            assessment = "Excellent Trinity alignment"
        elif alignment > 0.6:
            assessment = "Good alignment, minor enhancement suggested"
        elif alignment > 0.4:
            assessment = "Moderate alignment, healing recommended"
        else:
            assessment = "Poor alignment, intervention required"
        
        return alignment, assessment
    
    return 0.5, "Neutral alignment"


def demonstrate_major_languages():
    """Demonstrate LUKHÎ›S support for major languages"""
    print("ğŸŒ LUKHÎ›S Major Languages Support")
    print("=" * 70)
    print("Supporting: English, Spanish, French, German, Chinese, Japanese, Portuguese")
    print("=" * 70)
    
    glyph_map = load_glyph_map()
    
    # Test cases for each major language
    test_cases = [
        {
            "language": "English",
            "text": "Finding wisdom through protection and love ğŸ§ ğŸ›¡ï¸ğŸ’– brings harmony to life âš›ï¸"
        },
        {
            "language": "Spanish",
            "text": "El corazÃ³n encuentra equilibrio y sabidurÃ­a en el amor ğŸ’–âš–ï¸ con armonÃ­a ğŸŒˆ"
        },
        {
            "language": "French",
            "text": "La sagesse et l'Ã©quilibre apportent protection et lumiÃ¨re âœ¨ğŸ›¡ï¸ au cÅ“ur ğŸ’–"
        },
        {
            "language": "German",
            "text": "Weisheit und Schutz schaffen Gleichgewicht ğŸ§ ğŸ›¡ï¸ mit Liebe und Harmonie â˜¯ï¸"
        },
        {
            "language": "Chinese",
            "text": "æ™ºæ…§ä¸å’Œè°ï¼Œå®ˆæŠ¤å¿ƒçµä¹‹é“ ğŸ§ â˜¯ï¸ çˆ±ä¸å¹³è¡¡ ğŸ’–âš–ï¸"
        },
        {
            "language": "Japanese",  
            "text": "å¿ƒã®é“ã‚’å®ˆã‚Šã€æ„›ã¨å’Œã§æ‚Ÿã‚Šã‚’é–‹ã ğŸ’–â˜¯ï¸ğŸª· æ™ºæ…§ã®å…‰ âœ¨"
        },
        {
            "language": "Portuguese",
            "text": "A sabedoria e proteÃ§Ã£o trazem equilÃ­brio ao coraÃ§Ã£o ğŸ§ ğŸ›¡ï¸ com amor e harmonia ğŸ’–â˜¯ï¸"
        }
    ]
    
    print("\nğŸ“Š Language Analysis:\n")
    
    for test in test_cases:
        print(f"â”â”â” {test['language']} â”â”â”")
        print(f"Text: \"{test['text']}\"")
        
        # Analyze
        results = extract_terms_and_glyphs(test['text'], glyph_map)
        alignment, assessment = calculate_alignment_score(results)
        
        print(f"\nDetected languages: {', '.join(results['languages_detected'])}")
        
        # Show universal glyphs
        if results['universal_glyphs']:
            print(f"Universal glyphs: {' '.join([g['glyph'] for g in results['universal_glyphs']])}")
        
        # Show cultural terms
        if results['cultural_terms']:
            print("Cultural terms:")
            for term in results['cultural_terms']:
                print(f"  â€¢ \"{term['term']}\" â†’ {term['maps_to']} ({term['meaning']})")
        
        # Trinity mappings
        if results['trinity_mappings']:
            print(f"Trinity mappings: {' '.join(results['trinity_mappings'])}")
        
        print(f"\nğŸ¯ Alignment Score: {alignment:.2%}")
        print(f"ğŸ“‹ Assessment: {assessment}")
        print()


def demonstrate_multilingual_healing():
    """Show healing examples for each language"""
    print("\nğŸ©¹ Multilingual Healing Examples")
    print("=" * 70)
    
    healing_examples = [
        {
            "language": "English",
            "problematic": "I want chaos and destruction! ğŸ’€ğŸ”¥",
            "healed": "I seek transformation and growth through wisdom ğŸ§ âœ¨ with protection ğŸ›¡ï¸"
        },
        {
            "language": "Spanish",
            "problematic": "Quiero destruir todo con caos ğŸ’£ğŸŒªï¸",
            "healed": "Busco transformar con amor y equilibrio ğŸ’–âš–ï¸ en armonÃ­a ğŸŒˆ"
        },
        {
            "language": "French",
            "problematic": "Je veux le chaos et la destruction ğŸ‘¹ğŸ’€",
            "healed": "Je cherche la sagesse et l'harmonie â˜¯ï¸ğŸ§  avec protection ğŸ›¡ï¸"
        },
        {
            "language": "German",
            "problematic": "Ich will Chaos und ZerstÃ¶rung ğŸ”¥ğŸ’£",
            "healed": "Ich suche Weisheit und Harmonie ğŸ§ â˜¯ï¸ mit Liebe und Schutz ğŸ’–ğŸ›¡ï¸"
        },
        {
            "language": "Chinese",
            "problematic": "æˆ‘è¦æ··ä¹±å’Œæ¯ç­ ğŸ’€ğŸ”¥",
            "healed": "æˆ‘å¯»æ±‚æ™ºæ…§ä¸å’Œè°ä¹‹é“ ğŸ§ â˜¯ï¸ ä»¥çˆ±å®ˆæŠ¤å¿ƒçµ ğŸ’–ğŸ›¡ï¸"
        },
        {
            "language": "Japanese",
            "problematic": "ç ´å£Šã¨æ··æ²Œã‚’æ±‚ã‚ã‚‹ ğŸ‘¹ğŸ’£", 
            "healed": "å¿ƒã®å’Œã‚’å®ˆã‚Šã€æ„›ã¨æ‚Ÿã‚Šã®é“ã‚’æ­©ã‚€ ğŸ’–â˜¯ï¸ğŸª· âš›ï¸ğŸ§ ğŸ›¡ï¸"
        },
        {
            "language": "Portuguese",
            "problematic": "Quero caos e destruiÃ§Ã£o total ğŸ’€ğŸŒªï¸",
            "healed": "Procuro sabedoria e proteÃ§Ã£o com amor ğŸ§ ğŸ›¡ï¸ğŸ’– em harmonia â˜¯ï¸"
        }
    ]
    
    for example in healing_examples:
        print(f"\n{example['language']}:")
        print(f"  âŒ Before: \"{example['problematic']}\"")
        print(f"  âœ… After:  \"{example['healed']}\"")


def show_language_statistics():
    """Display language support statistics"""
    print("\n\nğŸ“ˆ Language Support Statistics")
    print("=" * 70)
    
    glyph_map = load_glyph_map()
    
    # Count terms per language
    language_stats = {}
    for lang, terms in glyph_map['cultural_variants'].items():
        language_stats[lang] = {
            'term_count': len(terms),
            'trinity_mappings': set(),
            'unique_mappings': set()
        }
        
        for term_data in terms.values():
            language_stats[lang]['unique_mappings'].add(term_data['maps_to'])
            if term_data['maps_to'] in ['âš›ï¸', 'ğŸ§ ', 'ğŸ›¡ï¸']:
                language_stats[lang]['trinity_mappings'].add(term_data['maps_to'])
    
    # Display stats
    print(f"Total supported languages: {len(language_stats)}")
    print(f"Total cultural terms: {sum(s['term_count'] for s in language_stats.values())}")
    print(f"\nPer-language breakdown:")
    
    for lang, stats in language_stats.items():
        trinity_coverage = len(stats['trinity_mappings']) / 3 * 100
        print(f"\n{lang.title()}:")
        print(f"  â€¢ Terms: {stats['term_count']}")
        print(f"  â€¢ Unique mappings: {len(stats['unique_mappings'])}")
        print(f"  â€¢ Trinity coverage: {trinity_coverage:.0f}%")
        print(f"  â€¢ Common mappings: {' '.join(list(stats['unique_mappings'])[:5])}")


if __name__ == "__main__":
    print("\nğŸŒ LUKHÎ›S Major Languages Demo")
    print("Trinity Framework: âš›ï¸ğŸ§ ğŸ›¡ï¸")
    print("=" * 70)
    
    demonstrate_major_languages()
    demonstrate_multilingual_healing()
    show_language_statistics()
    
    print("\n\nâœ… Major languages support demonstrated!")
    print("\nğŸŒ LUKHÎ›S speaks your language!")
    print("Ready for global deployment across English, Spanish, French,")
    print("German, Chinese, Japanese, and Portuguese communities.")