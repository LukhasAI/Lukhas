# Bridge Layer â€” Provider Adapters with Endocrine Modulation
## Where vendor SDKs meet our Trinity patterns (âš›ï¸ğŸ§ ğŸ›¡ï¸)

Purpose
- Centralize provider logic (OpenAI, Anthropic, Azure, Gemini) and keep serve/orchestration clean.
- Compose signals â†’ homeostasis â†’ prompt modulation â†’ provider calls, with safety and metrics.

Whatâ€™s here
- `llm_wrappers/` â€” provider-specific clients and services
- `openai_modulated_service.py` â€” orchestrates signals, retrieval v1, moderation, streaming
- `unified_openai_client.py` â€” async OpenAI client with retries and task model mapping

Why it matters
- Swap providers without touching API or orchestration logic.
- Enforce best practices (retries, rate handling, safety) in one place.

How it connects
- serve â†’ bridge/llm_wrappers â†’ external LLMs
- orchestration/signals â†’ modulation inputs (stress/urgency/trust)
- Guardian moderation hooks (pre/post) wrap outbound and inbound content

Key features
- Modulation: styles/params tuned by signals and homeostasis
- Retrieval v1: simple context notes injection; pluggable retriever later
- Streaming: token generator with post-moderation of concatenated text
- Metrics: counters for requests/streams/blocks; exposed via serve
- Testability: fake clients keep unit tests network-free

Trinity alignment
- âš›ï¸ Identity: stable adapter interfaces and traceable model choices
- ğŸ§  Consciousness: context weaving and modulation under dynamic conditions
- ğŸ›¡ï¸ Guardian: moderation-first design with safe fallbacks
