# Prometheus Alert Rules for Guardian PDP and Rate Limiting
#
# These alerts provide warning-level notifications for Guardian/RL health degradation.
# During RC soak period, these run in NO-PAGE mode (warnings only, no PagerDuty/SMS).
#
# Purpose: GA Guard Pack - additive alert rules for Guardian/RL monitoring
# Version: 1.0.0
# Date: 2025-10-14
# Mode: WARNING (no-page during RC soak)

groups:
  - name: guardian_pdp_performance
    interval: 60s
    rules:
      # PDP latency degradation (SLO: <10ms p95)
      - alert: GuardianPDPLatencyHigh
        expr: guardian:pdp_latency:p95 > 0.010
        for: 15m
        labels:
          severity: warning
          component: guardian_pdp
          slo: latency
        annotations:
          summary: "Guardian PDP p95 latency {{ $value | humanizeDuration }} exceeds 10ms SLO"
          description: |
            Guardian PDP is taking longer than expected to make policy decisions.

            Current p95 latency: {{ $value | humanizeDuration }}
            SLO threshold: 10ms

            Possible causes:
            - Complex policy rules (too many conditions)
            - High rule evaluation count
            - Resource constraints (CPU/memory)

            Actions:
            1. Check policy complexity: Review rule count and condition nesting
            2. Check resource usage: CPU and memory for LUKHAS process
            3. Review guardian:rule_eval_freq:by_rule for hot rules
          runbook_url: "https://docs.lukhas.ai/runbooks/guardian-latency-high"

      # PDP latency critical (p99 >50ms indicates serious issue)
      - alert: GuardianPDPLatencyCritical
        expr: guardian:pdp_latency:p99 > 0.050
        for: 10m
        labels:
          severity: warning  # Still warning during RC soak
          component: guardian_pdp
          slo: latency
        annotations:
          summary: "Guardian PDP p99 latency {{ $value | humanizeDuration }} critically high"
          description: |
            Guardian PDP p99 latency is severely degraded.

            Current p99 latency: {{ $value | humanizeDuration }}
            Critical threshold: 50ms

            This indicates a serious performance issue.
            Consider rolling back recent policy changes.

  - name: guardian_denial_rates
    interval: 60s
    rules:
      # Denial rate spike (>15% may indicate overly restrictive policy)
      - alert: GuardianDenialRateHigh
        expr: guardian:denial_rate:ratio > 0.15
        for: 15m
        labels:
          severity: warning
          component: guardian_pdp
          sli: denial_rate
        annotations:
          summary: "Guardian denial rate {{ $value | humanizePercentage }} exceeds 15% threshold"
          description: |
            Guardian is denying a high percentage of requests.

            Current denial rate: {{ $value | humanizePercentage }}
            Threshold: 15%

            Possible causes:
            - Policy is overly restrictive
            - Misconfigured scopes or subjects
            - Attack or unusual traffic pattern

            Actions:
            1. Check top denial reasons: guardian:denials:top_reasons
            2. Review recent policy changes
            3. Verify client authentication and scopes
          runbook_url: "https://docs.lukhas.ai/runbooks/guardian-denial-rate-high"

      # Denial rate by scope anomaly (one scope denying >50%)
      - alert: GuardianScopeDenialSpike
        expr: guardian:denial_rate:by_scope > 0.50
        for: 10m
        labels:
          severity: warning
          component: guardian_pdp
          sli: denial_rate
        annotations:
          summary: "Scope {{ $labels.scope }} denial rate {{ $value | humanizePercentage }} exceeds 50%"
          description: |
            A specific scope is experiencing very high denial rate.

            Scope: {{ $labels.scope }}
            Denial rate: {{ $value | humanizePercentage }}

            This may indicate:
            - Missing scope in client credentials
            - Policy rule targeting this scope too restrictive
            - Client misconfiguration

            Review policy rules for scope "{{ $labels.scope }}".

  - name: rate_limiter_exhaustion
    interval: 60s
    rules:
      # Many principals near rate limit exhaustion
      - alert: RateLimitNearExhaustion
        expr: rl:near_exhaustion:ratio > 0.30
        for: 10m
        labels:
          severity: warning
          component: rate_limiting
          sli: capacity
        annotations:
          summary: "{{ $value | humanizePercentage }} of principals near rate limit exhaustion"
          description: |
            Many principals are close to hitting their rate limits (< 20% remaining).

            Near-exhaustion ratio: {{ $value | humanizePercentage }}
            Threshold: 30%

            This may indicate:
            - Traffic spike or burst pattern
            - Rate limits set too low for actual usage
            - Bot or automated client behavior

            Actions:
            1. Review rl:utilization:by_route for hot endpoints
            2. Check rl:hit_rate:by_principal for top consumers
            3. Consider increasing quotas if traffic is legitimate
          runbook_url: "https://docs.lukhas.ai/runbooks/ratelimit-near-exhaustion"

      # High hit rate (many 429s being returned)
      - alert: RateLimitHitRateHigh
        expr: rl:hit_rate:ratio > 0.10
        for: 15m
        labels:
          severity: warning
          component: rate_limiting
          sli: hit_rate
        annotations:
          summary: "Rate limit hit rate {{ $value | humanizePercentage }} exceeds 10%"
          description: |
            Many requests are being rate-limited (HTTP 429).

            Hit rate: {{ $value | humanizePercentage }}
            Threshold: 10%

            This indicates:
            - Clients are exceeding their quotas frequently
            - Possible retry storm or poorly configured client
            - Rate limits may be too restrictive

            Actions:
            1. Check rl:hit_rate:by_principal for top violators
            2. Review rl:hit_rate:by_route for problematic endpoints
            3. Verify quota configuration matches expected usage
          runbook_url: "https://docs.lukhas.ai/runbooks/ratelimit-hit-rate-high"

      # Specific route experiencing high RL hits
      - alert: RateLimitRouteHitSpike
        expr: rl:hit_rate:by_route > 0.25
        for: 10m
        labels:
          severity: warning
          component: rate_limiting
          sli: hit_rate
        annotations:
          summary: "Route {{ $labels.route }} rate limit hit rate {{ $value | humanizePercentage }}"
          description: |
            A specific endpoint is experiencing high rate limit rejections.

            Route: {{ $labels.route }}
            Hit rate: {{ $value | humanizePercentage }}

            This endpoint may need:
            - Higher quota allocation
            - Different rate limiting strategy (per-user vs global)
            - Investigation for abuse

  - name: combined_health_monitoring
    interval: 120s
    rules:
      # Combined health score degradation
      - alert: GuardianRLHealthLow
        expr: lukhas:guardian_rl:health_score < 0.70
        for: 20m
        labels:
          severity: warning
          component: guardian_rl
          sli: health
        annotations:
          summary: "Guardian/RL combined health score {{ $value }} below 70%"
          description: |
            Overall Guardian and Rate Limiting health is degraded.

            Health score: {{ $value }} (0-1 scale)
            Threshold: 0.70

            Health score factors:
            - Denial rate (40% weight)
            - RL hit rate (40% weight)
            - PDP latency (20% weight)

            Actions:
            1. Check guardian:denial_rate:ratio
            2. Check rl:hit_rate:ratio
            3. Check guardian:pdp_latency:p95
            4. Review Grafana Guardian/RL dashboard
          runbook_url: "https://docs.lukhas.ai/runbooks/guardian-rl-health-low"

      # Health score critical (indicates multiple issues)
      - alert: GuardianRLHealthCritical
        expr: lukhas:guardian_rl:health_score < 0.50
        for: 10m
        labels:
          severity: warning  # Still warning during RC soak
          component: guardian_rl
          sli: health
        annotations:
          summary: "Guardian/RL health score {{ $value }} critically low (< 50%)"
          description: |
            Multiple Guardian/RL subsystems are degraded.

            Health score: {{ $value }}
            Critical threshold: 0.50

            This indicates compounding issues.
            Consider rollback if during RC soak period.

  - name: guardian_rule_coverage
    interval: 120s
    rules:
      # Low rule evaluation coverage (may indicate policy gaps)
      - alert: GuardianRuleCoverageLow
        expr: |
          sum(rate(lukhas_guardian_rule_evaluations_total[15m]))
          /
          sum(rate(lukhas_guardian_decision_total[15m]))
          < 0.50
        for: 20m
        labels:
          severity: info  # Informational only
          component: guardian_pdp
        annotations:
          summary: "Guardian rule evaluation coverage {{ $value | humanizePercentage }}"
          description: |
            Less than 50% of decisions are evaluating rules.

            Coverage: {{ $value | humanizePercentage }}

            This may indicate:
            - Default-deny policy with few allow rules
            - Many requests not matching any rule subjects/resources
            - Policy design issue (rules too specific)

            Review policy design for intended coverage.

  - name: rate_limiter_backend_health
    interval: 60s
    rules:
      # Redis backend errors (if using RedisRateLimitBackend)
      - alert: RateLimitRedisErrors
        expr: rate(lukhas_ratelimit_backend_errors_total{backend="redis"}[10m]) > 0.01
        for: 5m
        labels:
          severity: warning
          component: rate_limiting
          backend: redis
        annotations:
          summary: "Rate limit Redis backend error rate {{ $value }}/sec"
          description: |
            Redis backend for rate limiting is experiencing errors.

            Error rate: {{ $value }} errors/sec

            Possible causes:
            - Redis connection issues
            - Redis server overload
            - Network problems

            Actions:
            1. Check Redis health: redis-cli ping
            2. Check Redis logs for errors
            3. Verify network connectivity to Redis
            4. Rate limiter will fall back to in-memory if Redis unavailable

---

# Alert Routing Configuration (Example for Prometheus Alertmanager)
#
# Add to alertmanager.yml during RC soak:
#
# route:
#   receiver: 'rc-soak-team'
#   group_by: ['alertname', 'component']
#   group_wait: 10s
#   group_interval: 5m
#   repeat_interval: 4h
#   routes:
#     - match:
#         severity: warning
#       receiver: 'rc-soak-slack'
#       continue: true
#
# receivers:
#   - name: 'rc-soak-slack'
#     slack_configs:
#       - api_url: 'https://hooks.slack.com/services/YOUR/WEBHOOK/URL'
#         channel: '#lukhas-rc-soak'
#         title: '{{ .GroupLabels.alertname }}'
#         text: '{{ range .Alerts }}{{ .Annotations.summary }}{{ end }}'
#
#   - name: 'rc-soak-team'
#     email_configs:
#       - to: 'team@lukhas.ai'
#         from: 'alerts@lukhas.ai'
#         headers:
#           Subject: '[RC Soak] {{ .GroupLabels.alertname }}'
