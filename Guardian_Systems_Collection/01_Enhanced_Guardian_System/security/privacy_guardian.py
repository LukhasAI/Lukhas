#!/usr/bin/env python3
"""
Privacy Guardian - Privacy protection and data security for sensitive information
Provides encryption, anonymization, and privacy compliance features
"""

import asyncio
import json
import logging
import time
from pathlib import Path
from typing import Dict, List, Optional, Any, Tuple
from dataclasses import dataclass, asdict
import hashlib
import uuid
import base64
from datetime import datetime, timedelta

logger = logging.getLogger(__name__)


@dataclass
class PrivacyPolicy:
    """Privacy policy configuration"""
    policy_id: str
    policy_name: str
    data_retention_days: int
    encryption_required: bool
    anonymization_level: str  # none, basic, advanced, full
    sharing_permissions: Dict[str, bool]
    consent_required: bool
    audit_logging: bool
    geographic_restrictions: List[str]


@dataclass
class DataClassification:
    """Data classification and sensitivity"""
    classification_id: str
    data_type: str
    sensitivity_level: str  # public, internal, confidential, restricted
    regulatory_requirements: List[str]  # HIPAA, GDPR, CCPA, etc.
    retention_period: int
    encryption_required: bool
    access_restrictions: List[str]
    anonymization_rules: List[str]


@dataclass
class PrivacyIncident:
    """Privacy incident tracking"""
    incident_id: str
    incident_type: str
    severity_level: str
    affected_data_types: List[str]
    discovery_time: datetime
    containment_time: Optional[datetime]
    resolution_time: Optional[datetime]
    impact_assessment: Dict
    remediation_actions: List[str]
    notification_required: bool


class PrivacyGuardian:
    """
    Privacy protection system for sensitive data
    Provides encryption, anonymization, and compliance features
    """
    
    # Data sensitivity levels
    SENSITIVITY_LEVELS = {
        "public": {
            "level": 0,
            "description": "Public information",
            "encryption": False,
            "anonymization": False,
            "access_control": "none"
        },
        "internal": {
            "level": 1,
            "description": "Internal use only",
            "encryption": False,
            "anonymization": False,
            "access_control": "basic"
        },
        "confidential": {
            "level": 2,
            "description": "Confidential information",
            "encryption": True,
            "anonymization": True,
            "access_control": "strict"
        },
        "restricted": {
            "level": 3,
            "description": "Highly restricted data",
            "encryption": True,
            "anonymization": True,
            "access_control": "maximum"
        }
    }
    
    # Privacy regulations
    PRIVACY_REGULATIONS = {
        "GDPR": {
            "name": "General Data Protection Regulation",
            "jurisdiction": "EU",
            "data_subject_rights": ["access", "rectification", "erasure", "portability"],
            "consent_requirements": "explicit",
            "breach_notification": 72,  # hours
            "penalties": "up to 4% of annual revenue"
        },
        "HIPAA": {
            "name": "Health Insurance Portability and Accountability Act",
            "jurisdiction": "US",
            "data_subject_rights": ["access", "amendment", "accounting"],
            "consent_requirements": "written",
            "breach_notification": 60,  # days
            "penalties": "up to $1.5M per incident"
        },
        "CCPA": {
            "name": "California Consumer Privacy Act",
            "jurisdiction": "California",
            "data_subject_rights": ["access", "deletion", "opt-out"],
            "consent_requirements": "opt-out",
            "breach_notification": "without unreasonable delay",
            "penalties": "up to $7,500 per violation"
        }
    }
    
    # Anonymization techniques
    ANONYMIZATION_TECHNIQUES = {
        "masking": "Replace sensitive data with mask characters",
        "hashing": "One-way hash transformation",
        "tokenization": "Replace with non-sensitive tokens",
        "generalization": "Replace with broader categories",
        "suppression": "Remove specific data elements",
        "noise_addition": "Add statistical noise to data",
        "k_anonymity": "Ensure k identical records exist",
        "differential_privacy": "Add mathematical privacy guarantees"
    }
    
    # Privacy symbols
    PRIVACY_SYMBOLS = {
        "protected": ["ğŸ”’", "ğŸ›¡ï¸", "ğŸ”"],
        "encrypted": ["ğŸ”‘", "ğŸ”’", "ğŸ’"],
        "anonymized": ["ğŸ‘¤", "â“", "ğŸ­"],
        "compliance": ["âœ…", "ğŸ“‹", "âš–ï¸"],
        "incident": ["ğŸš¨", "âš ï¸", "ğŸ”"],
        "audit": ["ğŸ“Š", "ğŸ”", "ğŸ“"]
    }
    
    def __init__(self, 
                 config_path: str = "config/privacy_config.yaml",
                 policies_path: str = "config/privacy_policies.json",
                 audit_log_path: str = "logs/privacy_audit.log"):
        
        self.config_path = Path(config_path)
        self.policies_path = Path(policies_path)
        self.audit_log_path = Path(audit_log_path)
        
        # Privacy policies and classifications
        self.privacy_policies: Dict[str, PrivacyPolicy] = {}
        self.data_classifications: Dict[str, DataClassification] = {}
        self.active_regulations: List[str] = ["GDPR", "HIPAA"]
        
        # Encryption and anonymization
        self.encryption_enabled = True
        self.default_anonymization = "basic"
        self.encryption_key = self._generate_encryption_key()
        
        # Privacy incidents and auditing
        self.privacy_incidents: List[PrivacyIncident] = []
        self.audit_events: List[Dict] = []
        self.consent_records: Dict[str, Dict] = {}
        
        # Compliance tracking
        self.compliance_status = {
            "GDPR": {"compliant": True, "last_check": None, "issues": []},
            "HIPAA": {"compliant": True, "last_check": None, "issues": []},
            "CCPA": {"compliant": True, "last_check": None, "issues": []}
        }
        
        # Configuration
        self.config = {
            "data_retention_default": 365,  # days
            "encryption_algorithm": "AES-256",
            "audit_level": "comprehensive",
            "consent_tracking": True,
            "incident_auto_detection": True,
            "compliance_monitoring": True
        }
        
        # Performance tracking
        self.stats = {
            "data_encrypted": 0,
            "data_anonymized": 0,
            "privacy_checks": 0,
            "incidents_detected": 0,
            "consent_requests": 0,
            "compliance_violations": 0,
            "audit_events_logged": 0
        }
        
        # Load configuration
        self._load_configuration()
        self._load_privacy_policies()
        
        logger.info("ğŸ”’ Privacy Guardian initialized")
    
    def _load_configuration(self):
        """Load privacy configuration"""
        try:
            if self.config_path.exists():
                with open(self.config_path, 'r') as f:
                    config_data = json.load(f)
                
                if "privacy_settings" in config_data:
                    self.config.update(config_data["privacy_settings"])
                
                if "active_regulations" in config_data:
                    self.active_regulations = config_data["active_regulations"]
                
                logger.info("Privacy configuration loaded")
            else:
                self._create_default_configuration()
                
        except Exception as e:
            logger.warning(f"Failed to load privacy configuration: {e}")
            self._create_default_configuration()
    
    def _load_privacy_policies(self):
        """Load privacy policies"""
        try:
            if self.policies_path.exists():
                with open(self.policies_path, 'r') as f:
                    policies_data = json.load(f)
                
                for policy_data in policies_data.get("policies", []):
                    policy = PrivacyPolicy(**policy_data)
                    self.privacy_policies[policy.policy_id] = policy
                
                for classification_data in policies_data.get("classifications", []):
                    classification = DataClassification(**classification_data)
                    self.data_classifications[classification.classification_id] = classification
                
                logger.info(f"Loaded {len(self.privacy_policies)} privacy policies")
            else:
                self._create_default_policies()
                
        except Exception as e:
            logger.warning(f"Failed to load privacy policies: {e}")
            self._create_default_policies()
    
    def _create_default_configuration(self):
        """Create default privacy configuration"""
        default_config = {
            "privacy_settings": self.config,
            "active_regulations": self.active_regulations,
            "encryption_settings": {
                "algorithm": "AES-256",
                "key_rotation_days": 90,
                "secure_key_storage": True
            },
            "anonymization_settings": {
                "default_level": "basic",
                "techniques": list(self.ANONYMIZATION_TECHNIQUES.keys()),
                "quality_threshold": 0.8
            }
        }
        
        try:
            self.config_path.parent.mkdir(parents=True, exist_ok=True)
            with open(self.config_path, 'w') as f:
                json.dump(default_config, f, indent=2)
            
            logger.info(f"Created default privacy configuration: {self.config_path}")
        except Exception as e:
            logger.error(f"Failed to create default configuration: {e}")
    
    def _create_default_policies(self):
        """Create default privacy policies"""
        default_policies = {
            "policies": [
                {
                    "policy_id": "medical_data_policy",
                    "policy_name": "Medical Data Privacy Policy",
                    "data_retention_days": 2555,  # 7 years
                    "encryption_required": True,
                    "anonymization_level": "advanced",
                    "sharing_permissions": {
                        "healthcare_providers": True,
                        "insurance": False,
                        "research": True,
                        "marketing": False
                    },
                    "consent_required": True,
                    "audit_logging": True,
                    "geographic_restrictions": []
                },
                {
                    "policy_id": "personal_data_policy",
                    "policy_name": "Personal Data Privacy Policy",
                    "data_retention_days": 1095,  # 3 years
                    "encryption_required": True,
                    "anonymization_level": "basic",
                    "sharing_permissions": {
                        "service_providers": True,
                        "analytics": True,
                        "advertising": False,
                        "third_parties": False
                    },
                    "consent_required": True,
                    "audit_logging": True,
                    "geographic_restrictions": ["CN", "RU"]
                }
            ],
            "classifications": [
                {
                    "classification_id": "medical_records",
                    "data_type": "medical",
                    "sensitivity_level": "restricted",
                    "regulatory_requirements": ["HIPAA", "GDPR"],
                    "retention_period": 2555,
                    "encryption_required": True,
                    "access_restrictions": ["healthcare_staff", "patient"],
                    "anonymization_rules": ["remove_identifiers", "generalize_dates", "mask_locations"]
                },
                {
                    "classification_id": "personal_identifiers",
                    "data_type": "personal",
                    "sensitivity_level": "confidential",
                    "regulatory_requirements": ["GDPR", "CCPA"],
                    "retention_period": 1095,
                    "encryption_required": True,
                    "access_restrictions": ["authorized_personnel"],
                    "anonymization_rules": ["hash_identifiers", "tokenize_names"]
                }
            ]
        }
        
        try:
            self.policies_path.parent.mkdir(parents=True, exist_ok=True)
            with open(self.policies_path, 'w') as f:
                json.dump(default_policies, f, indent=2)
            
            # Load the policies we just created
            self._load_privacy_policies()
            
            logger.info(f"Created default privacy policies: {self.policies_path}")
        except Exception as e:
            logger.error(f"Failed to create default policies: {e}")
    
    def _generate_encryption_key(self) -> str:
        """Generate encryption key"""
        # In production, this would use proper key management
        return base64.b64encode(hashlib.sha256(str(uuid.uuid4()).encode()).digest()).decode()
    
    async def initialize_privacy_services(self):
        """Initialize privacy protection services"""
        logger.info("ğŸ”’ Initializing privacy protection services")
        
        try:
            # Initialize encryption services
            await self._initialize_encryption()
            
            # Initialize anonymization engine
            await self._initialize_anonymization()
            
            # Start compliance monitoring
            await self._start_compliance_monitoring()
            
            # Initialize audit logging
            await self._initialize_audit_logging()
            
            logger.info("âœ… Privacy services initialized successfully")
            return True
            
        except Exception as e:
            logger.error(f"Failed to initialize privacy services: {e}")
            return False
    
    async def _initialize_encryption(self):
        """Initialize encryption services"""
        await asyncio.sleep(0.1)  # Simulate initialization
        logger.info("ğŸ”‘ Encryption services initialized")
    
    async def _initialize_anonymization(self):
        """Initialize anonymization engine"""
        await asyncio.sleep(0.1)  # Simulate initialization
        logger.info("ğŸ­ Anonymization engine initialized")
    
    async def _start_compliance_monitoring(self):
        """Start compliance monitoring"""
        await asyncio.sleep(0.1)  # Simulate initialization
        logger.info("âš–ï¸ Compliance monitoring started")
    
    async def _initialize_audit_logging(self):
        """Initialize audit logging"""
        self.audit_log_path.parent.mkdir(parents=True, exist_ok=True)
        logger.info("ğŸ“ Audit logging initialized")
    
    async def classify_data(self, data: Dict, data_type: str = None) -> Dict:
        """Classify data sensitivity and determine protection requirements"""
        start_time = time.time()
        
        try:
            # Determine data type if not provided
            if not data_type:
                data_type = self._detect_data_type(data)
            
            # Find matching classification
            classification = self._find_classification(data_type)
            
            if not classification:
                # Create default classification
                classification = DataClassification(
                    classification_id=f"auto_{data_type}",
                    data_type=data_type,
                    sensitivity_level="internal",
                    regulatory_requirements=[],
                    retention_period=365,
                    encryption_required=False,
                    access_restrictions=["authenticated_users"],
                    anonymization_rules=[]
                )
            
            # Analyze data content for sensitive patterns
            sensitive_patterns = await self._detect_sensitive_patterns(data)
            
            # Adjust classification based on detected patterns
            if sensitive_patterns:
                classification = await self._adjust_classification_for_patterns(classification, sensitive_patterns)
            
            self.stats["privacy_checks"] += 1
            
            # Log classification event
            await self._log_audit_event({
                "event_type": "data_classification",
                "data_type": data_type,
                "classification": classification.sensitivity_level,
                "patterns_detected": len(sensitive_patterns),
                "timestamp": datetime.now().isoformat()
            })
            
            return {
                "success": True,
                "data_type": data_type,
                "classification": asdict(classification),
                "sensitive_patterns": sensitive_patterns,
                "protection_requirements": {
                    "encryption_required": classification.encryption_required,
                    "anonymization_required": len(classification.anonymization_rules) > 0,
                    "access_restrictions": classification.access_restrictions,
                    "regulatory_compliance": classification.regulatory_requirements
                },
                "classification_time": time.time() - start_time,
                "symbolic_signature": self.PRIVACY_SYMBOLS["protected"]
            }
            
        except Exception as e:
            logger.error(f"Data classification failed: {e}")
            return {
                "success": False,
                "error": str(e),
                "classification_time": time.time() - start_time,
                "symbolic_signature": self.PRIVACY_SYMBOLS["incident"]
            }
    
    def _detect_data_type(self, data: Dict) -> str:
        """Detect data type from content"""
        # Simple pattern-based detection
        content_str = json.dumps(data).lower()
        
        medical_keywords = ["patient", "diagnosis", "medication", "treatment", "symptom", "doctor", "hospital"]
        personal_keywords = ["name", "email", "phone", "address", "ssn", "passport", "license"]
        financial_keywords = ["account", "payment", "credit", "bank", "transaction", "balance"]
        
        if any(keyword in content_str for keyword in medical_keywords):
            return "medical"
        elif any(keyword in content_str for keyword in financial_keywords):
            return "financial"
        elif any(keyword in content_str for keyword in personal_keywords):
            return "personal"
        else:
            return "general"
    
    def _find_classification(self, data_type: str) -> Optional[DataClassification]:
        """Find matching data classification"""
        for classification in self.data_classifications.values():
            if classification.data_type == data_type:
                return classification
        return None
    
    async def _detect_sensitive_patterns(self, data: Dict) -> List[str]:
        """Detect sensitive patterns in data"""
        patterns = []
        
        # Convert data to string for pattern matching
        data_str = json.dumps(data)
        
        # Email pattern
        if "@" in data_str and "." in data_str:
            patterns.append("email_address")
        
        # Phone number pattern (simple)
        import re
        phone_pattern = r'\b\d{3}[-.]?\d{3}[-.]?\d{4}\b'
        if re.search(phone_pattern, data_str):
            patterns.append("phone_number")
        
        # SSN pattern
        ssn_pattern = r'\b\d{3}-\d{2}-\d{4}\b'
        if re.search(ssn_pattern, data_str):
            patterns.append("ssn")
        
        # Credit card pattern (simple)
        cc_pattern = r'\b\d{4}[-\s]?\d{4}[-\s]?\d{4}[-\s]?\d{4}\b'
        if re.search(cc_pattern, data_str):
            patterns.append("credit_card")
        
        # Medical record number pattern
        mrn_pattern = r'\bMRN[-\s]?\d+\b'
        if re.search(mrn_pattern, data_str, re.IGNORECASE):
            patterns.append("medical_record_number")
        
        return patterns
    
    async def _adjust_classification_for_patterns(self, classification: DataClassification, patterns: List[str]) -> DataClassification:
        """Adjust classification based on detected sensitive patterns"""
        sensitive_patterns = {"ssn", "credit_card", "medical_record_number"}
        
        if any(pattern in sensitive_patterns for pattern in patterns):
            # Upgrade to restricted if sensitive patterns found
            classification.sensitivity_level = "restricted"
            classification.encryption_required = True
            if "remove_identifiers" not in classification.anonymization_rules:
                classification.anonymization_rules.append("remove_identifiers")
            
            # Add relevant regulations
            if "ssn" in patterns or "credit_card" in patterns:
                if "CCPA" not in classification.regulatory_requirements:
                    classification.regulatory_requirements.append("CCPA")
            
            if "medical_record_number" in patterns:
                if "HIPAA" not in classification.regulatory_requirements:
                    classification.regulatory_requirements.append("HIPAA")
        
        return classification
    
    async def encrypt_data(self, data: Any, encryption_level: str = "standard") -> Dict:
        """Encrypt sensitive data"""
        start_time = time.time()
        
        try:
            if not self.encryption_enabled:
                return {
                    "success": False,
                    "error": "Encryption is disabled",
                    "encryption_time": time.time() - start_time,
                    "symbolic_signature": self.PRIVACY_SYMBOLS["incident"]
                }
            
            # Convert data to string for encryption
            data_str = json.dumps(data) if isinstance(data, dict) else str(data)
            
            # Simulate encryption (in production, use proper encryption)
            encrypted_data = self._mock_encrypt(data_str, encryption_level)
            
            self.stats["data_encrypted"] += 1
            
            # Log encryption event
            await self._log_audit_event({
                "event_type": "data_encryption",
                "encryption_level": encryption_level,
                "data_size": len(data_str),
                "timestamp": datetime.now().isoformat()
            })
            
            return {
                "success": True,
                "encrypted_data": encrypted_data,
                "encryption_level": encryption_level,
                "algorithm": self.config["encryption_algorithm"],
                "encryption_time": time.time() - start_time,
                "symbolic_signature": self.PRIVACY_SYMBOLS["encrypted"]
            }
            
        except Exception as e:
            logger.error(f"Data encryption failed: {e}")
            return {
                "success": False,
                "error": str(e),
                "encryption_time": time.time() - start_time,
                "symbolic_signature": self.PRIVACY_SYMBOLS["incident"]
            }
    
    def _mock_encrypt(self, data: str, level: str) -> str:
        """Mock encryption function"""
        # Simple base64 encoding for demo (NOT for production use)
        encoded = base64.b64encode(data.encode()).decode()
        
        if level == "high":
            # Double encoding for "high" security
            encoded = base64.b64encode(encoded.encode()).decode()
        
        return f"ENC:{level}:{encoded}"
    
    async def anonymize_data(self, data: Dict, anonymization_level: str = None, techniques: List[str] = None) -> Dict:
        """Anonymize sensitive data"""
        start_time = time.time()
        
        try:
            if not anonymization_level:
                anonymization_level = self.default_anonymization
            
            if not techniques:
                techniques = ["masking", "generalization"]
            
            # Create anonymized copy of data
            anonymized_data = data.copy()
            applied_techniques = []
            
            # Apply anonymization techniques
            for technique in techniques:
                if technique in self.ANONYMIZATION_TECHNIQUES:
                    anonymized_data = await self._apply_anonymization_technique(anonymized_data, technique)
                    applied_techniques.append(technique)
            
            # Calculate anonymization quality score
            quality_score = self._calculate_anonymization_quality(data, anonymized_data, applied_techniques)
            
            self.stats["data_anonymized"] += 1
            
            # Log anonymization event
            await self._log_audit_event({
                "event_type": "data_anonymization",
                "techniques_applied": applied_techniques,
                "quality_score": quality_score,
                "timestamp": datetime.now().isoformat()
            })
            
            return {
                "success": True,
                "anonymized_data": anonymized_data,
                "anonymization_level": anonymization_level,
                "techniques_applied": applied_techniques,
                "quality_score": quality_score,
                "anonymization_time": time.time() - start_time,
                "symbolic_signature": self.PRIVACY_SYMBOLS["anonymized"]
            }
            
        except Exception as e:
            logger.error(f"Data anonymization failed: {e}")
            return {
                "success": False,
                "error": str(e),
                "anonymization_time": time.time() - start_time,
                "symbolic_signature": self.PRIVACY_SYMBOLS["incident"]
            }
    
    async def _apply_anonymization_technique(self, data: Dict, technique: str) -> Dict:
        """Apply specific anonymization technique"""
        if technique == "masking":
            return self._apply_masking(data)
        elif technique == "generalization":
            return self._apply_generalization(data)
        elif technique == "suppression":
            return self._apply_suppression(data)
        elif technique == "hashing":
            return self._apply_hashing(data)
        else:
            return data
    
    def _apply_masking(self, data: Dict) -> Dict:
        """Apply data masking"""
        masked_data = data.copy()
        
        for key, value in masked_data.items():
            if isinstance(value, str):
                if "email" in key.lower() and "@" in value:
                    # Mask email
                    name, domain = value.split("@")
                    masked_data[key] = f"{name[0]}***@{domain}"
                elif "phone" in key.lower() or "tel" in key.lower():
                    # Mask phone number
                    masked_data[key] = "***-***-" + value[-4:] if len(value) >= 4 else "***"
                elif "name" in key.lower():
                    # Mask name
                    masked_data[key] = value[0] + "*" * (len(value) - 1) if value else "***"
        
        return masked_data
    
    def _apply_generalization(self, data: Dict) -> Dict:
        """Apply data generalization"""
        generalized_data = data.copy()
        
        for key, value in generalized_data.items():
            if "age" in key.lower() and isinstance(value, (int, float)):
                # Generalize age to range
                age = int(value)
                if age < 18:
                    generalized_data[key] = "under-18"
                elif age < 30:
                    generalized_data[key] = "18-30"
                elif age < 50:
                    generalized_data[key] = "30-50"
                else:
                    generalized_data[key] = "over-50"
            elif "zip" in key.lower() or "postal" in key.lower():
                # Generalize zip code
                if isinstance(value, str) and len(value) >= 3:
                    generalized_data[key] = value[:3] + "**"
        
        return generalized_data
    
    def _apply_suppression(self, data: Dict) -> Dict:
        """Apply data suppression"""
        suppressed_data = data.copy()
        
        # Remove highly sensitive fields
        sensitive_fields = ["ssn", "passport", "license", "account_number"]
        
        for field in sensitive_fields:
            keys_to_remove = [key for key in suppressed_data.keys() if field in key.lower()]
            for key in keys_to_remove:
                suppressed_data[key] = "[SUPPRESSED]"
        
        return suppressed_data
    
    def _apply_hashing(self, data: Dict) -> Dict:
        """Apply data hashing"""
        hashed_data = data.copy()
        
        for key, value in hashed_data.items():
            if "id" in key.lower() and isinstance(value, str):
                # Hash ID fields
                hash_object = hashlib.sha256(value.encode())
                hashed_data[key] = hash_object.hexdigest()[:16]  # Truncated hash
        
        return hashed_data
    
    def _calculate_anonymization_quality(self, original: Dict, anonymized: Dict, techniques: List[str]) -> float:
        """Calculate anonymization quality score"""
        # Simple quality calculation based on techniques applied and data preserved
        base_score = 0.5
        
        # Higher score for more techniques
        technique_bonus = len(techniques) * 0.1
        
        # Score based on data utility preserved
        preserved_fields = sum(1 for key in original.keys() if key in anonymized and anonymized[key] != "[SUPPRESSED]")
        total_fields = len(original)
        utility_score = preserved_fields / total_fields if total_fields > 0 else 0
        
        final_score = min(1.0, base_score + technique_bonus + (utility_score * 0.3))
        return round(final_score, 2)
    
    async def check_compliance(self, regulation: str = None) -> Dict:
        """Check privacy compliance status"""
        start_time = time.time()
        
        try:
            regulations_to_check = [regulation] if regulation else self.active_regulations
            compliance_results = {}
            
            for reg in regulations_to_check:
                if reg in self.PRIVACY_REGULATIONS:
                    compliance_result = await self._check_regulation_compliance(reg)
                    compliance_results[reg] = compliance_result
                    
                    # Update compliance status
                    self.compliance_status[reg] = {
                        "compliant": compliance_result["compliant"],
                        "last_check": datetime.now().isoformat(),
                        "issues": compliance_result.get("issues", [])
                    }
            
            overall_compliant = all(result["compliant"] for result in compliance_results.values())
            
            if not overall_compliant:
                self.stats["compliance_violations"] += 1
            
            # Log compliance check
            await self._log_audit_event({
                "event_type": "compliance_check",
                "regulations_checked": regulations_to_check,
                "overall_compliant": overall_compliant,
                "timestamp": datetime.now().isoformat()
            })
            
            return {
                "success": True,
                "overall_compliant": overall_compliant,
                "regulation_results": compliance_results,
                "compliance_status": self.compliance_status,
                "check_time": time.time() - start_time,
                "symbolic_signature": self.PRIVACY_SYMBOLS["compliance"] if overall_compliant else self.PRIVACY_SYMBOLS["incident"]
            }
            
        except Exception as e:
            logger.error(f"Compliance check failed: {e}")
            return {
                "success": False,
                "error": str(e),
                "check_time": time.time() - start_time,
                "symbolic_signature": self.PRIVACY_SYMBOLS["incident"]
            }
    
    async def _check_regulation_compliance(self, regulation: str) -> Dict:
        """Check compliance with specific regulation"""
        reg_info = self.PRIVACY_REGULATIONS[regulation]
        issues = []
        
        # Check basic requirements
        if regulation == "GDPR":
            # Check consent tracking
            if not self.config.get("consent_tracking", False):
                issues.append("Consent tracking not enabled")
            
            # Check data subject rights implementation
            # (This would be more comprehensive in production)
            
        elif regulation == "HIPAA":
            # Check encryption requirements
            if not self.encryption_enabled:
                issues.append("Encryption not enabled for medical data")
            
            # Check audit logging
            if not self.config.get("audit_level") == "comprehensive":
                issues.append("Comprehensive audit logging not enabled")
        
        elif regulation == "CCPA":
            # Check opt-out mechanisms
            # (This would check actual implementation)
            pass
        
        compliant = len(issues) == 0
        
        return {
            "regulation": regulation,
            "compliant": compliant,
            "issues": issues,
            "requirements_checked": len(reg_info.get("data_subject_rights", [])),
            "regulation_info": reg_info
        }
    
    async def _log_audit_event(self, event: Dict):
        """Log privacy audit event"""
        try:
            event["event_id"] = str(uuid.uuid4())
            event["timestamp"] = event.get("timestamp", datetime.now().isoformat())
            
            self.audit_events.append(event)
            self.stats["audit_events_logged"] += 1
            
            # Write to audit log file
            with open(self.audit_log_path, 'a') as f:
                f.write(json.dumps(event) + "\n")
            
        except Exception as e:
            logger.error(f"Audit logging failed: {e}")
    
    def get_privacy_statistics(self) -> Dict:
        """Get privacy protection statistics"""
        stats = self.stats.copy()
        
        # Add current state information
        stats["active_policies"] = len(self.privacy_policies)
        stats["data_classifications"] = len(self.data_classifications)
        stats["active_regulations"] = len(self.active_regulations)
        stats["privacy_incidents_total"] = len(self.privacy_incidents)
        stats["audit_events_total"] = len(self.audit_events)
        
        # Calculate compliance rate
        compliant_regulations = sum(1 for status in self.compliance_status.values() if status["compliant"])
        total_regulations = len(self.compliance_status)
        stats["compliance_rate"] = compliant_regulations / total_regulations if total_regulations > 0 else 0.0
        
        return stats
    
    async def health_check(self) -> bool:
        """Perform health check"""
        try:
            # Test data classification
            test_data = {"name": "Test User", "email": "test@example.com"}
            classification_result = await self.classify_data(test_data)
            
            if not classification_result["success"]:
                return False
            
            # Test encryption
            encryption_result = await self.encrypt_data(test_data)
            
            if not encryption_result["success"]:
                return False
            
            # Test anonymization
            anonymization_result = await self.anonymize_data(test_data)
            
            if not anonymization_result["success"]:
                return False
            
            return True
            
        except Exception as e:
            logger.error(f"Privacy guardian health check failed: {e}")
            return False


if __name__ == "__main__":
    async def demo():
        """Demo privacy guardian functionality"""
        print("ğŸ”’ Privacy Guardian Demo")
        print("=" * 25)
        
        guardian = PrivacyGuardian()
        await guardian.initialize_privacy_services()
        
        # Test data classification
        test_data = {
            "patient_name": "John Doe",
            "email": "john.doe@email.com",
            "phone": "555-123-4567",
            "ssn": "123-45-6789",
            "diagnosis": "Hypertension",
            "age": 45,
            "zip_code": "12345"
        }
        
        print("\nğŸ” Testing data classification...")
        classification_result = await guardian.classify_data(test_data, "medical")
        
        if classification_result["success"]:
            print(f"   âœ… Data type: {classification_result['data_type']}")
            print(f"   ğŸ“Š Sensitivity: {classification_result['classification']['sensitivity_level']}")
            print(f"   ğŸ”’ Encryption required: {classification_result['protection_requirements']['encryption_required']}")
            print(f"   ğŸ­ Anonymization required: {classification_result['protection_requirements']['anonymization_required']}")
            print(f"   ğŸ” Sensitive patterns: {', '.join(classification_result['sensitive_patterns'])}")
        
        # Test encryption
        print("\nğŸ”‘ Testing data encryption...")
        encryption_result = await guardian.encrypt_data(test_data, "high")
        
        if encryption_result["success"]:
            print(f"   âœ… Encryption successful")
            print(f"   ğŸ“Š Algorithm: {encryption_result['algorithm']}")
            print(f"   â±ï¸  Time: {encryption_result['encryption_time']:.3f}s")
            print(f"   ğŸ” Encrypted data: {encryption_result['encrypted_data'][:50]}...")
        
        # Test anonymization
        print("\nğŸ­ Testing data anonymization...")
        anonymization_result = await guardian.anonymize_data(test_data, "advanced", ["masking", "generalization", "suppression"])
        
        if anonymization_result["success"]:
            print(f"   âœ… Anonymization successful")
            print(f"   ğŸ”§ Techniques: {', '.join(anonymization_result['techniques_applied'])}")
            print(f"   ğŸ“Š Quality score: {anonymization_result['quality_score']}")
            print(f"   ğŸ“ Sample anonymized data:")
            for key, value in list(anonymization_result['anonymized_data'].items())[:4]:
                print(f"      {key}: {value}")
        
        # Test compliance check
        print("\nâš–ï¸ Testing compliance check...")
        compliance_result = await guardian.check_compliance()
        
        if compliance_result["success"]:
            print(f"   âœ… Overall compliant: {compliance_result['overall_compliant']}")
            
            for regulation, result in compliance_result['regulation_results'].items():
                status = "âœ…" if result['compliant'] else "âŒ"
                print(f"   {status} {regulation}: {result['compliant']}")
                if result['issues']:
                    for issue in result['issues']:
                        print(f"      âš ï¸ Issue: {issue}")
        
        # Show statistics
        stats = guardian.get_privacy_statistics()
        print(f"\nğŸ“Š Privacy Guardian Statistics:")
        print(f"   Data encrypted: {stats['data_encrypted']}")
        print(f"   Data anonymized: {stats['data_anonymized']}")
        print(f"   Privacy checks: {stats['privacy_checks']}")
        print(f"   Compliance rate: {stats['compliance_rate']:.2f}")
        print(f"   Audit events logged: {stats['audit_events_logged']}")
        print(f"   Active regulations: {stats['active_regulations']}")
    
    asyncio.run(demo())
