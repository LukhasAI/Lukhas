Boom‚Äîhere‚Äôs a clean, provable ‚Äúreview queue ‚Üí promote to vocab‚Äù loop. It catches unmapped feature phrases during enrichment, stores them in a single accumulator, and gives you a safe CLI to promote any item to a canonical feature (or add as a synonym) without corrupting the vocab.

‚∏ª

1) Review queue spec

schema/review_queue.schema.json

{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "title": "LUKHŒõS Review Queue",
  "type": "object",
  "additionalProperties": false,
  "required": ["items", "updated_at"],
  "properties": {
    "updated_at": { "type": "string", "format": "date-time" },
    "items": {
      "type": "array",
      "uniqueItems": true,
      "items": {
        "type": "object",
        "additionalProperties": false,
        "required": ["raw", "module", "source", "count"],
        "properties": {
          "raw": { "type": "string", "minLength": 2 },
          "module": { "type": "string" },
          "source": { "type": "string", "enum": ["claude.me:bullets", "claude.me:headers", "other"] },
          "count": { "type": "integer", "minimum": 1 },
          "first_seen": { "type": "string", "format": "date-time" },
          "last_seen": { "type": "string", "format": "date-time" },
          "notes": { "type": "string" }
        }
      }
    }
  }
}


‚∏ª

2) Queue writer + helper

scripts/enrich/review_queue.py

from __future__ import annotations
import json
from dataclasses import dataclass
from datetime import datetime, timezone
from pathlib import Path
from typing import Dict, List

@dataclass
class ReviewItem:
    raw: str
    module: str
    source: str  # e.g., "claude.me:bullets"
    count: int = 1
    first_seen: str = datetime.now(timezone.utc).isoformat()
    last_seen: str = datetime.now(timezone.utc).isoformat()

class ReviewQueue:
    def __init__(self, repo_root: Path):
        self.root = repo_root
        self.path = self.root / "manifests" / "review_queue.json"
        self.data = {"updated_at": datetime.now(timezone.utc).isoformat(), "items": []}
        if self.path.exists():
            try:
                self.data = json.loads(self.path.read_text())
            except Exception:
                pass
        # index by (raw.lower())
        self.idx = {i["raw"].lower(): i for i in self.data.get("items", [])}

    def add(self, raw: str, module: str, source: str):
        key = raw.strip().lower()
        now = datetime.now(timezone.utc).isoformat()
        if key in self.idx:
            item = self.idx[key]
            item["count"] = int(item.get("count", 1)) + 1
            item["last_seen"] = now
            # merge module hint if different
            if module and module not in item.get("notes", ""):
                item["notes"] = (item.get("notes", "") + f" module:{module}").strip()
        else:
            item = ReviewItem(raw=raw.strip(), module=module, source=source).__dict__
            self.idx[key] = item
            self.data["items"].append(item)
        self.data["updated_at"] = now

    def save(self):
        self.path.parent.mkdir(parents=True, exist_ok=True)
        self.path.write_text(json.dumps(self.data, indent=2) + "\n")


‚∏ª

3) Wire queue into the feature extractor

Modify your ClaudeExtractor to catch unmapped phrases and push them to the queue. Below is a drop-in replacement for the feature methods inside scripts/enrich/collectors.py (only the changed parts shown):

# inside scripts/enrich/collectors.py
from scripts.enrich.review_queue import ReviewQueue

class ClaudeExtractor:
    def __init__(self, vocab: Vocab, repo_root: Path):
        self.vocab = vocab
        self.repo_root = repo_root
        self.queue = ReviewQueue(repo_root)

    # call this once per module at end to persist queue
    def flush_queue(self):
        self.queue.save()

    def features(self, p: Path, module_name: str = "") -> Signal:
        txt = self.read(p) or ""
        sha = hashlib.sha256(txt.encode()).hexdigest() if txt else None

        def add_unmapped(raw_text: str, source: str):
            raw = re.sub(r'[*_`~]', '', raw_text).strip()
            if raw and self.vocab.map_feature(raw) is None:
                self.queue.add(raw, module_name or p.parent.name, source)

        bullets, heads = set(), set()

        # Bullets
        for m in re.finditer(r'^[\s]*[-‚Ä¢*]\s*(.+)$', txt, re.MULTILINE):
            raw = m.group(1).split(':')[0]
            canon = self.vocab.map_feature(raw)
            if canon:
                bullets.add(canon)
            else:
                add_unmapped(raw, "claude.me:bullets")

        # Headers
        for m in re.finditer(r'^(?:##|###)\s+(.+)$', txt, re.MULTILINE):
            raw = m.group(1)
            canon = self.vocab.map_feature(raw)
            if canon:
                heads.add(canon)
            else:
                add_unmapped(raw, "claude.me:headers")

        both = len(bullets & heads)
        if both >= 2: conf, reasons = "high", [f"multi_match:{both}"]
        elif len(bullets) >= 5: conf, reasons = "medium", [f"bullets:{len(bullets)}"]
        else: conf, reasons = "low", ["weak_evidence"]

        feats = sorted((bullets | heads))[:15]
        return Signal(feats, ["claude.me:bullets","claude.me:headers"], conf, reasons, _now(), sha)

And in scripts/enrich_manifests.py, flush once per manifest:

# after building signals
s_features = claude.features(claude_me, module_name=mod_dir.name)
...
claude.flush_queue()  # persist new unmapped items


‚∏ª

4) CLI: promote a queued phrase ‚Üí vocab (canonical or synonym)

scripts/vocab_promote.py

from __future__ import annotations
import argparse, json, sys
from pathlib import Path
from datetime import datetime, timezone
from jsonschema import Draft202012Validator

ROOT = Path(__file__).resolve().parents[1]
FEATURES = ROOT / "vocab" / "features.json"
QUEUE = ROOT / "manifests" / "review_queue.json"
QUEUE_SCHEMA = json.loads((ROOT / "schema" / "review_queue.schema.json").read_text())
VALIDATOR = Draft202012Validator(QUEUE_SCHEMA)

def load_json(p: Path) -> dict:
    return json.loads(p.read_text()) if p.exists() else {}

def save_json(p: Path, data: dict):
    p.write_text(json.dumps(data, indent=2) + "\n")

def ensure_unique_canonical(features: dict, canonical: str):
    if canonical in features:
        print(f"‚ùå canonical '{canonical}' already exists")
        sys.exit(2)

def add_synonym(features: dict, canonical: str, synonym: str):
    if canonical not in features:
        print(f"‚ùå canonical '{canonical}' not found; create it first with --canonical")
        sys.exit(2)
    syns = features[canonical].setdefault("synonyms", [])
    if synonym in syns:
        print(f"‚ÜîÔ∏è synonym already present")
    else:
        syns.append(synonym)

def promote(args):
    features = load_json(FEATURES) or {}
    queue = load_json(QUEUE) or {"items": []}
    VALIDATOR.validate(queue)

    # find item (case-insensitive)
    items = queue.get("items", [])
    idx = {i["raw"].lower(): i for i in items}
    key = args.raw.strip().lower()
    if key not in idx:
        print(f"‚ùå '{args.raw}' not found in review_queue.json")
        sys.exit(1)

    item = idx[key]
    raw_norm = item["raw"].strip()

    if args.canonical:
        canonical = args.canonical.strip()
        ensure_unique_canonical(features, canonical)
        features[canonical] = {
            "canonical": canonical,
            "synonyms": [raw_norm],
            "category": args.category or "uncategorized",
            **({"constellation": args.constellation} if args.constellation else {}),
            **({"matriz_stage": args.matriz_stage} if args.matriz_stage else {})
        }
        print(f"‚úÖ created canonical '{canonical}' with synonym '{raw_norm}'")
    else:
        # add as synonym to existing canonical
        target = args.to.strip()
        if target not in features:
            print(f"‚ùå target canonical '{target}' not found")
            sys.exit(2)
        add_synonym(features, target, raw_norm)
        print(f"‚úÖ added synonym '{raw_norm}' ‚Üí '{target}'")

    # remove item from queue (it‚Äôs resolved)
    queue["items"] = [i for i in items if i is not item]
    queue["updated_at"] = datetime.now(timezone.utc).isoformat()

    save_json(FEATURES, features)
    save_json(QUEUE, queue)

def list_queue(args):
    queue = load_json(QUEUE) or {"items": []}
    VALIDATOR.validate(queue)
    if not queue["items"]:
        print("üéâ review_queue is empty")
        return
    # sort by count desc, last_seen desc
    items = sorted(queue["items"], key=lambda i: (i.get("count", 1), i.get("last_seen","")), reverse=True)
    for i in items:
        print(f"- {i['raw']}  [count:{i['count']}]  source:{i['source']}  module:{i['module']}  last:{i.get('last_seen','')}")

def main():
    ap = argparse.ArgumentParser(description="Promote unmapped features to vocab")
    sub = ap.add_subparsers(dest="cmd", required=True)

    ls = sub.add_parser("list", help="List review queue")
    ls.set_defaults(func=list_queue)

    pr = sub.add_parser("promote", help="Promote a raw phrase")
    g = pr.add_mutually_exclusive_group(required=True)
    g.add_argument("--canonical", help="Create a new canonical key (e.g., memory.convergence)")
    g.add_argument("--to", help="Add as synonym to existing canonical key")
    pr.add_argument("raw", help="Exact raw phrase as seen in review_queue")
    pr.add_argument("--category", help="Category for new canonical", default=None)
    pr.add_argument("--constellation", help="Optional constellation tag", default=None)
    pr.add_argument("--matriz-stage", help="Optional matriz stage", dest="matriz_stage", default=None)
    pr.set_defaults(func=promote)

    args = ap.parse_args()
    args.func(args)

if __name__ == "__main__":
    main()


‚∏ª

5) CI hook to assert queue shape

scripts/ci/validate_review_queue.py

import json, sys
from pathlib import Path
from jsonschema import Draft202012Validator

root = Path(".")
schema = json.loads((root/"schema/review_queue.schema.json").read_text())
v = Draft202012Validator(schema)

p = root/"manifests/review_queue.json"
if not p.exists():
    print("‚úÖ no review_queue.json (nothing to validate)")
    sys.exit(0)

try:
    v.validate(json.loads(p.read_text()))
except Exception as e:
    print("‚ùå review_queue invalid:", e)
    sys.exit(1)

print("‚úÖ review_queue valid")

And add to .pre-commit-config.yaml:

  - repo: local
    hooks:
      - id: review-queue-validate
        name: Validate review_queue.json
        entry: python scripts/ci/validate_review_queue.py
        language: system
        pass_filenames: false


‚∏ª

6) Usage

During enrichment

python scripts/enrich_manifests.py
# Any unmapped feature phrases encountered get appended to manifests/review_queue.json

Inspect queue (sorted by frequency, then recency)

python scripts/vocab_promote.py list

Promote a phrase into the vocab

A) Create a new canonical key

python scripts/vocab_promote.py promote "phenomenal pipeline" \
  --canonical phenomenology.pipeline \
  --category consciousness \
  --matriz-stage thought

B) Add as a synonym to an existing key

python scripts/vocab_promote.py promote "temporal stability" --to temporal.coherence

Each promotion:
	‚Ä¢	updates vocab/features.json deterministically,
	‚Ä¢	removes the item from review_queue.json,
	‚Ä¢	keeps both files schema-compliant,
	‚Ä¢	is safe to run repeatedly (idempotent for existing synonyms).

‚∏ª

7) Tiny guardrail: keep entropy out
	‚Ä¢	The extractor never writes unknown features to manifests.
	‚Ä¢	The queue is your single inbox.
	‚Ä¢	Only the CLI mutates features.json, with explicit choices and categories.
	‚Ä¢	Pre-commit enforces the queue schema so it can‚Äôt rot.

‚∏ª

Got it ‚Äî here‚Äôs a clean bulk-promote path that‚Äôs safe, idempotent, and reviewable. You‚Äôll get: a mapping file spec (JSON or CSV), a bulk CLI with dry-run, collision checks, and a summary report.

1) Mapping file formats

Option A ‚Äî JSON (vocab/promotions.json)

{
  "items": [
    {
      "raw": "temporal stability",
      "to": "temporal.coherence"
    },
    {
      "raw": "phenomenal pipeline",
      "canonical": "phenomenology.pipeline",
      "category": "consciousness",
      "matriz_stage": "thought"
    },
    {
      "raw": "ŒõID Core",
      "to": "identity.lambda_id"
    }
  ]
}

Option B ‚Äî CSV (vocab/promotions.csv)

Headers: raw,to,canonical,category,constellation,matriz_stage

temporal stability,temporal.coherence,,,,thought
phenomenal pipeline,,phenomenology.pipeline,consciousness,,thought
ŒõID Core,identity.lambda_id,,,,

Rules:
	‚Ä¢	Exactly one of to or canonical must be provided per row.
	‚Ä¢	category/constellation/matriz_stage only apply when creating a new canonical.

‚∏ª

2) Bulk CLI

scripts/vocab_bulk_promote.py

from __future__ import annotations
import argparse, csv, json, sys
from pathlib import Path
from datetime import datetime, timezone
from jsonschema import Draft202012Validator

ROOT = Path(__file__).resolve().parents[1]
FEATURES = ROOT / "vocab" / "features.json"
QUEUE = ROOT / "manifests" / "review_queue.json"
QUEUE_SCHEMA = json.loads((ROOT / "schema" / "review_queue.schema.json").read_text())
VALIDATOR = Draft202012Validator(QUEUE_SCHEMA)

def load_json(p: Path) -> dict:
    return json.loads(p.read_text()) if p.exists() else {}

def save_json(p: Path, obj: dict):
    p.write_text(json.dumps(obj, indent=2) + "\n")

def load_mapping(path: Path) -> list[dict]:
    if path.suffix.lower() == ".json":
        data = json.loads(path.read_text())
        return data.get("items", []) if isinstance(data, dict) else data
    if path.suffix.lower() == ".csv":
        rows = []
        with path.open(newline="", encoding="utf-8") as f:
            for r in csv.DictReader(f):
                rows.append({k: (v.strip() if v is not None else "") for k, v in r.items()})
        return rows
    raise SystemExit(f"Unsupported mapping format: {path.suffix}")

def validate_row(row: dict, i: int):
    raw = (row.get("raw") or "").strip()
    to = (row.get("to") or "").strip()
    canonical = (row.get("canonical") or "").strip()
    if not raw:
        raise ValueError(f"[row {i}] missing 'raw'")
    if bool(to) == bool(canonical):
        raise ValueError(f"[row {i}] must specify exactly one of 'to' or 'canonical'")
    return raw, to, canonical

def main():
    ap = argparse.ArgumentParser(description="Bulk promote review_queue items to vocab")
    ap.add_argument("mapping", type=str, help="JSON or CSV mapping file")
    ap.add_argument("--dry-run", action="store_true", help="Do not write any files")
    ap.add_argument("--create-missing", action="store_true",
                    help="Allow creation even if 'raw' is not in review_queue (use with care)")
    args = ap.parse_args()

    features = load_json(FEATURES) or {}
    queue = load_json(QUEUE) or {"items": []}
    try:
        VALIDATOR.validate(queue)
    except Exception as e:
        print(f"‚ùå review_queue invalid: {e}")
        sys.exit(2)

    qitems = {i["raw"].lower(): i for i in queue.get("items", [])}
    mapping = load_mapping(Path(args.mapping))

    report = {"created": [], "synonym_added": [], "skipped": [], "errors": []}

    for idx, row in enumerate(mapping, start=1):
        try:
            raw, to, canonical = validate_row(row, idx)
            key = raw.lower()

            # presence in queue (unless overridden)
            if key not in qitems and not args.create-missing:
                report["skipped"].append({"raw": raw, "reason": "not_in_review_queue"})
                continue

            if canonical:
                # create new canonical
                if canonical in features:
                    # already exists ‚Äî add 'raw' as synonym if missing
                    syns = features[canonical].setdefault("synonyms", [])
                    if raw in syns:
                        report["skipped"].append({"raw": raw, "canonical": canonical, "reason": "synonym_exists"})
                    else:
                        syns.append(raw)
                        report["synonym_added"].append({"raw": raw, "canonical": canonical})
                else:
                    features[canonical] = {
                        "canonical": canonical,
                        "synonyms": [raw],
                        "category": (row.get("category") or "uncategorized")
                    }
                    if row.get("constellation"):
                        features[canonical]["constellation"] = row["constellation"]
                    if row.get("matriz_stage"):
                        features[canonical]["matriz_stage"] = row["matriz_stage"]
                    report["created"].append({"canonical": canonical, "raw": raw})
            else:
                # add as synonym to existing canonical
                target = to
                if target not in features:
                    report["errors"].append({"raw": raw, "to": target, "error": "target_canonical_missing"})
                    continue
                syns = features[target].setdefault("synonyms", [])
                if raw in syns:
                    report["skipped"].append({"raw": raw, "to": target, "reason": "synonym_exists"})
                else:
                    syns.append(raw)
                    report["synonym_added"].append({"raw": raw, "to": target})

            # remove from queue if present
            if key in qitems:
                queue["items"] = [i for i in queue["items"] if i["raw"].lower() != key]
                qitems.pop(key, None)

        except Exception as e:
            report["errors"].append({"row": idx, "error": str(e)})

    # Writes
    if args.dry_run:
        print(json.dumps(report, indent=2))
        print("DRY-RUN: no files written")
        sys.exit(0)

    queue["updated_at"] = datetime.now(timezone.utc).isoformat()
    save_json(FEATURES, features)
    save_json(QUEUE, queue)

    # pretty summary
    print("=== Bulk Promote Summary ===")
    print(f"Created canonicals  : {len(report['created'])}")
    print(f"Synonyms added      : {len(report['synonym_added'])}")
    print(f"Skipped             : {len(report['skipped'])}")
    print(f"Errors              : {len(report['errors'])}")
    if report["errors"]:
        print(json.dumps(report["errors"], indent=2))


‚∏ª

3) Pre-commit & CI wiring

Add to .pre-commit-config.yaml (optional helper):

  - repo: local
    hooks:
      - id: vocab-bulk-dry-run
        name: Bulk promote (dry-run) check
        entry: bash -c 'test -f vocab/promotions.json || test -f vocab/promotions.csv || exit 0; python scripts/vocab_bulk_promote.py vocab/promotions.json --dry-run || python scripts/vocab_bulk_promote.py vocab/promotions.csv --dry-run'
        language: system
        pass_filenames: false

You can also run it as a CI step on a special branch if you prefer human review before writing.

‚∏ª

4) Usage

List queue:

python scripts/vocab_promote.py list

Dry-run bulk promote (JSON or CSV):

python scripts/vocab_bulk_promote.py vocab/promotions.json --dry-run
# or
python scripts/vocab_bulk_promote.py vocab/promotions.csv --dry-run

Apply changes:

python scripts/vocab_bulk_promote.py vocab/promotions.json

Allow promoting items not yet in the queue (rare):

python scripts/vocab_bulk_promote.py vocab/promotions.csv --create-missing


‚∏ª

5) Guarantees (T4/0.01%)
	‚Ä¢	Idempotent: re-running with the same mapping won‚Äôt duplicate synonyms or re-create canonicals.
	‚Ä¢	Scoped writes: only vocab/features.json and manifests/review_queue.json are touched.
	‚Ä¢	Safety rails: dry-run by default in pre-commit; CSV/JSON supported; strict row validation.
	‚Ä¢	Traceable: queue shrinks as entries are promoted; commit diff is small and reviewable.

Follow-on ideas: a --report md option to emit a changelog snippet for PR descriptions, and a --validate-only to check mappings against current queue without attempting promotions.