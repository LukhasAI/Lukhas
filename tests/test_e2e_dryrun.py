"""
End-to-End Dry-Run Test for LUKHAS AI
======================================

This test verifies that the system works in dry-run mode with all safety features enabled.
It tests the core integration path through identity, governance, orchestration, and policy.

This is our truth on the ground - one green test that proves the system works safely.
"""

import pytest
import sys
from pathlib import Path

# Add project root to path
sys.path.insert(0, str(Path(__file__).parents[1]))

from candidate.identity.lambda_id import authenticate
from candidate.governance.consent_ledger import record_consent
from candidate.orchestration.context import handoff_context
from candidate.core.core_wrapper import decide


@pytest.mark.e2e
@pytest.mark.dry_run
def test_e2e_dryrun():
    """
    Test end-to-end dry-run flow through core LUKHAS systems.
    
    This test verifies:
    1. Identity authentication works in dry-run mode
    2. Consent can be recorded safely
    3. Context can be built without side effects
    4. Policy decisions work in dry-run mode
    
    All operations should return success with no real processing.
    """
    
    # Test 1: Identity authentication in dry-run mode
    auth_result = authenticate("LID-demo", mode="dry_run")
    assert auth_result.get("ok", True), "Authentication should succeed in dry-run"
    assert auth_result.get("mode") == "dry_run" or "dry_run" in str(auth_result), \
        "Should indicate dry-run mode"
    
    # Test 2: Consent recording in dry-run mode
    consent_result = record_consent("LID-demo", "ctx:build", {"scopes": ["ctx:build", "policy:read"]})
    assert consent_result.get("ok", True), "Consent recording should succeed in dry-run"
    
    # Test 3: Context handoff in dry-run mode
    context_result = handoff_context({
        "session_id": "test_session_001",
        "tenant": "default",
        "user": "LID-demo"
    })
    assert context_result.get("ok", True), "Context handoff should succeed in dry-run"
    
    # Test 4: Policy decision in dry-run mode
    decision_result = decide({
        "action": "read",
        "resource": "test_resource",
        "context": context_result
    }, mode="dry_run")
    assert decision_result["decision"] in ("allow", "deny"), \
        "Decision should be either allow or deny"
    assert decision_result.get("explain") or decision_result.get("reason"), \
        "Decision should include explanation"
    assert decision_result.get("risk", 1.0) <= 1.0, \
        "Risk score should be between 0 and 1"


@pytest.mark.e2e
@pytest.mark.dry_run
def test_e2e_safety_defaults():
    """
    Verify that safety defaults are properly configured.
    
    This test ensures:
    - Dry-run mode is enabled by default
    - Feature flags are disabled by default
    - Guardian enforcement is strict
    """
    import os
    
    # Verify dry-run mode is enabled
    assert os.environ.get("LUKHAS_DRY_RUN_MODE", "true") == "true", \
        "Dry-run mode should be enabled by default"
    
    # Verify feature flags are disabled
    assert os.environ.get("FEATURE_POLICY_DECIDER", "false") == "false", \
        "Policy decider should be disabled by default"
    assert os.environ.get("FEATURE_ORCHESTRATION_HANDOFF", "false") == "false", \
        "Orchestration handoff should be disabled by default"
    assert os.environ.get("FEATURE_IDENTITY_PASSKEY", "false") == "false", \
        "Identity passkey should be disabled by default"
    assert os.environ.get("FEATURE_GOVERNANCE_LEDGER", "false") == "false", \
        "Governance ledger should be disabled by default"
    
    # Verify guardian enforcement
    assert os.environ.get("GUARDIAN_ENFORCEMENT", "strict") == "strict", \
        "Guardian enforcement should be strict by default"
    assert os.environ.get("ETHICS_ENFORCEMENT_LEVEL", "strict") == "strict", \
        "Ethics enforcement should be strict by default"


@pytest.mark.e2e
@pytest.mark.dry_run
def test_trinity_framework_integration():
    """
    Test Trinity Framework integration (âš›ï¸ðŸ§ ðŸ›¡ï¸).
    
    Verifies that the three pillars work together:
    - Identity (âš›ï¸): Authentication and self
    - Consciousness (ðŸ§ ): Awareness and processing
    - Guardian (ðŸ›¡ï¸): Ethics and safety
    """
    from candidate.core.core_wrapper import create_trinity_glyph, get_core_status
    
    # Test Trinity glyph creation
    trinity_glyph = create_trinity_glyph(emphasis="balanced", mode="dry_run")
    assert trinity_glyph.success, "Trinity glyph creation should succeed"
    assert trinity_glyph.symbol, "Trinity glyph should have a symbol"
    assert "trinity" in trinity_glyph.glyph_id.lower(), \
        "Glyph ID should indicate Trinity"
    
    # Test core status includes Trinity context
    status = get_core_status()
    assert "trinity_framework" in status, "Status should include Trinity Framework"
    assert status["dry_run_mode"], "Should be in dry-run mode"
    
    # Verify Trinity balance
    trinity = status.get("trinity_framework", {})
    assert trinity.get("identity") == "âš›ï¸", "Identity symbol should be present"
    assert trinity.get("consciousness") == "ðŸ§ ", "Consciousness symbol should be present"
    assert trinity.get("guardian") == "ðŸ›¡ï¸", "Guardian symbol should be present"


@pytest.mark.e2e
@pytest.mark.dry_run
@pytest.mark.safety
def test_no_side_effects_in_dryrun():
    """
    Verify that dry-run mode produces no side effects.
    
    This test ensures:
    - No external API calls are made
    - No data is persisted
    - No state changes occur
    """
    import os
    
    # Ensure offline mode
    assert os.environ.get("LUKHAS_OFFLINE", "true") == "true", \
        "System should be offline in tests"
    
    # Test multiple operations produce consistent results
    result1 = authenticate("test_user", mode="dry_run")
    result2 = authenticate("test_user", mode="dry_run")
    
    # Results should be similar (no state changes between calls)
    assert result1.get("ok") == result2.get("ok"), \
        "Repeated calls should produce consistent results"
    
    # Verify no real processing indicators
    decision = decide({"action": "write", "resource": "sensitive"}, mode="dry_run")
    assert "dry_run" in decision.get("explain", "").lower() or \
           decision.get("mode") == "dry_run", \
        "Decision should indicate dry-run mode"


if __name__ == "__main__":
    # Run the tests
    pytest.main([__file__, "-v", "--tb=short"])