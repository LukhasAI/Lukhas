"""
Tests for Unified Drift Management Module

Verifies determinism, attribution, bounds, and integration with IntegrityProbe.

#TAG:test
#TAG:drift
#TAG:monitoring
"""
import os
from unittest.mock import Mock

import pytest

# Set feature flag for testing
os.environ['LUKHAS_EXPERIMENTAL'] = '1'
os.environ['LUKHAS_LANE'] = 'labs'

from monitoring.drift_manager import DriftKind, DriftManager


class TestDriftManager:
    """Test suite for DriftManager functionality."""

    def test_determinism(self):
        """Test that same inputs produce same drift scores and top_symbols."""
        manager = DriftManager()

        # Define test states
        prev_state = {
            'compliance': 0.95,
            'constitutional': 0.90,
            'drift_score': 0.05,
            'ethics_phi': 0.98,
            'guardian_score': 0.92
        }
        curr_state = {
            'compliance': 0.85,
            'constitutional': 0.88,
            'drift_score': 0.12,
            'ethics_phi': 0.95,
            'guardian_score': 0.89
        }

        # Compute drift multiple times
        result1 = manager.compute('ethical', prev_state, curr_state)
        result2 = manager.compute('ethical', prev_state, curr_state)
        result3 = manager.compute('ethical', prev_state, curr_state)

        # Verify determinism
        assert result1['score'] == result2['score'] == result3['score']
        assert result1['top_symbols'] == result2['top_symbols'] == result3['top_symbols']
        assert result1['confidence'] == result2['confidence'] == result3['confidence']

    def test_attribution(self):
        """Test that top contributors are correctly identified."""
        manager = DriftManager()

        # Create states with known large drift in specific features
        prev_state = {
            'compliance': 1.0,  # No drift
            'constitutional': 1.0,  # No drift
            'drift_score': 0.0,  # Large drift
            'ethics_phi': 1.0,  # No drift
            'guardian_score': 0.5  # Medium drift
        }
        curr_state = {
            'compliance': 1.0,  # No drift
            'constitutional': 1.0,  # No drift
            'drift_score': 0.8,  # Large drift (0.8 delta)
            'ethics_phi': 1.0,  # No drift
            'guardian_score': 0.1  # Medium drift (0.4 delta)
        }

        result = manager.compute('ethical', prev_state, curr_state)

        # Verify top symbols include the largest contributors
        assert 'ethical.drift_score' in result['top_symbols']
        assert 'ethical.guardian_score' in result['top_symbols']

        # Verify ordering (largest drift first)
        if len(result['top_symbols']) >= 2:
            assert result['top_symbols'][0] == 'ethical.drift_score'
            assert result['top_symbols'][1] == 'ethical.guardian_score'

    def test_bounds(self):
        """Test that drift scores stay within [0,1] and are monotonic."""
        manager = DriftManager()

        # Test zero drift (identical states)
        same_state = {'compliance': 0.9, 'drift_score': 0.1}
        result = manager.compute('ethical', same_state, same_state)
        assert result['score'] == 0.0
        assert len(result['top_symbols']) == 0

        # Test small drift
        prev_small = {'compliance': 0.90, 'drift_score': 0.10}
        curr_small = {'compliance': 0.89, 'drift_score': 0.11}
        result_small = manager.compute('ethical', prev_small, curr_small)
        assert 0.0 <= result_small['score'] <= 1.0

        # Test medium drift
        prev_med = {'compliance': 0.90, 'drift_score': 0.10}
        curr_med = {'compliance': 0.70, 'drift_score': 0.30}
        result_med = manager.compute('ethical', prev_med, curr_med)
        assert 0.0 <= result_med['score'] <= 1.0

        # Test large drift
        prev_large = {'compliance': 1.0, 'drift_score': 0.0}
        curr_large = {'compliance': 0.0, 'drift_score': 1.0}
        result_large = manager.compute('ethical', prev_large, curr_large)
        assert 0.0 <= result_large['score'] <= 1.0

        # Verify monotonicity (larger changes â†’ larger scores)
        assert result_small['score'] < result_med['score'] < result_large['score']

    def test_memory_drift(self):
        """Test memory-specific drift calculations."""
        manager = DriftManager()

        prev_memory = {
            'fold_count': 500,
            'entropy': 0.3,
            'coherence': 0.95,
            'retrieval_accuracy': 0.98,
            'cascade_risk': 0.02
        }
        curr_memory = {
            'fold_count': 520,  # 20 fold increase
            'entropy': 0.35,  # Slight entropy increase
            'coherence': 0.93,  # Slight coherence drop
            'retrieval_accuracy': 0.97,
            'cascade_risk': 0.03
        }

        result = manager.compute('memory', prev_memory, curr_memory)

        assert 0.0 <= result['score'] <= 1.0
        assert 'memory.fold_stability' in result['top_symbols']
        assert result['confidence'] > 0.0

    def test_identity_drift(self):
        """Test identity-specific drift calculations."""
        manager = DriftManager()

        prev_identity = {
            'coherence': 0.98,
            'namespace_integrity': 1.0,
            'auth_consistency': 0.99,
            'tier_compliance': 1.0,
            'namespace_hash': 'abc123'
        }
        curr_identity = {
            'coherence': 0.95,
            'namespace_integrity': 0.98,
            'auth_consistency': 0.99,
            'tier_compliance': 1.0,
            'namespace_hash': 'def456'  # Changed namespace
        }

        result = manager.compute('identity', prev_identity, curr_identity)

        assert 0.0 <= result['score'] <= 1.0
        # Namespace change should be detected
        assert 'identity.namespace_change' in result['top_symbols']
        assert result['confidence'] > 0.0

    def test_unified_drift(self):
        """Test unified drift calculation across all dimensions."""
        manager = DriftManager()

        prev_unified = {
            'ethical': {'compliance': 0.95, 'drift_score': 0.05},
            'memory': {'fold_count': 500, 'entropy': 0.3},
            'identity': {'coherence': 0.98, 'namespace_hash': 'abc'}
        }
        curr_unified = {
            'ethical': {'compliance': 0.85, 'drift_score': 0.12},
            'memory': {'fold_count': 550, 'entropy': 0.4},
            'identity': {'coherence': 0.92, 'namespace_hash': 'abc'}
        }

        result = manager.compute('unified', prev_unified, curr_unified)

        assert 0.0 <= result['score'] <= 1.0
        # Should have symbols from all dimensions
        ethical_symbols = [s for s in result['top_symbols'] if s.startswith('ethical.')]
        memory_symbols = [s for s in result['top_symbols'] if s.startswith('memory.')]
        identity_symbols = [s for s in result['top_symbols'] if s.startswith('identity.')]

        assert len(ethical_symbols) > 0
        assert len(memory_symbols) > 0
        assert len(identity_symbols) > 0

        # Check metadata
        assert 'ethical_score' in result['metadata']
        assert 'memory_score' in result['metadata']
        assert 'identity_score' in result['metadata']

    def test_threshold_exceedance(self):
        """Test on_exceed handler for threshold violations."""
        manager = DriftManager()

        # Create states that will exceed threshold
        prev_state = {'compliance': 1.0, 'drift_score': 0.0}
        curr_state = {'compliance': 0.5, 'drift_score': 0.5}

        # Compute drift (should be high)
        result = manager.compute('ethical', prev_state, curr_state)
        assert result['score'] > 0.15  # Should exceed critical threshold

        # Test on_exceed handler
        initial_ledger_size = len(manager.drift_ledger)
        manager.on_exceed('ethical', result['score'], {'test': 'context'})

        # Verify ledger entry was added
        assert len(manager.drift_ledger) > initial_ledger_size

        # Should have threshold exceeded event
        threshold_events = [
            e for e in manager.drift_ledger
            if e.get('event') == 'threshold_exceeded'
        ]
        assert len(threshold_events) > 0
        last_threshold = threshold_events[-1]
        assert last_threshold['kind'] == 'ethical'
        assert last_threshold['score'] == result['score']

        # May also have auto-repair event if repair engine is available
        repair_events = [
            e for e in manager.drift_ledger
            if e.get('event', '').startswith('auto_repair')
        ]
        print(f"Auto-repair events: {len(repair_events)}")

    def test_drift_history(self):
        """Test drift history retrieval from ledger."""
        manager = DriftManager()

        # Generate some drift calculations
        states = [
            {'compliance': 0.9 + i * 0.01, 'drift_score': 0.1 + i * 0.01}
            for i in range(5)
        ]

        for i in range(1, len(states)):
            manager.compute('ethical', states[i-1], states[i])

        # Retrieve history
        history = manager.get_drift_history()
        assert len(history) >= 4  # Should have at least 4 entries

        # Filter by kind
        ethical_history = manager.get_drift_history(kind='ethical')
        assert all(e['kind'] == 'ethical' for e in ethical_history)

    def test_error_handling(self):
        """Test graceful error handling."""
        manager = DriftManager()

        # Test with invalid kind
        result = manager.compute('invalid_kind', {}, {})
        assert result['score'] == 0.0
        assert 'error' in result['metadata']

        # Test with None states
        result = manager.compute('ethical', None, None)
        assert result['score'] == 0.0
        assert result['top_symbols'] == []

    def test_configuration_overrides(self):
        """Test configuration parameter overrides."""
        config = {
            'critical_threshold': 0.25,
            'warning_threshold': 0.15,
            'ethical_weight': 0.5,
            'memory_weight': 0.25,
            'identity_weight': 0.25
        }
        manager = DriftManager(config)

        assert manager.critical_threshold == 0.25
        assert manager.warning_threshold == 0.15
        assert manager.weights[DriftKind.ETHICAL] == 0.5
        assert manager.weights[DriftKind.MEMORY] == 0.25
        assert manager.weights[DriftKind.IDENTITY] == 0.25


class TestIntegrityProbeIntegration:
    """Test IntegrityProbe integration with DriftManager."""

    def test_integrity_probe_drift_consumption(self):
        """Test that IntegrityProbe correctly consumes drift scores."""
        # Skip if module has import issues
        try:
            from qi.states.integrity_probe import IntegrityProbe
        except (ImportError, NameError) as e:
            pytest.skip(f"IntegrityProbe import failed: {e}")

        # Use manual mocking instead of patch decorator

        # Mock drift manager
        mock_manager = Mock()
        mock_manager.critical_threshold = 0.15
        mock_manager.compute.return_value = {
            'score': 0.10,
            'top_symbols': ['test.symbol1', 'test.symbol2'],
            'confidence': 0.9,
            'metadata': {}
        }
        mock_get_manager.return_value = mock_manager  # noqa: F821  # TODO: mock_get_manager

        # Create probe with mocked dependencies
        mock_drift_calc = Mock()
        mock_verifier = Mock()
        mock_repair = Mock()

        probe = IntegrityProbe(mock_drift_calc, mock_verifier, mock_repair)

        # Test with state containing all dimensions
        state = {
            'ethical': {'compliance': 0.9},
            'memory': {'fold_count': 500},
            'identity': {'coherence': 0.95}
        }

        # First call establishes baseline
        result = probe.run_consistency_check(state)
        assert result is True  # No previous state

        # Second call calculates drift
        state2 = {
            'ethical': {'compliance': 0.85},
            'memory': {'fold_count': 520},
            'identity': {'coherence': 0.93}
        }
        result = probe.run_consistency_check(state2)
        assert result is True  # Below threshold

        # Would verify compute was called for each dimension
        # assert mock_manager.compute.call_count >= 3  # ethical, memory, identity

    def test_integrity_probe_threshold_failure(self):
        """Test IntegrityProbe returns False when drift exceeds threshold."""
        # Skip if module has import issues
        try:
            from qi.states.integrity_probe import IntegrityProbe
        except (ImportError, NameError) as e:
            pytest.skip(f"IntegrityProbe import failed: {e}")

        # Mock drift manager with high drift score
        mock_manager = Mock()
        mock_manager.critical_threshold = 0.15
        mock_manager.compute.return_value = {
            'score': 0.25,  # Exceeds threshold
            'top_symbols': ['critical.drift'],
            'confidence': 0.95,
            'metadata': {}
        }

        # Create probe with manual injection
        probe = IntegrityProbe(Mock(), Mock(), Mock())
        probe.drift_manager = mock_manager

        # Set previous state
        probe.prev_state = {'ethical': {'compliance': 0.9}}
        probe.curr_state = {'ethical': {'compliance': 0.5}}

        # Run check with high drift state
        result = probe.run_consistency_check({'ethical': {'compliance': 0.5}})
        assert result is False  # Should fail due to high drift

        # Verify on_exceed was called
        mock_manager.on_exceed.assert_called()


if __name__ == '__main__':
    pytest.main([__file__, '-v'])
