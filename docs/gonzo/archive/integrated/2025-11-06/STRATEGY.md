## T4 Strategic Audit & Roadmap
Continuing from our earlier discussion on T4's strategic audit and roadmap, we identified three immediate deliverables to enhance our transparency, verifiability, and user experience:

1. **Evidence page template** and a **generator** that creates evidence-page stubs from the claims registry. (`branding/templates/evidence_page.md` + `tools/generate_evidence_pages.py`)
2. **Audit pack builder** that collects artifacts, computes hashes, builds `metadata.json`, and optionally signs the pack (`tools/build_audit_pack.py`) + a short GH Actions snippet for signing in CI.
3. **Reasoning Lab Redaction UX spec** (detailed) plus a **React/TSX skeleton component** that implements: mode toggle (public/dev/enterprise), redaction slider, accessible keyboard support, “Why?” side panel and Assistive-mode behaviors (`src/components/ReasoningLab/*`).

I’ll also give usage instructions, CI notes, and quick test commands. Let’s go.

---

## 1) Evidence page template + generator

### A. Template — `branding/templates/evidence_page.md`

Use this as the canonical file authors fill when they publish a claim. It is intentionally explicit: methodology, artifacts, signature, limitations.

```markdown
---
id: "{{CLAIM_ID}}"                  # unique id, e.g., matriz-p95-2025q3
title: "Evidence — {{SHORT_CLAIM}}" # short descriptive title
claim_text: "{{EXACT_CLAIM_TEXT}}"  # exact marketing copy that asserted the claim
domain: "lukhas.ai"
owner: "@web-architect"
last_verified: "YYYY-MM-DD"
verified_by:
  - "@web-architect"
status: "draft" # draft | verified | deprecated
artifact_links:
  - "release_artifacts/perf/2025-10-26-matriz-smoke.json"
signature: null   # filled after signing: {"method":"gpg","sig_file":"...","sig_hash":"..."}
metadata:
  created_at: "YYYY-MM-DD"
  generated_by: "generate_evidence_pages.py"
---

# Evidence: {{SHORT_CLAIM}}

**Claim:**  
{{EXACT_CLAIM_TEXT}}

## Summary
Short 2–3 sentence explanation of what the claim means and the scope (environments, dataset, date).

## Methodology
Describe precisely how the measurement was made:
- Test harness (branch/commit)
- Workload description
- Input data / dataset names
- Number of runs, aggregation method (e.g., p95 across N runs)
- Environment (regions, hardware, network conditions)

## Artifacts
List of artifacts (linked, with short descriptions and sha256).
- `release_artifacts/perf/2025-10-26-matriz-smoke.json` — smoke test raw output. SHA256: `<sha256>`

## Limitations & assumptions
Short bullet list: what this claim does *not* cover, known caveats, expected variation.

## Approval & signature
- Verified by: `@web-architect` on `YYYY-MM-DD`
- Legal: `@legal` (if required)
- Signature: See `signature` field in front-matter (signed metadata / detached signature)

## Full artifacts
(If the artifact is too large, add pointer to internal blobstore or S3 path and the signed `metadata.json` below.)

---
```

### B. Generator — `tools/generate_evidence_pages.py`

This script looks at `release_artifacts/claims_registry.yaml` (we produced earlier) and creates `release_artifacts/evidence/<claim-id>.md` stubs for any registry entries missing an evidence page. It populates the front-matter and leaves TODOs for authors. Run it to bootstrap your evidence pages.

Save as `tools/generate_evidence_pages.py`:

```python
#!/usr/bin/env python3
"""
tools/generate_evidence_pages.py

Reads release_artifacts/claims_registry.yaml (generated by generate_claims_registry.py)
and creates evidence page stubs under release_artifacts/evidence/<claim_id>.md when missing.

Usage:
  python3 tools/generate_evidence_pages.py
"""
import sys, os
from pathlib import Path
import yaml
import hashlib
import re

REGISTRY = Path("release_artifacts/claims_registry.yaml")
OUT_DIR = Path("release_artifacts/evidence")
TEMPLATE = Path("branding/templates/evidence_page.md")

def slugify(s):
    s = re.sub(r'[^a-z0-9\-]+','-', s.lower())
    s = re.sub(r'-+','-', s).strip('-')
    return s or "claim"

def ensure_template():
    if not TEMPLATE.exists():
        print("Missing template:", TEMPLATE)
        sys.exit(2)
    return TEMPLATE.read_text(encoding='utf-8')

def main():
    if not REGISTRY.exists():
        print("No registry found at", REGISTRY)
        sys.exit(0)
    out = yaml.safe_load(REGISTRY.read_text(encoding='utf-8'))
    claims = out.get("claims", [])
    if not claims:
        print("No claims in registry.")
        sys.exit(0)
    tpl = ensure_template()
    OUT_DIR.mkdir(parents=True, exist_ok=True)
    created = []
    for entry in claims:
        # build a unique claim id
        page = entry.get("page","unknown")
        claim_list = entry.get("claims_found",[])
        # make identifier based on page + claim snippet
        base = f"{Path(page).stem}-" + "-".join([re.sub(r'[^a-z0-9]', '', c.lower()) for c in claim_list[:2]])
        cid = slugify(base)
        outpath = OUT_DIR / f"{cid}.md"
        if outpath.exists():
            print("Exists:", outpath)
            continue
        # fill template
        fm = {
            "id": cid,
            "title": f"Evidence — {'; '.join(claim_list)}",
            "claim_text": "; ".join(claim_list),
            "domain": entry.get("domain"),
            "owner": entry.get("claims_verified_by", ["@web-architect"])[0] if entry.get("claims_verified_by") else "@web-architect",
            "last_verified": entry.get("last_reviewed") or "",
            "verified_by": entry.get("claims_verified_by") or [],
            "status": "draft",
            "artifact_links": entry.get("evidence_links") or [],
            "signature": None,
            "metadata": {
                "created_at": "", "generated_by": "generate_evidence_pages.py"
            }
        }
        # Render front-matter
        import yaml as _yaml
        front = "---\n" + _yaml.safe_dump(fm, sort_keys=False).strip() + "\n---\n\n"
        body = tpl
        # If template contains placeholders, rudimentary replace:
        body = body.replace("{{CLAIM_ID}}", cid).replace("{{SHORT_CLAIM}}", "; ".join(claim_list)[:60]).replace("{{EXACT_CLAIM_TEXT}}", "; ".join(claim_list))
        outpath.write_text(front + body, encoding='utf-8')
        created.append(outpath)
        print("Created:", outpath)
    print(f"Created {len(created)} evidence stubs in {OUT_DIR}")

if __name__ == "__main__":
    main()
```

**How to use**

```bash
python3 tools/generate_claims_registry.py   # if not already run
python3 tools/generate_evidence_pages.py
# Review release_artifacts/evidence/*.md, edit and fill METHOD, artifacts, signatures.
```

---

## 2) Audit pack builder script

### Purpose

A small, secure utility that:

* Collects artifacts (files/dirs) related to a claim or a set of claims,
* Computes SHA-256 for each artifact,
* Writes `metadata.json` with artifact entries and metadata,
* Optionally signs the `metadata.json` (GPG recommended) and bundles everything into `audit-packs/<pack-id>.zip`.

PKI signing makes packs verifiable by customers or auditors.

### A. Script — `tools/build_audit_pack.py`

```python
#!/usr/bin/env python3
"""
tools/build_audit_pack.py

Usage:
  python3 tools/build_audit_pack.py --claim-id matriz-p95-2025q3 \
      --artifacts release_artifacts/perf/2025-10-26-matriz-smoke.json \
      --out audit-packs/matriz-p95-2025q3.zip \
      --sign  # optional, requires gpg

Produces:
 - audit-packs/<pack>.zip containing artifacts, metadata.json, and optionally metadata.json.asc (gpg signature)
"""
import argparse, json, os, sys, shutil, hashlib
from pathlib import Path
import datetime
import subprocess

def sha256_file(path):
    h = hashlib.sha256()
    with open(path, 'rb') as f:
        while True:
            b = f.read(8192)
            if not b: break
            h.update(b)
    return h.hexdigest()

def build_pack(claim_id, artifacts, out_zip, signer=None):
    out_dir = Path("tmp_audit_pack") / claim_id
    if out_dir.exists():
        shutil.rmtree(out_dir)
    out_dir.mkdir(parents=True)
    artifacts_meta = []
    for art in artifacts:
        path = Path(art)
        if not path.exists():
            print(f"Missing artifact: {art}", file=sys.stderr)
            sys.exit(2)
        dest = out_dir / path.name
        shutil.copy2(path, dest)
        artifacts_meta.append({
            "file": str(path.name),
            "path": str(path),
            "sha256": sha256_file(path),
            "size": path.stat().st_size
        })
    metadata = {
        "claim_id": claim_id,
        "generated_at": datetime.datetime.utcnow().isoformat() + "Z",
        "artifacts": artifacts_meta,
        "tool": "build_audit_pack.py"
    }
    md_file = out_dir / "metadata.json"
    md_file.write_text(json.dumps(metadata, indent=2), encoding='utf-8')
    # Optionally sign metadata.json
    if signer:
        # signer is gpg key id or True for default gpg
        sig_file = out_dir / "metadata.json.asc"
        cmd = ["gpg", "--armor", "--output", str(sig_file), "--detach-sign", str(md_file)]
        if isinstance(signer, str):
            cmd = ["gpg", "--armor", "--output", str(sig_file), "--local-user", signer, "--detach-sign", str(md_file)]
        print("Signing metadata.json with gpg...")
        subprocess.check_call(cmd)
    # Create zip
    out_zip = Path(out_zip)
    out_zip.parent.mkdir(parents=True, exist_ok=True)
    shutil.make_archive(str(out_zip.with_suffix('')), 'zip', root_dir=out_dir)
    print(f"Created audit pack: {out_zip}")
    # cleanup
    shutil.rmtree(out_dir)

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--claim-id", required=True)
    parser.add_argument("--artifacts", nargs="+", required=True)
    parser.add_argument("--out", required=True)
    parser.add_argument("--sign", action="store_true", help="Sign metadata.json with gpg (needs gpg key)")
    parser.add_argument("--key", help="GPG key id to sign with (optional)")
    args = parser.parse_args()
    signer = args.key if args.key else (True if args.sign else None)
    build_pack(args.claim_id, args.artifacts, args.out, signer=signer)

if __name__=='__main__':
    main()
```

**Notes / CI usage**

* For CI signing: create a GPG key for signing (or use an HSM). Upload the private key to the runner (secure secret), import during CI run, sign using `gpg --batch --import` then run script with `--sign --key <KEYID>`.
* If gpg is not available, you can fallback to generating `metadata.json` and computing `metadata.json.sha256` and uploading both — less strong but usable.

**Example usage**

```bash
python3 tools/build_audit_pack.py \
  --claim-id matriz-p95-2025q3 \
  --artifacts release_artifacts/perf/2025-10-26-matriz-smoke.json release_artifacts/audit/guardian-compliance-2025-Q3.pdf \
  --out audit-packs/matriz-p95-2025q3.zip \
  --sign
```

### B. Sample GitHub Actions job fragment (run in a release workflow)

Add to your release workflow to build & sign audit pack:

```yaml
- name: Import GPG key
  if: secrets.SIGNING_KEY != ''
  run: |
    echo "$SIGNING_KEY" | gpg --batch --import
  env:
    SIGNING_KEY: ${{ secrets.SIGNING_KEY }}

- name: Build audit pack
  run: |
    python3 tools/build_audit_pack.py \
      --claim-id matriz-p95-2025q3 \
      --artifacts release_artifacts/perf/2025-10-26-matriz-smoke.json \
      --out audit-packs/matriz-p95-2025q3.zip \
      --sign --key $GPG_KEYID
  env:
    GPG_KEYID: ${{ secrets.GPG_KEYID }}
```

**Secrets**

* `SIGNING_KEY` — ASCII-armored private key (secure)
* `GPG_KEYID` — key identifier

**T4 note**: Use ephemeral GPG keys for CI if possible, store revocation policy, and rotate keys periodically.

---

## 3) Reasoning Lab: Redaction Slider spec + React skeleton

This is the UX + implementation skeleton that you can plug into your front-end. It supports:

* Modes: `public`, `developer`, `enterprise`
* Redaction slider (0–100) controlling granularity of redaction
* Keyboard + screen-reader accessible controls
* “Why?” side panel for node details
* Assistive mode behavior: default linear view and reduced motion

### A. UX Spec — core behaviors

**Modes**

* `public`: default. High redaction. Nodes show short labels only. No raw sources. “Why?” opens an explanation but not raw artifacts. Export disabled.
* `developer`: medium redaction. Nodes include technical IDs and performance metrics. Developers can run queries, export small traces.
* `enterprise`: low redaction. Full node metadata, signed artifacts view, `Request Audit` button visible. Requires role-based auth.

**Redaction slider**

* Range 0 (no redaction) → 100 (max redaction).
* Maps to an internal `redactionLevel` parameter that controls how node content is masked:

  * 0–10: full details (dev/enterprise)
  * 11–50: partial details (dev)
  * 51–100: high redaction (public)
* Slider keyboard: left/right arrow step ±1, PgUp/PgDn ±10, Home → 0, End → 100.
* Slider must be labeled and have aria-valuenow and aria-valuetext.

**Assistive Mode**

* On Assistive Mode, UI defaults to `public` but an alternate **linear view** is shown: each reasoning step as numbered list with 1–2 sentence description and explicit `Why?` content. Slider hidden (not necessary) but an "Explain step-by-step" toggle present.

**Side panel**

* Opens from a node `Why?` button. Contains:

  * Node name
  * Short narrative (1–2 sentences)
  * Sources (linked to evidence pages if allowed by mode)
  * Performance metrics (if available and permitted)
  * Keyboard focus trap while open; Escape closes.

**Accessibility**

* All controls focusable and labeled.
* Expose textual transcript of graph for screen readers: `aria-live="polite"` updates when node selected.
* Respect prefers-reduced-motion: animated transitions disabled.

**Audit request**

* In `enterprise` mode `Request Audit` posts to `/api/audit-requests` with `trace_id` and user/project metadata.

---

### B. React skeleton (TypeScript) — `src/components/ReasoningLab/ReasoningLab.tsx`

Place as `src/components/ReasoningLab/ReasoningLab.tsx`. This is minimal but functional; integrate into your UI and style using your design tokens.

```tsx
// src/components/ReasoningLab/ReasoningLab.tsx
import React, { useState, useMemo, useEffect } from "react";

export type Mode = "public" | "developer" | "enterprise";

export interface Node {
  id: string;
  label: string;
  detail?: string;
  sources?: string[]; // artifact links
  meta?: any;
}

export interface Trace {
  id: string;
  nodes: Node[];
  edges: Array<{ from: string; to: string }>;
  metadata?: any;
}

export interface Props {
  trace: Trace | null;
  initialMode?: Mode;
  initialRedaction?: number; // 0..100
  onRequestAudit?: (traceId: string) => Promise<void>;
  assistive?: boolean;
}

// redact helper - returns redacted node detail based on level
function redactText(text: string | undefined, level: number) {
  if (!text) return "";
  if (level >= 75) {
    // heavy redact -> short summary
    return text.split(".")[0] + " (redacted)";
  }
  if (level >= 40) {
    // partial redact -> remove specifics
    return text.replace(/\b([A-Za-z0-9_\-./]+)\b/g, "[redacted]");
  }
  return text;
}

export const ReasoningLab: React.FC<Props> = ({
  trace,
  initialMode = "public",
  initialRedaction = 80,
  onRequestAudit,
  assistive = false,
}) => {
  const [mode, setMode] = useState<Mode>(initialMode);
  const [redaction, setRedaction] = useState<number>(initialRedaction);
  const [selectedNode, setSelectedNode] = useState<Node | null>(null);
  const [panelOpen, setPanelOpen] = useState(false);

  useEffect(() => {
    if (assistive) {
      // In assistive mode, prefer linear view; ensure high redaction
      setMode("public");
      setRedaction(90);
    }
  }, [assistive]);

  const nodes = trace?.nodes || [];

  const displayedNodes = useMemo(() => {
    return nodes.map((n) => ({
      ...n,
      label: redactText(n.label, redaction),
      detail: redactText(n.detail, redaction),
    }));
  }, [nodes, redaction]);

  function openPanel(node: Node) {
    setSelectedNode(node);
    setPanelOpen(true);
  }
  function closePanel() {
    setPanelOpen(false);
    setSelectedNode(null);
  }

  async function handleRequestAudit() {
    if (!trace) return;
    if (!onRequestAudit) {
      alert("Audit request not configured.");
      return;
    }
    try {
      await onRequestAudit(trace.id);
      alert("Audit request submitted. Check your email/console for updates.");
    } catch (err) {
      console.error(err);
      alert("Audit request failed.");
    }
  }

  // Keyboard handler for slider (left/right)
  function onSliderKey(e: React.KeyboardEvent<HTMLInputElement>) {
    const step = e.shiftKey ? 10 : 1;
    if (e.key === "ArrowLeft") setRedaction((r) => Math.min(100, Math.max(0, r + step)));
    if (e.key === "ArrowRight") setRedaction((r) => Math.min(100, Math.max(0, r - step)));
    if (e.key === "Home") setRedaction(100);
    if (e.key === "End") setRedaction(0);
  }

  return (
    <div className="reasoning-lab" role="application" aria-label="Reasoning Lab">
      <div className="rl-header" style={{ display: "flex", gap: 12, alignItems: "center" }}>
        <div role="tablist" aria-label="Modes">
          <button onClick={() => setMode("public")} aria-pressed={mode === "public"}>Public</button>
          <button onClick={() => setMode("developer")} aria-pressed={mode === "developer"}>Developer</button>
          <button onClick={() => setMode("enterprise")} aria-pressed={mode === "enterprise"}>Enterprise</button>
        </div>

        <div style={{ marginLeft: "auto", display: "flex", gap: 8, alignItems: "center" }}>
          {!assistive && (
            <div className="redaction-control" aria-label="Redaction level">
              <label htmlFor="redaction-slider">Redaction</label>
              <input
                id="redaction-slider"
                type="range"
                min={0}
                max={100}
                value={redaction}
                onChange={(e) => setRedaction(parseInt(e.target.value))}
                onKeyDown={onSliderKey}
                aria-valuemin={0}
                aria-valuemax={100}
                aria-valuenow={redaction}
                aria-valuetext={`${redaction}% redaction`}
              />
            </div>
          )}
          {mode === "enterprise" && (
            <button onClick={() => handleRequestAudit()}>Request Audit</button>
          )}
        </div>
      </div>

      <div className="rl-main" style={{ display: "flex", gap: 16, marginTop: 16 }}>
        <div className="rl-graph" style={{ flex: 1 }}>
          {/* simple list view as placeholder for network visualization */}
          <ol aria-label="Reasoning steps">
            {displayedNodes.map((n) => (
              <li key={n.id} style={{ marginBottom: 12 }}>
                <div style={{ display: "flex", alignItems: "center", gap: 8 }}>
                  <div style={{ fontWeight: 600 }}>{n.label}</div>
                  <button onClick={() => openPanel(n)} aria-haspopup="dialog">Why?</button>
                </div>
                {mode !== "public" && (
                  <div style={{ marginTop: 6, color: "#888" }}>{n.detail}</div>
                )}
              </li>
            ))}
          </ol>
        </div>

        <aside
          className="rl-panel"
          style={{ width: 420, borderLeft: "1px solid #222", paddingLeft: 12 }}
          aria-hidden={!panelOpen}
        >
          {panelOpen && selectedNode ? (
            <div role="dialog" aria-label={`Details for ${selectedNode.label}`}>
              <button onClick={() => closePanel()} aria-label="Close panel">Close</button>
              <h3>{selectedNode.label}</h3>
              <p>{selectedNode.detail}</p>
              <h4>Sources</h4>
              <ul>
                {selectedNode.sources?.map((s, idx) => (
                  <li key={idx}><a href={s} target="_blank" rel="noreferrer">{s}</a></li>
                )) || <li>No sources available</li>}
              </ul>
              {mode !== "public" && selectedNode.meta && (
                <>
                  <h4>Meta</h4>
                  <pre style={{ whiteSpace: "pre-wrap" }}>{JSON.stringify(selectedNode.meta, null, 2)}</pre>
                </>
              )}
            </div>
          ) : (
            <div><em>Select a step and press "Why?" to open details.</em></div>
          )}
        </aside>
      </div>
    </div>
  );
};
```

**Notes**

* This skeleton is intentionally simple so it can be integrated with any graph visualization (D3/React-Flow/Visx) later.
* The `redactText` implementation is a placeholder; in production you should redact by metadata fields (e.g., remove `sources` arrays, mask PII, replace IDs with `[redacted]`), not by regex-only.

### C. Server-side audit request stub

Create an API route `/api/audit-requests` to accept audited packs. Example Node/Express:

```js
// server/auditRequests.js (Express)
import express from 'express';
import { spawn } from 'child_process';
const router = express.Router();

router.post('/', async (req, res) => {
  const { traceId, userId, notes } = req.body;
  // Validate user, role, etc. (must be enterprise)
  if (!req.user || !req.user.roles.includes('enterprise')) {
    return res.status(403).json({error: 'Forbidden'});
  }
  // Enqueue audit job (example)
  // auditQueue.add({ traceId, userId, notes });
  return res.json({ status: 'queued', traceId });
});

export default router;
```

In production, an audit request should:

* validate access (enterprise role),
* snapshot artifacts,
* call the Audit Pack Builder to build & sign the pack,
* notify the requestor with a secure download link.

---

## How to wire everything together (implementation notes / checklist)

1. **Evidence pages**: Run `tools/generate_claims_registry.py` then `tools/generate_evidence_pages.py`. Fill in `release_artifacts/evidence/<id>.md` with methodology, attach or point to artifacts, and run `tools/build_audit_pack.py` to create signed packs when ready.

2. **Audit workflow**: On verified claim release, have CI build an audit pack and place it in `audit-packs/` and update `evidence` page `signature` front-matter.

3. **Reasoning Lab**: Integrate React component in your front-end shell. Replace the simple list with a network visualization (React-Flow). Hook `onRequestAudit` to your API endpoint.

4. **CI / Secrets**: For signing, store `SIGNING_KEY` and `GPG_KEYID` as GitHub Secrets. Import in workflow then run build script.

5. **Testing**:

   * Unit test the redaction behavior for edge cases (PII detection).
   * Run accessibility tests on Reasoning Lab component (keyboard nav and ARIA labels).
   * Run audit pack builder locally and verify signatures.

---

## Short smoke commands to test locally

```bash
# 1. Generate registry & evidence stubs
python3 tools/generate_claims_registry.py
python3 tools/generate_evidence_pages.py

# 2. Fill an evidence page (release_artifacts/evidence/matriz-p95-...md) with real artifact links

# 3. Build audit pack (example):
python3 tools/build_audit_pack.py --claim-id matriz-p95-2025q3 \
  --artifacts release_artifacts/perf/2025-10-26-matriz-smoke.json release_artifacts/audit/guardian-compliance-2025-Q3.pdf \
  --out audit-packs/matriz-p95-2025q3.zip

# 4. Run a quick dev server and load ReasoningLab (integrate React component)
# Example: add ReasoningLab to a page and pass a mock trace to test UI.
```

---

## Final T4 notes and small guardrails

* **Do not** use evidence pages as a marketing hedge — each evidence page must be *technical and precise*. Keep marketing promises linked to them, not the other way around.
* **Sign artifacts**. A simple hash doesn’t protect against history or accidental overwrites. Signed metadata dramatically raises the trust bar.
* **Redaction policy should be deterministic and auditable.** Implement redaction rules on server-side (e.g., remove `sources` fields for public results) so that clients cannot accidentally leak data by changing slider values on the client.

---
