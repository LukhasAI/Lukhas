# T4 Unused Imports Platform Policy & Tooling
This collection of files implements the T4 Unused Imports Platform Policy, replacing the previous unused imports system with structured JSON inline annotations, an Intent Registry, and updated tooling for annotation and validation.
---

## 1) `docs/policies/T4_UNUSED_IMPORTS_PLATFORM.md`

````markdown
# T4 â€” Unused Imports Platform Policy (v1.0)

**Mission.** Transform F401 "unused import" findings in production lanes into structured, actionable, and governed technical intent.
T4 is a platform: annotations in code are the human-facing representation; the **Intent Registry** is the authoritative catalog.

## Scope
**Production lanes**: `lukhas/`, `core/`, `api/`, `consciousness/`, `memory/`, `identity/`, `MATRIZ/`  
**Excluded**: `candidate/`, `labs/`, `archive/`, `quarantine/`, `.venv/`, `node_modules/`, `.git/`, `reports/`

## Core concepts
- **Annotation**: a single-line JSON inline annotation on the import line with tag `TODO[T4-UNUSED-IMPORT]`.
- **Intent Registry**: a structured database (SQLite/JSONL) ingesting audit logs and inline annotations; source of truth for states and metrics.
- **Lifecycle**: `reserved` â†’ `planned` (ticket) â†’ `committed` (PR) â†’ `implemented` (import used/annotation removed) â†’ `expired` (abandoned).
- **Quality rules**: for `planned` and `committed`, `owner` and `ticket` required. All waivers must have TTL/expiry.

## Annotation schema (inline JSON)
**Example**:
```py
# TODO[T4-UNUSED-IMPORT]: {"id":"t4-9f1a2b3c","reason_category":"MATRIZ","reason":"MATRIZ-R2 trace routing","owner":"@ana","ticket":"GH-334","eta":"2025-12-01","status":"reserved","created_at":"2025-10-01T12:00:00Z"}
from MATRIZ.router import TraceRouter
````

### Fields

* `id` (string): unique id `t4-<hex>` (required)
* `reason_category` (string): one of `MATRIZ|CONSTELLATION|CORE_INFRA|API|BIO_QUANTUM|OTHER` (recommended)
* `reason` (string): human-friendly purpose (required)
* `owner` (string|null): GitHub handle (required for `planned/committed`)
* `ticket` (string|null): reference (e.g., `GH-334`) (required for `planned/committed`)
* `eta` (date|null): target implementation date (ISO `YYYY-MM-DD`)
* `status` (string): one of `reserved|planned|committed|implemented|expired` (required)
* `created_at` (ISO timestamp): generated by annotator (required)
* `modified_at` (ISO timestamp): updated when status changes (tool fills)

### Human rules

* Do not include secrets or sensitive data in `reason`.
* Avoid generic `reason` like â€œkept for futureâ€ â€” the validator will flag such entries.
* File-level waivers require `expires` in `AUDIT/waivers/unused_imports.yaml`.

## Tooling & automation

* **Annotator**: `tools/ci/unused_imports.py` writes structured inline JSON when annotating.
* **Validator**: `tools/ci/check_unused_imports_todo.py` parses inline JSON, enforces schema, and fails CI on violations.
* **Intent Registry**: `tools/ci/intent_registry.py` ingests `reports/todos/unused_imports.jsonl` into a SQLite DB and provides queries for stale items and metrics.
* **CI & pre-commit**: Pre-commit hooks propose quick-fixes and CI runs validator on PRs touching production lanes. PRs that add `status=reserved` without `owner` should create an issue or be blocked.

## Metrics

* **F401 elimination rate** (production lanes).
* **Annotation Quality Score**: % annotated with `owner`+`ticket`+`eta`.
* **Time-to-Implement**: average time `reserved` â†’ `implemented`.
* **Stale annotations** (> 90 days).
* **Waiver usage**.

## Migration

1. Update annotator to write JSON-inline and preserve audit logs.
2. Ingest current `reports/todos/unused_imports.jsonl` into Intent Registry; normalize old free-text reasons into `status=reserved`.
3. Run validator in CI in `--json-only` or `--dry-run` for an initial observation window (2â€“4 weeks), then enable strict mode.
4. Monthly cleanup: convert generic reasons into tickets or expire them.

## Governance

* `reserved` defaults to a 30-day review. Owners must convert `reserved` â†’ `planned` (with ticket) or mark as `expired`.
* File-level waivers must include `expires`.
* Architecture guild conducts monthly audits of T4 annotations.

## Operational guidance

* Start with **dry-run** for 2â€“4 weeks to collect data and tune the validator.
* Add `tools/ci/intent_registry.py` to the maintenance tasks and dashboard pipeline.
* Use the dashboard to prioritize high-impact `reserved` annotations to convert into work.

---

**Last Updated**: (update when you commit).
**Maintainers**: LUKHAS AI Engineering / Architecture.

````

---

## 2) `tools/ci/unused_imports.py` (annotator â€” full replacement)

> This script is a carefully merged version of the original annotator with **structured JSON inline annotations**. It preserves the original behavior (ruff invocation, waivers, header) and writes compact JSON inline tags.

```python
#!/usr/bin/env python3
"""
T4 structured unused-imports annotator.

- Scans for Ruff F401 in selected roots (default: lukhas, MATRIZ)
- Skips noisy trees (candidate, archive, quarantine, .venv, node_modules, reports, .git)
- If an F401 is found, adds an inline structured JSON TODO tag:
  # TODO[T4-UNUSED-IMPORT]: {"id":"t4-...", "reason":"...", ...}
- Ensures a small header block exists at top of file once
- Logs each action to reports/todos/unused_imports.jsonl
- --dry-run: preview only
"""
from __future__ import annotations

import argparse
import json
import re
import subprocess
import sys
from pathlib import Path
from datetime import datetime, timezone
import uuid

REPO = Path(__file__).resolve().parents[2]
REPORTS_DIR = REPO / "reports" / "todos"
REPORTS_DIR.mkdir(parents=True, exist_ok=True)
LOG = REPORTS_DIR / "unused_imports.jsonl"
WAIVERS = REPO / "AUDIT" / "waivers" / "unused_imports.yaml"

DEFAULT_ROOTS = ["lukhas", "MATRIZ"]
SKIP_DIRS = {".git", ".venv", "node_modules", "archive", "quarantine", "labs", "reports"}
HEADER_BLOCK = (
    "# ---\n"
    "# TODO[T4-UNUSED-IMPORT]: Structured JSON annotation required.\n"
    "# Schema: id, reason_category, reason, owner, ticket, eta, status, created_at\n"
    "# ---\n"
)

UNUSED_IMPORT_TAG = "TODO[T4-UNUSED-IMPORT]"
# Matches: "# TODO[T4-UNUSED-IMPORT]: {...}" capturing the {...}
INLINE_RE = re.compile(rf"#\s*{re.escape(UNUSED_IMPORT_TAG)}\s*:\s*(\{{.*\}})\s*$")
IMPORT_RE = re.compile(r"^\s*(from\s+\S+\s+import\s+.+|import\s+\S+.*)$")

def load_waivers() -> dict[str, set[int]]:
    try:
        import yaml  # type: ignore
    except Exception:
        return {}
    if not WAIVERS.exists():
        return {}
    try:
        data = yaml.safe_load(WAIVERS.read_text(encoding="utf-8")) or {}
    except Exception:
        return {}
    out: dict[str, set[int]] = {}
    for it in data.get("waivers", []):
        p = (REPO / it["file"]).resolve()
        out.setdefault(str(p), set()).add(int(it.get("line", 0)))
    return out

def run_ruff_f401(paths: list[str]) -> list[dict]:
    cmd = ["python3", "-m", "ruff", "check", "--select", "F401", "--output-format", "json", *paths]
    proc = subprocess.run(cmd, cwd=REPO, capture_output=True, text=True, check=False)
    if proc.returncode not in (0, 1):  # 0=clean, 1=findings
        print(proc.stderr or proc.stdout, file=sys.stderr)
        sys.exit(proc.returncode)
    try:
        return json.loads(proc.stdout or "[]")
    except json.JSONDecodeError:
        return []

def path_is_skipped(p: Path) -> bool:
    parts = set(p.parts)
    return bool(parts & SKIP_DIRS)

def ensure_header(text: str) -> str:
    return text if UNUSED_IMPORT_TAG in text else (HEADER_BLOCK + text)

def iso_now():
    return datetime.now(timezone.utc).isoformat(timespec="seconds")

def make_id() -> str:
    return f"t4-{uuid.uuid4().hex[:8]}"

def annotate_line_structured(text: str, line_no: int, reason: str, reason_category: str = "OTHER", owner: str | None = None, ticket: str | None = None, eta: str | None = None):
    lines = text.splitlines()
    idx = line_no - 1
    if idx < 0 or idx >= len(lines):
        return text, False, None
    line = lines[idx]
    if INLINE_RE.search(line):
        # Already has a structured inline annotation
        return text, False, None
    if not IMPORT_RE.match(line):
        return text, False, None

    entry = {
        "id": make_id(),
        "reason_category": reason_category,
        "reason": reason,
        "owner": owner,
        "ticket": ticket,
        "eta": eta,
        "status": "reserved",
        "created_at": iso_now(),
    }
    json_compact = json.dumps(entry, separators=(",", ":"), ensure_ascii=False)
    lines[idx] = f"{line}  # {UNUSED_IMPORT_TAG}: {json_compact}"
    new_text = "\n".join(lines)
    if not text.endswith("\n"):
        new_text += "\n"
    return new_text, True, entry

def main():
    ap = argparse.ArgumentParser(description="Annotate or enforce tracking tags for unused imports (F401).")
    ap.add_argument("--paths", nargs="+", default=DEFAULT_ROOTS, help="Roots to scan (default: lukhas MATRIZ).")
    ap.add_argument(
        "--reason",
        default="kept pending MATRIZ wiring (document or remove)",
        help="Reason appended to the tracking tag.",
    )
    ap.add_argument("--reason_category", default="CORE_INFRA", help="Reason category for the structured annotation.")
    ap.add_argument("--owner", default=None, help="Optional owner handle (e.g., @alice).")
    ap.add_argument("--ticket", default=None, help="Optional ticket reference (e.g., GH-123).")
    ap.add_argument("--eta", default=None, help="Optional ETA in YYYY-MM-DD.")
    ap.add_argument("--strict", action="store_true", help="Exit non-zero if any F401 remain unannotated.")
    ap.add_argument("--dry-run", action="store_true", help="Do not write changes; only print actions.")
    args = ap.parse_args()

    # Resolve roots: filter non-existing or skipped
    roots: list[str] = []
    for root_path in args.paths:
        root_path = root_path.strip()
        if not root_path or root_path in SKIP_DIRS:
            continue
        abs_r = (REPO / root_path).resolve()
        if abs_r.exists():
            roots.append(str(abs_r.relative_to(REPO)))

    if not roots:
        print("No valid roots to scan. Exiting.")
        sys.exit(0)

    waivers = load_waivers()
    findings = run_ruff_f401(roots)

    remaining_unannotated: list[str] = []
    existing_log = ""
    if LOG.exists():
        try:
            existing_log = LOG.read_text(encoding="utf-8", errors="ignore")
        except Exception:
            existing_log = ""

    edits = 0
    new_entries: list[dict] = []

    for it in findings:
        file_path = (REPO / it["filename"]).resolve()
        line = int(it["location"]["row"])
        msg = it.get("message", "F401 unused import")

        if path_is_skipped(file_path):
            continue
        # Waivers: file-level (line=0) or specific line
        if str(file_path) in waivers and (0 in waivers[str(file_path)] or line in waivers[str(file_path)]):
            continue

        try:
            code = file_path.read_text(encoding="utf-8", errors="ignore")
        except Exception:
            remaining_unannotated.append(f"{file_path}:{line} {msg} (unreadable)")
            continue

        # Already annotated?
        try:
            target_line = code.splitlines()[line - 1]
            already = bool(INLINE_RE.search(target_line)) or not IMPORT_RE.match(target_line)
        except Exception:
            already = False

        if already:
            continue

        # Annotate
        new_code, changed, entry = annotate_line_structured(
            code,
            line,
            reason=args.reason,
            reason_category=args.reason_category,
            owner=args.owner,
            ticket=args.ticket,
            eta=args.eta,
        )
        if changed:
            new_code = ensure_header(new_code)
            if args.dry_run:
                print(f"[DRY-RUN] Would annotate {file_path}:{line} -> {UNUSED_IMPORT_TAG} id={entry.get('id')}")
            else:
                try:
                    file_path.write_text(new_code, encoding="utf-8")
                except Exception as e:
                    remaining_unannotated.append(f"{file_path}:{line} {msg} (write-failed: {e})")
                    continue
            edits += 1

            log_entry = {
                "id": entry.get("id"),
                "file": str(file_path.relative_to(REPO)),
                "line": line,
                "reason_category": entry.get("reason_category"),
                "reason": entry.get("reason"),
                "owner": entry.get("owner"),
                "ticket": entry.get("ticket"),
                "eta": entry.get("eta"),
                "status": entry.get("status"),
                "message": msg,
                "timestamp": entry.get("created_at"),
                "tool": "T4-unused-imports-annotator",
            }
            new_entries.append(log_entry)
        else:
            remaining_unannotated.append(f"{file_path}:{line} {msg} (could not annotate)")

    # Append to log in an idempotent way
    if not args.dry_run:
        try:
            with LOG.open("a", encoding="utf-8") as fh:
                for e in new_entries:
                    fh.write(json.dumps(e, ensure_ascii=False) + "\n")
        except Exception as e:
            print("Failed to write log:", e, file=sys.stderr)

    print(f"Annotated {edits} unused import(s). Log: {LOG}")
    if args.strict and remaining_unannotated:
        print("Unannotated F401 findings:", *remaining_unannotated, sep="\n")
        sys.exit(1)

if __name__ == "__main__":
    main()
````

---

## 3) `tools/ci/check_unused_imports_todo.py` (validator â€” full replacement)

> Validator now parses structured inline JSON, validates schema rules (owner/ticket for planned/committed), and reports quality issues. Respects waivers.

```python
#!/usr/bin/env python3
"""
T4 UNUSED IMPORTS VALIDATOR - Production Lane Policy Enforcement

- Runs ruff F401 to find unused imports in selected roots (default: production lanes)
- Checks each finding has a TODO[T4-UNUSED-IMPORT] structured JSON annotation (or acceptable legacy tag)
- Outputs JSON report for CI/CD integration
- Enforces production lane policy (candidate/experimental code exempt)
"""
from __future__ import annotations

import argparse
import json
import re
import subprocess
import sys
from pathlib import Path
from datetime import datetime, timezone

REPO = Path(__file__).resolve().parents[2]
TODO_TAG = "TODO[T4-UNUSED-IMPORT]"
INLINE_JSON_RE = re.compile(rf"#\s*{re.escape(TODO_TAG)}\s*:\s*(\{{.*\}})\s*$")
SKIP_DIRS = {".git", ".venv", "node_modules", "archive", "quarantine", "labs", "reports"}
WAIVERS = REPO / "AUDIT" / "waivers" / "unused_imports.yaml"

def load_waivers() -> dict[str, set[int]]:
    try:
        import yaml  # type: ignore
    except Exception:
        return {}
    if not WAIVERS.exists():
        return {}
    try:
        data = yaml.safe_load(WAIVERS.read_text(encoding="utf-8")) or {}
    except Exception:
        return {}
    out: dict[str, set[int]] = {}
    for it in data.get("waivers", []):
        p = (REPO / it["file"]).resolve()
        out.setdefault(str(p), set()).add(int(it.get("line", 0)))
    return out

def parse_inline_json(line: str):
    m = INLINE_JSON_RE.search(line)
    if not m:
        return None
    try:
        return json.loads(m.group(1))
    except Exception:
        return None

def validate_entry(entry: dict):
    errors = []
    if not isinstance(entry, dict):
        errors.append("annotation is not a JSON object")
        return errors
    if "id" not in entry:
        errors.append("missing id")
    reason = entry.get("reason", "")
    if not reason or not isinstance(reason, str) or reason.strip().lower() in ("kept for future", "kept for future use", "kept for future use."):
        errors.append("reason missing or generic")
    if "status" not in entry:
        errors.append("missing status")
    if entry.get("status") in ("planned", "committed"):
        if not entry.get("owner"):
            errors.append("planned/committed entries must have owner")
        if not entry.get("ticket"):
            errors.append("planned/committed entries must have ticket")
    return errors

def ruff_F401(paths):
    cmd = ["python3", "-m", "ruff", "check", "--select", "F401", "--output-format", "json", *list(paths)]
    proc = subprocess.run(cmd, cwd=REPO, capture_output=True, text=True)
    if proc.returncode not in (0, 1):
        return {"error": f"ruff failed: {proc.stderr or proc.stdout}"}
    try:
        items = json.loads(proc.stdout or "[]")
    except Exception as e:
        return {"error": f"Failed to parse ruff output: {e}"}
    findings = []
    for item in items:
        file_path = (REPO / item["filename"]).resolve()
        # Skip if any path segment is in SKIP_DIRS
        if set(file_path.parts) & SKIP_DIRS:
            continue
        findings.append(
            {
                "file": str(file_path.relative_to(REPO)),
                "abs_path": str(file_path),
                "line": item["location"]["row"],
                "message": item["message"],
            }
        )
    return {"findings": findings}

def main():
    parser = argparse.ArgumentParser()
    parser.add_argument("--paths", nargs="+", default=["lukhas", "core", "api", "consciousness", "memory", "identity", "MATRIZ"])
    parser.add_argument("--json-only", action="store_true")
    args = parser.parse_args()

    valid_roots = []
    for path_arg in args.paths:
        if path_arg.strip() in SKIP_DIRS or path_arg.strip() == "labs":
            continue
        abs_path = (REPO / path_arg).resolve()
        if abs_path.exists():
            valid_roots.append(str(abs_path.relative_to(REPO)))

    if not valid_roots:
        result = {"status": "error", "message": "No valid production roots to validate", "unannotated": [], "summary": {"total": 0, "annotated": 0, "missing": 0}}
        print(json.dumps(result, indent=2))
        sys.exit(1)

    waivers = load_waivers()
    ruff_result = ruff_F401(valid_roots)
    if "error" in ruff_result:
        result = {"status": "error", "message": ruff_result["error"], "unannotated": [], "summary": {"total": 0, "annotated": 0, "missing": 0}}
        print(json.dumps(result, indent=2))
        sys.exit(1)

    findings = ruff_result["findings"]

    unannotated = []
    annotated_count = 0
    quality_issues = []

    for f in findings:
        file_abs = Path(f["abs_path"])
        # Waiver: if file is waived (0) or that exact line is waived, skip
        if str(file_abs) in waivers and (0 in waivers[str(file_abs)] or f["line"] in waivers[str(file_abs)]):
            continue
        try:
            lines = file_abs.read_text(encoding="utf-8", errors="ignore").splitlines()
            idx = f["line"] - 1
            if idx < 0 or idx >= len(lines):
                unannotated.append({"file": f["file"], "line": f["line"], "message": "line out of range"})
                continue
            line_content = lines[idx]
            entry = parse_inline_json(line_content)
            if not entry:
                # Accept legacy non-JSON that contains TODO[T4-UNUSED-IMPORT] but flag as low-quality
                if "TODO[T4-UNUSED-IMPORT]" in line_content:
                    annotated_count += 1
                    quality_issues.append({"file": f["file"], "line": f["line"], "issues": ["legacy annotation (non-structured) - migrate to structured JSON"]})
                else:
                    unannotated.append({"file": f["file"], "line": f["line"], "message": f["message"]})
            else:
                annotated_count += 1
                issues = validate_entry(entry)
                if issues:
                    quality_issues.append({"file": f["file"], "line": f["line"], "issues": issues})
        except Exception:
            unannotated.append({"file": f["file"], "line": f["line"], "message": "unreadable"})

    total = len(findings)
    missing = len(unannotated)

    result = {
        "status": "pass" if missing == 0 and not quality_issues else "fail",
        "message": f"Production lane policy: {annotated_count}/{total} imports properly annotated",
        "annotated": annotated_count,
        "total": total,
        "missing": missing,
        "quality_issues_count": len(quality_issues),
        "unannotated": unannotated,
        "quality_issues": quality_issues,
    }

    if args.json_only:
        print(json.dumps(result, indent=2))
    else:
        print("\nðŸ” T4 UNUSED IMPORTS VALIDATOR - Production Lane Policy")
        print("=" * 60)
        print(json.dumps(result, indent=2))

    sys.exit(0 if result["status"] == "pass" else 1)

if __name__ == "__main__":
    main()
```

---

## 4) `tools/ci/intent_registry.py` (new)

> Small tool to ingest `reports/todos/unused_imports.jsonl` into a SQLite DB for queries and dashboards.

```python
#!/usr/bin/env python3
"""
Intent Registry - ingest T4 annotations log into a SQLite DB for queries and reporting.
"""
import sqlite3
import json
from pathlib import Path

REPO = Path(__file__).resolve().parents[2]
LOG = REPO / "reports" / "todos" / "unused_imports.jsonl"
DB = REPO / "reports" / "todos" / "intent_registry.db"

def init_db(conn):
    conn.execute("""
    CREATE TABLE IF NOT EXISTS intents (
      id TEXT PRIMARY KEY,
      file TEXT,
      line INTEGER,
      import_text TEXT,
      reason_category TEXT,
      reason TEXT,
      owner TEXT,
      ticket TEXT,
      eta TEXT,
      status TEXT,
      created_at TEXT,
      raw JSON
    )
    """)
    conn.commit()

def normalize_log_obj(obj):
    # Given an older-format log or new structured entry, map to schema
    id_ = obj.get("id") or f"t4-{abs(hash(obj.get('file','') + str(obj.get('line',''))))}"
    return {
        "id": id_,
        "file": obj.get("file"),
        "line": int(obj.get("line") or 0),
        "import_text": obj.get("message"),
        "reason_category": obj.get("reason_category"),
        "reason": obj.get("reason"),
        "owner": obj.get("owner"),
        "ticket": obj.get("ticket"),
        "eta": obj.get("eta"),
        "status": obj.get("status", "reserved"),
        "created_at": obj.get("timestamp") or obj.get("created_at"),
        "raw": json.dumps(obj),
    }

def ingest():
    conn = sqlite3.connect(str(DB))
    init_db(conn)
    if not LOG.exists():
        print("No log found:", LOG)
        return
    inserted = 0
    with LOG.open("r", encoding="utf-8") as fh:
        for line in fh:
            if not line.strip():
                continue
            try:
                obj = json.loads(line)
            except Exception:
                continue
            row = normalize_log_obj(obj)
            try:
                conn.execute("""
                INSERT OR REPLACE INTO intents (id,file,line,import_text,reason_category,reason,owner,ticket,eta,status,created_at,raw)
                VALUES (?,?,?,?,?,?,?,?,?,?,?,?)
                """, (
                    row["id"], row["file"], row["line"], row["import_text"], row["reason_category"], row["reason"],
                    row["owner"], row["ticket"], row["eta"], row["status"], row["created_at"], row["raw"]
                ))
                inserted += 1
            except Exception as e:
                print("DB insert error:", e, row.get("id"))
    conn.commit()
    conn.close()
    print(f"Ingest finished. DB: {DB} (inserted {inserted})")

if __name__ == "__main__":
    ingest()
```

---

## 5) `scripts/replace_t4_policy.sh` (automation script for Claude Agent)

> The script creates a branch, moves old docs to backup, stages new files (assumes files exist in workspace), commits and opens a PR with `gh`. Edit `MAIN_BRANCH` if repo uses `master`.

```bash
#!/usr/bin/env bash
set -euo pipefail

MAIN_BRANCH="main" # change to `master` if needed
BRANCH="replace/t4-policy-platform-$(date +%Y%m%d%H%M%S)"
git checkout -b "$BRANCH"

# Backup old docs (if present)
mkdir -p docs/backup_t4 || true
if [ -f docs/development/T4_UNUSED_IMPORTS_SYSTEM.md ]; then
  git mv -f docs/development/T4_UNUSED_IMPORTS_SYSTEM.md docs/backup_t4/ || true
fi
if [ -f docs/T4_UNUSED_IMPORTS_GUIDE.md ]; then
  git mv -f docs/T4_UNUSED_IMPORTS_GUIDE.md docs/backup_t4/ || true
fi

# Add new files (assumes they are already created in the workspace)
git add docs/policies/T4_UNUSED_IMPORTS_PLATFORM.md
git add tools/ci/unused_imports.py
git add tools/ci/check_unused_imports_todo.py
git add tools/ci/intent_registry.py
git add .github/workflows/t4-policy-validation.yml
git add .github/PULL_REQUEST_TEMPLATE/t4_policy.md
git add scripts/replace_t4_policy.sh

git commit -m "chore(t4): replace unused-imports policy with T4 Platform; structured annotations and tooling"
git push --set-upstream origin "$BRANCH"

# Create PR (requires GitHub CLI `gh` authenticated)
if command -v gh >/dev/null 2>&1; then
  gh pr create --title "Replace T4 unused-imports policy with T4 Platform" --body-file .github/PULL_REQUEST_TEMPLATE/t4_policy.md --base "$MAIN_BRANCH"
else
  echo "gh CLI not found. Created branch '$BRANCH' and pushed. Please open a PR manually."
fi
```

---

## 6) `.github/workflows/t4-policy-validation.yml` (GitHub Action)

```yaml
name: T4 Policy Validation

on:
  pull_request:
    paths:
      - 'lukhas/**'
      - 'core/**'
      - 'api/**'
      - 'consciousness/**'
      - 'memory/**'
      - 'identity/**'
      - 'MATRIZ/**'

jobs:
  t4-check:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: "3.11"
      - name: Install deps
        run: pip install ruff pyyaml
      - name: Run T4 validator
        run: python3 tools/ci/check_unused_imports_todo.py --paths lukhas core api consciousness memory identity MATRIZ --json-only
```

---

## 7) `.github/PULL_REQUEST_TEMPLATE/t4_policy.md`

```markdown
## Summary
Replace T4 Unused Imports policy with T4 Platform (structured annotations + Intent Registry).

## Changes
- docs/policies/T4_UNUSED_IMPORTS_PLATFORM.md (new canonical policy)
- Updated tools/ci/unused_imports.py (annotator)
- Updated tools/ci/check_unused_imports_todo.py (validator)
- New tools/ci/intent_registry.py
- GitHub Action: .github/workflows/t4-policy-validation.yml
- Migration script: scripts/replace_t4_policy.sh

## How to test
1. Run: `python3 tools/ci/check_unused_imports_todo.py --paths lukhas core api consciousness memory identity MATRIZ --json-only`
2. Dry-run: `python3 tools/ci/unused_imports.py --paths lukhas MATRIZ --dry-run`
3. Ingest logs: `python3 tools/ci/intent_registry.py`

## Rollback plan
- Restore files from `docs/backup_t4/` which were moved.

## Notes
This change keeps backwards-compatibility: the annotator will accept old free-text tags but will prefer structured JSON. Start the validator in dry-run for 2â€“4 weeks to review quality issues, then enable strict CI enforcement.
```

---

### Operational instructions for Claude Agent (exact recommended sequence)

1. **Place files** above in the repository under the exact paths shown. Make sure `tools/ci/*.py`, `docs/policies/...`, `.github/...`, `scripts/...` are added.

2. **Make the scripts executable** (optional but good):

```bash
chmod +x tools/ci/unused_imports.py
chmod +x tools/ci/check_unused_imports_todo.py
chmod +x tools/ci/intent_registry.py
chmod +x scripts/replace_t4_policy.sh
```

3. **Run the safety dry-run locally**:

```bash
# Dry-run the annotator in repository
python3 tools/ci/unused_imports.py --paths lukhas MATRIZ --dry-run

# Run validator in json-only mode to gather results (won't modify files)
python3 tools/ci/check_unused_imports_todo.py --paths lukhas core api consciousness memory identity MATRIZ --json-only
```

4. **Ingest audit-log to registry**:

```bash
python3 tools/ci/intent_registry.py
```

5. **Create branch / PR**:

* Either run `scripts/replace_t4_policy.sh` (requires `gh` CLI), or run the commands manually (branch, commit, push, create PR).

6. **CI policy**:

* For first 2â€“4 weeks: let the validator run but **do not fail CI** yet â€” examine `quality_issues` output.
* After addressing noise and adjusting reasons, switch to strict fail-on-violations.

---

### Quick notes & cautions (T4 lens)

* First run will likely produce **many quality issues** (legacy annotations and generic reasons). Use the dry-run and the Intent Registry to triage and convert high-value annotations into `planned` (with owner+ticket).
* Avoid adding `status=reserved` in bulk without owners â€” prefer creating issues for unowned annotations.
* Update `AUDIT/waivers/unused_imports.yaml` with expirations for existing waivers.

---
