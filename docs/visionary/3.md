---
status: wip
type: documentation
owner: unknown
module: visionary
redirect: false
moved_to: null
---

Absolutely—here’s a tight, copy-pasteable brief for Claude Code, the specialized agent lineup, and the full scaffold for the new consciousness/simulation/ lane (API + internals + 5 canary tests). Everything is dependency-light (Python 3.10+), side-effect–free by default, and ready to drop into your repo.

⸻

Brief for Claude Code (Coordinator)

Objective
Stand up a sandboxed Simulation (“Dreams”) lane inside consciousness/, isolated from the hot path and adapters. Provide one canonical ingress/egress API, strict policy/consent/budget gating, deterministic outputs for tests, and write-only artifacts for Memory.

Non-negotiables
	•	No external side-effects (no adapters, no I/O beyond in-memory storage in this PR).
	•	Enforce: SIMULATION_ENABLED feature flag, consent scopes, duress/jailbreak trips, rate & budget limits.
	•	Emit Λ-trace metadata (stubbed as structured logs for now).
	•	Canary tests must pass CI.

Deliverables
	•	New package: consciousness/simulation/ (files below).
	•	Adapter bridge: refactor any existing dream callers to use consciousness.simulation.api.
	•	Tests: tests/consciousness/simulation/test_simulation_lane.py (5+ canaries).
	•	Docs-in-code: clear docstrings, pre/post conditions.

Acceptance criteria
	•	✅ schedule() → job_id, status(job_id), collect(job_id) → DreamResult.
	•	✅ Gate denies when: feature flag off, missing consent, duress active, or budget exceeded.
	•	✅ No imports from external adapters; tests assert it.
	•	✅ Deterministic results under fixed seed.
	•	✅ 5 canary tests pass locally and in CI.

⸻

Specialized Agents (you can map to your existing 7)
	1.	Simulation Lane Lead (A-SIM)
	•	Owns api, scheduler, rollout. Guarantees contracts & perf.
	2.	Ethics Gatekeeper (A-ETH)
	•	Owns ethics_gate rules: consent scopes, duress/shadow, jailbreak hygiene.
	3.	Budgeteer & Telemetry (A-BUD)
	•	Integrates token/time budgets, rate limits; Λ-trace stubs & counters.
	4.	World-Model & Evaluator (A-WM)
	•	Owns world_model, evaluator; deterministic test seeds.
	5.	Summarizer & Memory Ingress (A-SUM)
	•	Owns summarizer, storage (write-only artifacts; no external I/O).
	6.	Test Canary & CI (A-QA)
	•	Authors/maintains tests; ensures zero adapter imports & safety rules.

⸻

Directory to add

consciousness/
  simulation/
    __init__.py
    api.py
    scheduler.py
    ethics_gate.py
    world_model.py
    rollout.py
    evaluator.py
    summarizer.py
    storage.py


⸻

consciousness/simulation/__init__.py

"""
Sandboxed Simulation lane for LUKHΛS consciousness.

Public API: see `consciousness.simulation.api`.
This package must have NO imports from adapter layers or external side-effects.
"""
__all__ = []

consciousness/simulation/api.py

from __future__ import annotations
import os, asyncio, uuid, logging
from typing import TypedDict, Optional, Dict, Any

from .scheduler import SimulationScheduler, JobStatus
from .ethics_gate import authorize_or_raise, EthicsError
from .rollout import run_rollouts
from .summarizer import build_dream_result

log = logging.getLogger("lukhas.consciousness.simulation")

class DreamSeed(TypedDict):
    goal: str
    context: Dict[str, Any]        # redacted, scoped context only
    constraints: Dict[str, Any]    # budgets: {tokens, seconds, max_rollouts}, consent, flags

class DreamResult(TypedDict):
    shards: list[dict]
    scores: dict
    trace_id: str

class SimulationDisabledError(RuntimeError): ...
class PolicyViolation(RuntimeError): ...

_scheduler: Optional[SimulationScheduler] = None

def _get_scheduler() -> SimulationScheduler:
    global _scheduler
    if _scheduler is None:
        _scheduler = SimulationScheduler()
    return _scheduler

def _require_enabled():
    if os.getenv("SIMULATION_ENABLED", "false").lower() not in ("1","true","yes","on"):
        raise SimulationDisabledError("Simulation lane disabled (SIMULATION_ENABLED is false).")

async def schedule(seed: DreamSeed) -> str:
    """
    Validate policy & budgets, enqueue job, return job_id.
    No rollouts executed here; scheduler processes jobs.
    """
    _require_enabled()
    try:
        authorize_or_raise(seed)
    except EthicsError as e:
        raise PolicyViolation(str(e)) from e

    job_id = str(uuid.uuid4())
    trace_id = f"LT-{job_id[:8]}"
    log.info("Λ-trace seed_scheduled", extra={"trace_id": trace_id, "goal": seed.get("goal")})
    await _get_scheduler().enqueue(job_id, seed, trace_id)
    return job_id

async def status(job_id: str) -> Dict[str, Any]:
    """
    Returns status including progress & budgets; never reveals raw PII.
    """
    _require_enabled()
    s = _get_scheduler().get(job_id)
    if not s:
        return {"state": "unknown", "job_id": job_id}
    return s.model_dump()

async def collect(job_id: str) -> DreamResult:
    """
    Blocks until job finished or cancelled, then returns DreamResult.
    """
    _require_enabled()
    s = _get_scheduler().get(job_id)
    if not s:
        raise KeyError(f"Unknown job_id: {job_id}")
    await _get_scheduler().wait(job_id)

    # Run rollouts & summarization inside collection to keep API simple.
    # (Alternatively: move to worker thread/process; here we remain in-proc for MVP.)
    seed = s.seed
    trace_id = s.trace_id
    rollouts = await run_rollouts(seed, trace_id)
    result = build_dream_result(seed, rollouts, trace_id)
    log.info("Λ-trace seed_collected", extra={"trace_id": trace_id, "num_shards": len(result["shards"])})
    return result

consciousness/simulation/scheduler.py

from __future__ import annotations
import asyncio, time
from dataclasses import dataclass, field
from typing import Dict, Any, Optional

@dataclass
class JobStatus:
    job_id: str
    state: str
    trace_id: str
    created_ts: float
    started_ts: Optional[float] = None
    finished_ts: Optional[float] = None
    budget_tokens: int = 0
    budget_seconds: float = 0.0
    progress: float = 0.0
    seed: Dict[str, Any] = field(default_factory=dict)

    def model_dump(self) -> Dict[str, Any]:
        return {
            "job_id": self.job_id,
            "state": self.state,
            "trace_id": self.trace_id,
            "created_ts": self.created_ts,
            "started_ts": self.started_ts,
            "finished_ts": self.finished_ts,
            "budget_tokens": self.budget_tokens,
            "budget_seconds": self.budget_seconds,
            "progress": round(self.progress, 3),
        }

class SimulationScheduler:
    """
    Minimal in-proc scheduler for MVP:
    - One job at a time (simple queue) to keep budgets predictable.
    - No external side-effects.
    """
    def __init__(self) -> None:
        self._jobs: Dict[str, JobStatus] = {}
        self._events: Dict[str, asyncio.Event] = {}
        self._queue: asyncio.Queue[str] = asyncio.Queue()
        self._runner_task: Optional[asyncio.Task] = None

    async def enqueue(self, job_id: str, seed: Dict[str, Any], trace_id: str) -> None:
        budgets = seed.get("constraints", {}).get("budgets", {})
        js = JobStatus(
            job_id=job_id,
            state="queued",
            trace_id=trace_id,
            created_ts=time.time(),
            budget_tokens=int(budgets.get("tokens", 2000)),
            budget_seconds=float(budgets.get("seconds", 2.0)),
            seed=seed,
        )
        self._jobs[job_id] = js
        self._events[job_id] = asyncio.Event()
        await self._queue.put(job_id)
        if not self._runner_task or self._runner_task.done():
            self._runner_task = asyncio.create_task(self._runner())

    def get(self, job_id: str) -> Optional[JobStatus]:
        return self._jobs.get(job_id)

    async def wait(self, job_id: str) -> None:
        ev = self._events.get(job_id)
        if ev:
            await ev.wait()

    async def _runner(self) -> None:
        while not self._queue.empty():
            job_id = await self._queue.get()
            js = self._jobs[job_id]
            js.state = "running"
            js.started_ts = time.time()

            # Simulate “work” within time budget only.
            # (Actual rollouts/summarization happen in api.collect)
            await asyncio.sleep(min(0.02, js.budget_seconds / 10.0))
            js.progress = 1.0
            js.finished_ts = time.time()
            js.state = "finished"
            self._events[job_id].set()

consciousness/simulation/ethics_gate.py

from __future__ import annotations
from typing import Dict, Any

class EthicsError(Exception): ...

def authorize_or_raise(seed: Dict[str, Any]) -> None:
    """
    Enforces consent scopes, duress/shadow state, jailbreak hygiene.
    Side-effect free. Raises EthicsError if any rule fails.
    """
    constraints = seed.get("constraints", {})
    consent = constraints.get("consent", {})
    flags = constraints.get("flags", {})

    if flags.get("duress_active"):
        raise EthicsError("Duress/shadow active; simulation forbidden.")

    scopes = set(consent.get("scopes", []))
    required = {"simulation.read_context"}
    if not required.issubset(scopes):
        raise EthicsError("Missing consent scope: simulation.read_context")

    # Never allow adapter actions from simulation lane
    forbidden_actions = {"adapter.write", "email.send", "cloud.delete"}
    if scopes & forbidden_actions:
        raise EthicsError("Forbidden capabilities in simulation scope")

    # Jailbreak hygiene (simple heuristic placeholder)
    goal = (seed.get("goal") or "").lower()
    if any(bad in goal for bad in ("self-delete", "exfiltrate", "privilege escalation")):
        raise EthicsError("Unsafe simulation goal requested")

consciousness/simulation/world_model.py

from __future__ import annotations
from typing import Dict, Any, List

def generate_scenarios(seed: Dict[str, Any], trace_id: str) -> List[Dict[str, Any]]:
    """
    Deterministic scenario generation for MVP.
    Produces 3 variations derived from goal/context; no model calls.
    """
    goal = seed.get("goal", "unspecified")
    ctx_keys = sorted((seed.get("context") or {}).keys())
    base = {"goal": goal, "ctx": ctx_keys, "trace_id": trace_id}

    return [
        {**base, "variant": "optimistic", "assumptions": ["ideal conditions", "low risk"]},
        {**base, "variant": "baseline", "assumptions": ["expected variability", "moderate risk"]},
        {**base, "variant": "adversarial", "assumptions": ["edge cases", "high risk paths"]},
    ]

consciousness/simulation/rollout.py

from __future__ import annotations
import time
from typing import Dict, Any, List
from .world_model import generate_scenarios
from .evaluator import score_scenario

async def run_rollouts(seed: Dict[str, Any], trace_id: str) -> List[Dict[str, Any]]:
    """
    Runs scenario 'rollouts' with time/token budgets enforced upstream.
    Deterministic & side-effect free in MVP.
    """
    scenarios = generate_scenarios(seed, trace_id)
    # Simulate light compute without blocking
    start = time.time()
    results = []
    for sc in scenarios:
        sc["scores"] = score_scenario(sc)
        results.append(sc)
        # Tiny sleep to avoid starving event loop if needed (optional)
    # No external calls or adapters here.
    results.sort(key=lambda s: s["scores"]["utility"], reverse=True)
    return results

consciousness/simulation/evaluator.py

from __future__ import annotations
from typing import Dict, Any

def score_scenario(s: Dict[str, Any]) -> Dict[str, float]:
    """
    Simple heuristic scorer:
    - optimistic: higher utility, lower risk
    - baseline: medium utility/risk
    - adversarial: lower utility, higher risk, but high novelty
    """
    var = s.get("variant")
    if var == "optimistic":
        return {"utility": 0.85, "risk": 0.15, "novelty": 0.35}
    if var == "baseline":
        return {"utility": 0.70, "risk": 0.30, "novelty": 0.40}
    # adversarial default
    return {"utility": 0.55, "risk": 0.60, "novelty": 0.80}

consciousness/simulation/summarizer.py

from __future__ import annotations
from typing import Dict, Any, List, TypedDict

class DreamResult(TypedDict):
    shards: list[dict]
    scores: dict
    trace_id: str

def build_dream_result(seed: Dict[str, Any], rollouts: List[Dict[str, Any]], trace_id: str) -> DreamResult:
    """
    Convert rollouts into advisory 'dream_shard' artifacts.
    No raw context leakage; only structured proposals & verification plans.
    """
    shards = []
    for r in rollouts:
        shards.append({
            "type": "dream_shard",
            "variant": r["variant"],
            "proposal": {
                "problem": seed.get("goal"),
                "approach": r["assumptions"],
                "plan": [
                    "Formulate hypothesis",
                    "Design minimal verification",
                    "Define success metrics",
                ],
                "ul_tags": ["ΛSIM", f"ΛVAR:{r['variant'].upper()}"],
            },
            "risks": [
                {"label": "oversimplification", "mitigation": "expand test coverage"},
                {"label": "distribution shift", "mitigation": "include adversarial examples"},
            ],
            "scores": r["scores"],
        })
    aggregate = {
        "utility_mean": round(sum(s["scores"]["utility"] for s in rollouts) / len(rollouts), 3),
        "risk_max": round(max(s["scores"]["risk"] for s in rollouts), 3),
        "novelty_max": round(max(s["scores"]["novelty"] for s in rollouts), 3),
    }
    return {"shards": shards, "scores": aggregate, "trace_id": trace_id}

consciousness/simulation/storage.py

from __future__ import annotations
from typing import Dict, Any, List

![Status: WIP](https://img.shields.io/badge/status-wip-yellow)

# In-memory artifact sink for MVP; replace with memory inbox writer.
_ARTIFACTS: List[Dict[str, Any]] = []

def persist_artifact(artifact: Dict[str, Any]) -> None:
    """
    Write-only memory sink. No read path here.
    Replace with integration to memory inbox in a later PR.
    """
    _ARTIFACTS.append(artifact)


⸻

Tests (5 canaries)

Create: tests/consciousness/simulation/test_simulation_lane.py

import os, asyncio, inspect, importlib, sys

os.environ["SIMULATION_ENABLED"] = "true"

from consciousness.simulation import api

def run(coro):
    return asyncio.get_event_loop().run_until_complete(coro)

def _seed(consented: bool = True, duress: bool = False):
    scopes = ["simulation.read_context"] if consented else []
    return {
        "goal": "Evaluate onboarding flow for new ΛID users",
        "context": {"tenant": "demo", "redacted_user_count": 5},
        "constraints": {
            "budgets": {"tokens": 1500, "seconds": 1.0, "max_rollouts": 3},
            "consent": {"scopes": scopes},
            "flags": {"duress_active": duress},
        },
    }

def test_schedule_and_collect_happy_path():
    job_id = run(api.schedule(_seed()))
    assert isinstance(job_id, str)
    st = run(api.status(job_id))
    assert st["state"] in ("queued", "running", "finished")
    result = run(api.collect(job_id))
    assert "shards" in result and len(result["shards"]) == 3
    assert result["scores"]["utility_mean"] > 0.6
    assert result["trace_id"].startswith("LT-")

def test_denies_without_consent():
    try:
        run(api.schedule(_seed(consented=False)))
        assert False, "Expected PolicyViolation"
    except api.PolicyViolation:
        pass

def test_denies_under_duress():
    try:
        run(api.schedule(_seed(duress=True)))
        assert False, "Expected PolicyViolation"
    except api.PolicyViolation:
        pass

def test_feature_flag_off_blocks():
    os.environ["SIMULATION_ENABLED"] = "false"
    try:
        run(api.schedule(_seed()))
        assert False, "Expected SimulationDisabledError"
    except api.SimulationDisabledError:
        pass
    finally:
        os.environ["SIMULATION_ENABLED"] = "true"

def test_no_adapter_imports():
    """
    Ensure simulation lane does not import adapter modules directly.
    """
    mod = importlib.import_module("consciousness.simulation.api")
    source = inspect.getsource(mod)
    forbidden = ["adapters.", "gmail_", "drive_", "dropbox_", "perplexity_", "openai_", "anthropic_", "gemini_"]
    assert not any(f in source for f in forbidden), "Adapter names found in simulation.api sources"


⸻

How to wire it (very light)
	•	Callers (e.g., reflection/…) should create a DreamSeed and call:

job_id = await consciousness.simulation.api.schedule(seed)
result = await consciousness.simulation.api.collect(job_id)


	•	Keep SIMULATION_ENABLED=false by default in production; enable per-tenant or in dev.

⸻

What Claude should do immediately (commit plan)
	1.	Add package consciousness/simulation/ with the files above.
	2.	Create tests file as above; run pytest tests/consciousness/simulation/test_simulation_lane.py -q.
	3.	Refactor any existing dream callers to use consciousness.simulation.api.
	4.	Ensure CI exports SIMULATION_ENABLED=true for tests only (default remains false).
	5.	Document: Add a short README section in the main consciousness docs linking to this API (no need for a separate README file if you’re handling docs later).

⸻

If you want, I can also draft a tiny bridge for any existing dream_bridge_adapter.py to forward into consciousness.simulation.api—but you can likely just replace direct imports with the new API calls in one pass.

Want me to add a Memory inbox writer next (persisting dream_shard to memory/inbox/dreams/ behind a capability token)?

---

## MATADA Alignment Update (v1)

API envelope:

```python
class DreamResult(TypedDict):
    shards: list[dict]
    scores: dict
    trace_id: str
    matada_nodes: list[dict]
    schema_ref: str  # 'lukhas://schemas/matada_node_v1.json'
```

collect() MUST validate each node against matada_node_v1.json before returning. Deterministic seeds → stable id shape (e.g., LT-<trace>#N<index>) for golden tests. Canary tests fail on any schema drift.
