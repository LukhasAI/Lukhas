# TaskManager - Complete Orchestration Guide

**Version**: 1.0.0
**Status**: Production Ready
**Implementation**: [labs/core/task_manager.py](../../labs/core/task_manager.py)
**Last Updated**: 2025-01-08

---

## Table of Contents

- [Overview](#overview)
- [Core Concepts](#core-concepts)
- [Quick Start](#quick-start)
- [Task Lifecycle](#task-lifecycle)
- [Priority Management](#priority-management)
- [Queue Management](#queue-management)
- [Agent Coordination](#agent-coordination)
- [Real-World Examples](#real-world-examples)
- [Dependency Management](#dependency-management)
- [Error Handling & Retries](#error-handling--retries)
- [Monitoring & Observability](#monitoring--observability)
- [Production Patterns](#production-patterns)
- [API Reference](#api-reference)

---

## Overview

The **LUKHAS TaskManager** is a production-grade async task orchestration system designed for the symbolic AI ecosystem. It provides sophisticated workflow coordination, priority-based scheduling, and multi-agent task distribution.

### Key Features

âœ… **Priority-Based Scheduling**: 4-tier priority system (LOW, NORMAL, HIGH, CRITICAL)
âœ… **Dependency Management**: DAG-based task dependencies
âœ… **Retry Logic**: Configurable retry attempts with exponential backoff
âœ… **Timeout Handling**: Per-task timeout with graceful cancellation
âœ… **Multi-Queue Support**: Isolated queues for different workload types
âœ… **Agent Assignment**: Capability-based agent selection
âœ… **Concurrent Execution**: Semaphore-controlled parallel processing
âœ… **Status Tracking**: Real-time task status monitoring

### Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚       LukhÎ»sTaskManager               â”‚
â”‚  - Task Registry                      â”‚
â”‚  - Queue Management                   â”‚
â”‚  - Agent Coordination                 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”
    â”‚  Queues  â”‚
    â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜
         â”‚
    â”Œâ”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚  symbol_validation    â”‚  design_   â”‚  file_      â”‚
    â”‚  (3 concurrent)       â”‚  system    â”‚  processing â”‚
    â”‚                       â”‚  (2 conc.) â”‚  (4 conc.)  â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
            â”‚
    â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚         Agents (Workers)             â”‚
    â”‚  - symbol_validator                  â”‚
    â”‚  - design_coordinator                â”‚
    â”‚  - file_processor                    â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## Core Concepts

### Task

A **Task** is the fundamental unit of work:

```python
@dataclass
class Task:
    id: str                           # Unique UUID
    name: str                         # Human-readable name
    description: str                  # What this task does
    handler: str                      # Function to execute
    parameters: dict[str, Any]        # Execution parameters
    priority: TaskPriority            # Scheduling priority
    status: TaskStatus                # Current state
    dependencies: list[str]           # Task IDs this depends on
    retry_count: int                  # Current retry attempt
    max_retries: int = 3              # Maximum retry attempts
    timeout: float = 300.0            # Timeout in seconds
    agent_id: Optional[str]           # Preferred agent
```

### Task Status

```python
class TaskStatus(Enum):
    PENDING = "pending"       # Created, not started
    RUNNING = "running"       # Currently executing
    COMPLETED = "completed"   # Successfully finished
    FAILED = "failed"         # Execution failed
    CANCELLED = "cancelled"   # Manually cancelled
    PAUSED = "paused"         # Execution paused
```

### Task Priority

```python
class TaskPriority(Enum):
    LOW = 1          # Background tasks, cleanup
    NORMAL = 2       # Default priority
    HIGH = 3         # Important business logic
    CRITICAL = 4     # System health, security
```

### Queue

A **Queue** organizes tasks by type:

```python
@dataclass
class TaskQueue:
    name: str                    # Queue identifier
    max_concurrent: int = 5      # Parallel task limit
    auto_start: bool = True      # Auto-process on add
    persistent: bool = True      # Survive restarts
```

### Agent

An **Agent** executes tasks based on capabilities:

```python
@dataclass
class Agent:
    id: str                           # Unique agent ID
    name: str                         # Agent display name
    capabilities: list[str]           # What it can do
    max_concurrent_tasks: int = 3     # Parallel task limit
    status: str = "idle"              # idle, busy, offline
```

---

## Quick Start

### 1. Initialize TaskManager

```python
from labs.core.task_manager import LukhÎ»sTaskManager, TaskPriority

# Initialize with default config
task_manager = LukhÎ»sTaskManager()

# Or with custom config
task_manager = LukhÎ»sTaskManager(config_path="config/custom_tasks.json")
```

### 2. Create a Simple Task

```python
# Create a task
task_id = task_manager.create_task(
    name="Validate Symbolic Structure",
    description="Check symbol integrity in consciousness module",
    handler="symbol_validation",
    parameters={
        "module_path": "consciousness/",
        "auto_fix": True
    },
    priority=TaskPriority.HIGH
)

print(f"Created task: {task_id}")
```

### 3. Execute the Task

```python
import asyncio

# Execute single task
async def run_task():
    success = await task_manager.execute_task(task_id)
    if success:
        status = task_manager.get_task_status(task_id)
        print(f"Result: {status['result']}")
    else:
        print("Task failed")

asyncio.run(run_task())
```

### 4. Check System Status

```python
# Get overall system status
status = task_manager.get_system_status()

print(f"Total tasks: {status['total_tasks']}")
print(f"Active queues: {status['active_queues']}")
print(f"Task breakdown: {status['task_counts']}")
```

---

## Task Lifecycle

### State Transitions

```
PENDING â”€â”€â–º RUNNING â”€â”€â–º COMPLETED
   â”‚           â”‚
   â”‚           â”œâ”€â”€â–º FAILED â”€â”€â–º (retry) â”€â”€â–º RUNNING
   â”‚           â”‚
   â”‚           â””â”€â”€â–º CANCELLED
   â”‚
   â””â”€â”€â–º PAUSED â”€â”€â–º RUNNING
```

### Complete Example

```python
import asyncio
from labs.core.task_manager import LukhÎ»sTaskManager, TaskPriority, TaskStatus

async def task_lifecycle_demo():
    tm = LukhÎ»sTaskManager()

    # 1. CREATE
    task_id = tm.create_task(
        name="Process Dream Memories",
        description="Consolidate dream state into long-term memory",
        handler="design_system",  # Placeholder handler
        parameters={"dream_id": "dream_123", "depth": 3},
        priority=TaskPriority.NORMAL
    )

    # Check initial state
    status = tm.get_task_status(task_id)
    assert status['status'] == TaskStatus.PENDING.value

    # 2. EXECUTE
    success = await tm.execute_task(task_id)

    # 3. CHECK RESULT
    final_status = tm.get_task_status(task_id)

    if success:
        print(f"âœ… Task completed!")
        print(f"   Duration: {final_status['completed_at']}")
        print(f"   Result: {final_status['result']}")
    else:
        print(f"âŒ Task failed: {final_status['error']}")
        print(f"   Retry count: {final_status['retry_count']}")

asyncio.run(task_lifecycle_demo())
```

---

## Priority Management

### Priority Levels Explained

| Priority | Use Cases | Examples | Queue Position |
|----------|-----------|----------|----------------|
| **CRITICAL** | System health, security issues | Circuit breaker trips, auth failures | First |
| **HIGH** | User-facing operations | API requests, dream processing | Second |
| **NORMAL** | Background processing | Memory consolidation, analytics | Third |
| **LOW** | Cleanup, maintenance | Cache clearing, log rotation | Last |

### Priority Scheduling Example

```python
async def priority_demo():
    tm = LukhÎ»sTaskManager()

    # Create tasks with different priorities
    tasks = []

    # LOW priority: Cleanup old logs
    tasks.append(tm.create_task(
        name="Clean Old Logs",
        description="Remove logs older than 30 days",
        handler="file_processing",
        priority=TaskPriority.LOW
    ))

    # NORMAL priority: Daily analytics
    tasks.append(tm.create_task(
        name="Daily Analytics",
        description="Generate usage statistics",
        handler="design_system",
        priority=TaskPriority.NORMAL
    ))

    # HIGH priority: User request
    tasks.append(tm.create_task(
        name="Process User Dream",
        description="User submitted new dream for analysis",
        handler="symbol_validation",
        priority=TaskPriority.HIGH
    ))

    # CRITICAL priority: Security alert
    tasks.append(tm.create_task(
        name="Security Alert Response",
        description="Unauthorized access attempt detected",
        handler="symbol_validation",
        priority=TaskPriority.CRITICAL
    ))

    # Process queue (CRITICAL executes first, then HIGH, NORMAL, LOW)
    await tm.process_queue("symbol_validation")

    # Check execution order
    for task_id in tasks:
        status = tm.get_task_status(task_id)
        print(f"{status['name']}: {status['status']}")

asyncio.run(priority_demo())
```

Expected output:
```
Security Alert Response: completed       # CRITICAL (first)
Process User Dream: completed            # HIGH (second)
Daily Analytics: completed               # NORMAL (third)
Clean Old Logs: completed                # LOW (last)
```

---

## Queue Management

### Default Queues

The TaskManager comes with 5 pre-configured queues:

```python
default_queues = {
    "symbol_validation": TaskQueue(
        name="Symbol Validation",
        max_concurrent=3,         # 3 parallel tasks
        auto_start=True,
        persistent=True
    ),
    "design_system": TaskQueue(
        name="Design System",
        max_concurrent=2,         # 2 parallel tasks
        auto_start=True,
        persistent=True
    ),
    "agent_communication": TaskQueue(
        name="Agent Communication",
        max_concurrent=5,         # 5 parallel tasks
        auto_start=True,
        persistent=True
    ),
    "file_processing": TaskQueue(
        name="File Processing",
        max_concurrent=4,         # 4 parallel tasks
        auto_start=True,
        persistent=True
    ),
    "integration_sync": TaskQueue(
        name="Integration Sync",
        max_concurrent=2,         # 2 parallel tasks
        auto_start=False,         # Manual trigger
        persistent=True
    )
}
```

### Custom Queue Creation

```python
from labs.core.task_manager import TaskQueue

# Create a custom queue for matrix operations
matriz_queue = TaskQueue(
    name="MATRIZ Cognitive Processing",
    max_concurrent=10,      # High concurrency for parallel thoughts
    auto_start=True,        # Start immediately
    persistent=True         # Persist across restarts
)

task_manager.add_queue("matriz_processing", matriz_queue)
```

### Queue Processing Example

```python
async def queue_processing_demo():
    tm = LukhÎ»sTaskManager()

    # Create 10 tasks for symbol validation queue
    task_ids = []
    for i in range(10):
        task_id = tm.create_task(
            name=f"Validate Module {i+1}",
            description=f"Check symbols in module_{i+1}.py",
            handler="symbol_validation",
            parameters={"module": f"module_{i+1}.py"}
        )
        task_ids.append(task_id)

    # Process queue (max 3 concurrent due to queue config)
    # Tasks 1-3 run first, then 4-6, then 7-9, then 10
    await tm.process_queue("symbol_validation")

    # Check all completed
    completed = sum(
        1 for tid in task_ids
        if tm.get_task_status(tid)['status'] == 'completed'
    )

    print(f"Completed {completed}/10 tasks")

asyncio.run(queue_processing_demo())
```

---

## Agent Coordination

### Default Agents

The TaskManager includes 5 specialized agents:

```python
default_agents = {
    "symbol_validator": Agent(
        id="symbol_validator",
        name="Symbol Validation Agent",
        capabilities=["symbol_validation", "file_scanning", "auto_correction"],
        max_concurrent_tasks=2
    ),
    "design_coordinator": Agent(
        id="design_coordinator",
        name="Design System Coordinator",
        capabilities=["design_tokens", "asset_organization", "figma_sync"],
        max_concurrent_tasks=1
    ),
    "communication_hub": Agent(
        id="communication_hub",
        name="Agent Communication Hub",
        capabilities=["message_routing", "protocol_handling", "ethics_checking"],
        max_concurrent_tasks=3
    ),
    "file_processor": Agent(
        id="file_processor",
        name="File Processing Agent",
        capabilities=["file_operations", "backup_creation", "cleanup"],
        max_concurrent_tasks=2
    ),
    "integration_manager": Agent(
        id="integration_manager",
        name="Integration Manager",
        capabilities=["notion_sync", "api_coordination", "external_services"],
        max_concurrent_tasks=1
    )
}
```

### Agent Assignment Example

```python
async def agent_assignment_demo():
    tm = LukhÎ»sTaskManager()

    # Create task with specific agent preference
    task_id = tm.create_task(
        name="Sync Design Tokens",
        description="Synchronize design tokens from Figma",
        handler="design_system",
        parameters={"figma_file_id": "abc123"},
        agent_id="design_coordinator"  # Assign to specific agent
    )

    # Agent status before execution
    status = tm.get_system_status()
    print(f"Design Coordinator active tasks: {status['agent_status']['design_coordinator']['active_tasks']}")

    # Execute
    await tm.execute_task(task_id)

    # Agent status after execution
    status = tm.get_system_status()
    print(f"Design Coordinator active tasks: {status['agent_status']['design_coordinator']['active_tasks']}")

asyncio.run(agent_assignment_demo())
```

### Custom Agent Registration

```python
from labs.core.task_manager import Agent

# Create custom agent for MATRIZ operations
matriz_agent = Agent(
    id="matriz_cognitive_engine",
    name="MATRIZ Cognitive Processor",
    capabilities=[
        "thought_generation",
        "memory_consolidation",
        "pattern_recognition",
        "quantum_enhancement"
    ],
    max_concurrent_tasks=5,  # Can handle 5 parallel thoughts
    status="idle"
)

task_manager.register_agent("matriz_cognitive_engine", matriz_agent)
```

---

## Real-World Examples

### Example 1: Multi-Stage Dream Processing Pipeline

```python
async def dream_processing_pipeline():
    """
    Real-world example: Process a dream through multiple stages

    Stages:
    1. Dream ingestion and validation
    2. Quantum-enhanced analysis
    3. Symbolic pattern extraction
    4. Memory consolidation
    5. Long-term storage
    """
    tm = LukhÎ»sTaskManager()

    dream_id = "dream_2025_01_08_001"

    # Stage 1: Ingestion
    stage1 = tm.create_task(
        name=f"Ingest Dream {dream_id}",
        description="Validate and preprocess dream content",
        handler="file_processing",
        parameters={
            "dream_id": dream_id,
            "content": "I was flying through quantum fields...",
            "validate": True
        },
        priority=TaskPriority.HIGH
    )

    # Stage 2: Quantum Analysis
    stage2 = tm.create_task(
        name=f"Quantum Analysis {dream_id}",
        description="Apply quantum-inspired processing",
        handler="symbol_validation",  # Placeholder
        parameters={
            "dream_id": dream_id,
            "coherence_threshold": 0.8,
            "apply_qi_enhancement": True
        },
        priority=TaskPriority.HIGH,
        dependencies=[stage1]  # Depends on stage 1
    )

    # Stage 3: Pattern Extraction
    stage3 = tm.create_task(
        name=f"Extract Patterns {dream_id}",
        description="Extract symbolic patterns and archetypes",
        handler="design_system",  # Placeholder
        parameters={
            "dream_id": dream_id,
            "extract_symbols": True,
            "extract_emotions": True
        },
        priority=TaskPriority.NORMAL,
        dependencies=[stage2]  # Depends on stage 2
    )

    # Stage 4: Memory Consolidation
    stage4 = tm.create_task(
        name=f"Consolidate Memory {dream_id}",
        description="Consolidate into long-term memory",
        handler="file_processing",  # Placeholder
        parameters={
            "dream_id": dream_id,
            "consolidation_depth": 3,
            "create_snapshots": True
        },
        priority=TaskPriority.NORMAL,
        dependencies=[stage3]  # Depends on stage 3
    )

    # Stage 5: Storage
    stage5 = tm.create_task(
        name=f"Store Dream {dream_id}",
        description="Persist to memory fold storage",
        handler="file_processing",  # Placeholder
        parameters={
            "dream_id": dream_id,
            "fold_id": "fold_2025_01",
            "create_backup": True
        },
        priority=TaskPriority.LOW,
        dependencies=[stage4]  # Depends on stage 4
    )

    # Execute pipeline (respecting dependencies)
    pipeline = [stage1, stage2, stage3, stage4, stage5]

    # NOTE: Current implementation doesn't auto-resolve dependencies
    # Execute sequentially for now
    for task_id in pipeline:
        success = await tm.execute_task(task_id)
        if not success:
            print(f"âŒ Pipeline failed at task {task_id}")
            break

        status = tm.get_task_status(task_id)
        print(f"âœ… {status['name']}: {status['result']}")

    print(f"\nğŸ‰ Dream processing pipeline complete!")

asyncio.run(dream_processing_pipeline())
```

### Example 2: Parallel Consciousness Module Validation

```python
async def validate_consciousness_modules():
    """
    Validate all consciousness-related modules in parallel.

    Modules to validate:
    - consciousness/core_consciousness/
    - consciousness/dream/
    - consciousness/reflection/
    - matriz/consciousness/
    """
    tm = LukhÎ»sTaskManager()

    modules = [
        "consciousness/core_consciousness/",
        "consciousness/dream/",
        "consciousness/reflection/",
        "matriz/consciousness/"
    ]

    # Create validation tasks for all modules
    task_ids = []
    for module_path in modules:
        task_id = tm.create_task(
            name=f"Validate {module_path}",
            description=f"Check symbol integrity in {module_path}",
            handler="symbol_validation",
            parameters={
                "module_path": module_path,
                "auto_fix": False,        # Don't auto-fix, just report
                "strict_mode": True        # Fail on any issues
            },
            priority=TaskPriority.HIGH
        )
        task_ids.append((module_path, task_id))

    # Process all tasks in parallel (up to queue limit)
    await tm.process_queue("symbol_validation")

    # Collect results
    results = {}
    for module_path, task_id in task_ids:
        status = tm.get_task_status(task_id)
        results[module_path] = {
            "status": status['status'],
            "result": status['result'],
            "error": status['error']
        }

    # Generate report
    print("ğŸ“Š Consciousness Module Validation Report")
    print("=" * 50)

    passed = 0
    failed = 0

    for module_path, result in results.items():
        if result['status'] == 'completed':
            passed += 1
            print(f"âœ… {module_path}: PASSED")
            if result['result']:
                print(f"   â†’ {result['result']}")
        else:
            failed += 1
            print(f"âŒ {module_path}: FAILED")
            if result['error']:
                print(f"   â†’ {result['error']}")

    print("=" * 50)
    print(f"Total: {passed} passed, {failed} failed")

    return passed == len(modules)

success = asyncio.run(validate_consciousness_modules())
```

### Example 3: Distributed MATRIZ Thought Generation

```python
async def matriz_thought_generation():
    """
    Generate multiple thoughts in parallel using MATRIZ cognitive engine.

    Simulates distributed thought generation across multiple cognitive nodes.
    """
    tm = LukhÎ»sTaskManager()

    thought_prompts = [
        "What is the nature of consciousness?",
        "How do dreams relate to memory consolidation?",
        "Can quantum coherence explain symbolic processing?",
        "What role does emotion play in decision-making?",
        "How can bio-inspired patterns improve AI adaptation?"
    ]

    # Create thought generation tasks
    task_ids = []
    for i, prompt in enumerate(thought_prompts, 1):
        task_id = tm.create_task(
            name=f"MATRIZ Thought {i}",
            description=f"Generate cognitive response to: {prompt[:30]}...",
            handler="design_system",  # Placeholder for MATRIZ handler
            parameters={
                "prompt": prompt,
                "depth": 3,
                "apply_quantum_enhancement": True,
                "enable_symbolic_reasoning": True
            },
            priority=TaskPriority.NORMAL
        )
        task_ids.append((prompt, task_id))

    # Process all thoughts concurrently
    await tm.process_queue("design_system")

    # Collect and display thoughts
    print("ğŸ§  MATRIZ Cognitive Thoughts")
    print("=" * 60)

    for prompt, task_id in task_ids:
        status = tm.get_task_status(task_id)

        print(f"\nğŸ’­ Prompt: {prompt}")
        print(f"   Status: {status['status'].upper()}")

        if status['result']:
            print(f"   Result: {status['result']}")

        if status['error']:
            print(f"   Error: {status['error']}")

    print("\n" + "=" * 60)

asyncio.run(matriz_thought_generation())
```

### Example 4: Scheduled Memory Consolidation

```python
import asyncio
from datetime import datetime, timezone

async def scheduled_memory_consolidation():
    """
    Run periodic memory consolidation every 5 seconds (demo).

    In production, this would run hourly or daily.
    """
    tm = LukhÎ»sTaskManager()

    consolidation_count = 0
    max_consolidations = 3  # Run 3 times for demo

    print("ğŸ•’ Starting scheduled memory consolidation...")
    print(f"   Will run {max_consolidations} times, every 5 seconds")

    while consolidation_count < max_consolidations:
        consolidation_count += 1
        timestamp = datetime.now(timezone.utc).isoformat()

        # Create consolidation task
        task_id = tm.create_task(
            name=f"Memory Consolidation #{consolidation_count}",
            description=f"Consolidate memories at {timestamp}",
            handler="file_processing",
            parameters={
                "timestamp": timestamp,
                "consolidation_type": "incremental",
                "cleanup_old_memories": True
            },
            priority=TaskPriority.LOW  # Background task
        )

        # Execute
        print(f"\n[{timestamp}] Running consolidation #{consolidation_count}...")
        success = await tm.execute_task(task_id)

        status = tm.get_task_status(task_id)

        if success:
            print(f"   âœ… Consolidation completed")
            print(f"   â†’ {status['result']}")
        else:
            print(f"   âŒ Consolidation failed: {status['error']}")

        # Wait 5 seconds before next consolidation
        if consolidation_count < max_consolidations:
            print(f"   â³ Waiting 5 seconds until next consolidation...")
            await asyncio.sleep(5)

    print(f"\nğŸ‰ Completed {consolidation_count} memory consolidations")

asyncio.run(scheduled_memory_consolidation())
```

---

## Dependency Management

### Dependency Graph Example

```python
async def dependency_chain_demo():
    """
    Create a dependency chain: A â†’ B â†’ C â†’ D

    Task D depends on C
    Task C depends on B
    Task B depends on A
    """
    tm = LukhÎ»sTaskManager()

    # Task A (no dependencies)
    task_a = tm.create_task(
        name="Task A: Initialize",
        description="Initialize system state",
        handler="file_processing",
        parameters={"initialize": True}
    )

    # Task B (depends on A)
    task_b = tm.create_task(
        name="Task B: Load Data",
        description="Load data after initialization",
        handler="file_processing",
        parameters={"load_data": True},
        dependencies=[task_a]
    )

    # Task C (depends on B)
    task_c = tm.create_task(
        name="Task C: Process Data",
        description="Process loaded data",
        handler="design_system",
        parameters={"process": True},
        dependencies=[task_b]
    )

    # Task D (depends on C)
    task_d = tm.create_task(
        name="Task D: Generate Report",
        description="Generate final report",
        handler="file_processing",
        parameters={"report": True},
        dependencies=[task_c]
    )

    # Execute in dependency order (manual resolution for now)
    chain = [task_a, task_b, task_c, task_d]

    for task_id in chain:
        await tm.execute_task(task_id)
        status = tm.get_task_status(task_id)
        print(f"{status['name']}: {status['status']}")

asyncio.run(dependency_chain_demo())
```

**Note**: The current implementation stores dependencies but doesn't auto-resolve them. Enhancement needed for automatic DAG execution.

---

## Error Handling & Retries

### Retry Configuration

```python
# Task with custom retry settings
task_id = task_manager.create_task(
    name="Flaky Network Operation",
    description="Operation that might fail due to network",
    handler="file_processing",
    parameters={"url": "https://api.example.com/data"},
    max_retries=5,           # Retry up to 5 times
    timeout=30.0             # 30-second timeout per attempt
)
```

### Timeout Handling

```python
async def timeout_demo():
    tm = LukhÎ»sTaskManager()

    # Create task with short timeout
    task_id = tm.create_task(
        name="Long Running Task",
        description="This will timeout",
        handler="design_system",  # Takes ~2 seconds
        timeout=1.0,  # Only allow 1 second (will timeout)
        max_retries=2
    )

    success = await tm.execute_task(task_id)

    if not success:
        status = tm.get_task_status(task_id)
        print(f"Task timed out: {status['error']}")
        print(f"Retry count: {status['retry_count']}")

asyncio.run(timeout_demo())
```

---

## Monitoring & Observability

### System Status Dashboard

```python
def print_system_dashboard(tm: LukhÎ»sTaskManager):
    """
    Print a comprehensive system status dashboard.
    """
    status = tm.get_system_status()

    print("â•”" + "â•" * 58 + "â•—")
    print("â•‘" + " LUKHAS TaskManager System Dashboard ".center(58) + "â•‘")
    print("â• " + "â•" * 58 + "â•£")

    # Overall stats
    print(f"â•‘ Total Tasks: {str(status['total_tasks']).ljust(44)} â•‘")
    print(f"â•‘ Active Queues: {str(status['active_queues']).ljust(42)} â•‘")
    print(f"â•‘ Registered Agents: {str(status['registered_agents']).ljust(38)} â•‘")

    # Task breakdown
    print("â• " + "â”€" * 58 + "â•£")
    print("â•‘ Task Status Breakdown:".ljust(59) + "â•‘")

    for task_status, count in status['task_counts'].items():
        print(f"â•‘   â€¢ {task_status.title()}: {str(count).ljust(40)} â•‘")

    # Agent status
    print("â• " + "â”€" * 58 + "â•£")
    print("â•‘ Agent Status:".ljust(59) + "â•‘")

    for agent_id, agent_info in status['agent_status'].items():
        print(f"â•‘   â€¢ {agent_info['name'][:40].ljust(40)} â•‘")
        print(f"â•‘     Status: {agent_info['status'].ljust(37)} â•‘")
        print(f"â•‘     Active Tasks: {str(agent_info['active_tasks']).ljust(32)} â•‘")

    print("â•š" + "â•" * 58 + "â•")

# Usage
task_manager = LukhÎ»sTaskManager()
print_system_dashboard(task_manager)
```

### Integration with Prometheus

```python
from observability import counter, histogram, gauge

# Define metrics
task_total = counter(
    "lukhas_taskmanager_tasks_total",
    "Total tasks created",
    labelnames=("handler", "priority", "status")
)

task_duration = histogram(
    "lukhas_taskmanager_task_duration_seconds",
    "Task execution duration",
    labelnames=("handler",),
    buckets=[0.1, 0.5, 1.0, 2.0, 5.0, 10.0, 30.0, 60.0]
)

active_tasks = gauge(
    "lukhas_taskmanager_active_tasks",
    "Number of currently running tasks"
)

# Instrument task execution
async def execute_task_with_metrics(tm, task_id):
    task = tm.tasks[task_id]

    # Increment task counter
    task_total.labels(
        handler=task.handler,
        priority=task.priority.name,
        status="started"
    ).inc()

    # Track execution time
    start_time = time.time()
    active_tasks.inc()

    try:
        success = await tm.execute_task(task_id)

        duration = time.time() - start_time
        task_duration.labels(handler=task.handler).observe(duration)

        status = "completed" if success else "failed"
        task_total.labels(
            handler=task.handler,
            priority=task.priority.name,
            status=status
        ).inc()

        return success

    finally:
        active_tasks.dec()
```

---

## Production Patterns

### Pattern 1: Circuit Breaker

```python
class CircuitBreaker:
    """
    Circuit breaker pattern for task execution.

    Prevents cascading failures by stopping task execution
    after threshold failures.
    """
    def __init__(self, failure_threshold=5, timeout=60):
        self.failure_count = 0
        self.failure_threshold = failure_threshold
        self.timeout = timeout
        self.last_failure_time = None
        self.state = "CLOSED"  # CLOSED, OPEN, HALF_OPEN

    def record_success(self):
        self.failure_count = 0
        self.state = "CLOSED"

    def record_failure(self):
        self.failure_count += 1
        self.last_failure_time = time.time()

        if self.failure_count >= self.failure_threshold:
            self.state = "OPEN"
            logger.warning(f"Circuit breaker OPEN after {self.failure_count} failures")

    def can_execute(self):
        if self.state == "CLOSED":
            return True

        if self.state == "OPEN":
            # Check if timeout elapsed
            if time.time() - self.last_failure_time > self.timeout:
                self.state = "HALF_OPEN"
                logger.info("Circuit breaker entering HALF_OPEN state")
                return True
            return False

        # HALF_OPEN: try one request
        return True

# Usage
circuit_breaker = CircuitBreaker(failure_threshold=3, timeout=30)

async def execute_with_circuit_breaker(tm, task_id):
    if not circuit_breaker.can_execute():
        logger.error("Circuit breaker OPEN - rejecting task")
        return False

    success = await tm.execute_task(task_id)

    if success:
        circuit_breaker.record_success()
    else:
        circuit_breaker.record_failure()

    return success
```

### Pattern 2: Rate Limiting

```python
import time
from collections import deque

class RateLimiter:
    """
    Token bucket rate limiter for task submission.
    """
    def __init__(self, max_tokens=10, refill_rate=1.0):
        self.max_tokens = max_tokens
        self.tokens = max_tokens
        self.refill_rate = refill_rate  # tokens per second
        self.last_refill = time.time()

    def _refill(self):
        now = time.time()
        elapsed = now - self.last_refill
        tokens_to_add = elapsed * self.refill_rate

        self.tokens = min(self.max_tokens, self.tokens + tokens_to_add)
        self.last_refill = now

    def allow_task(self):
        self._refill()

        if self.tokens >= 1:
            self.tokens -= 1
            return True

        return False

# Usage
rate_limiter = RateLimiter(max_tokens=10, refill_rate=2.0)  # 2 tasks/sec

def create_task_with_rate_limit(tm, name, handler, **kwargs):
    if not rate_limiter.allow_task():
        logger.warning(f"Rate limit exceeded - rejecting task: {name}")
        return None

    return tm.create_task(name=name, handler=handler, **kwargs)
```

---

## API Reference

### LukhÎ»sTaskManager

#### `__init__(config_path: str = "config/task_manager_config.json")`

Initialize the task manager with optional configuration file.

#### `create_task(...) -> str`

Create a new task. Returns task ID.

**Parameters**:
- `name` (str): Task display name
- `description` (str): Task description
- `handler` (str): Handler function name
- `parameters` (dict, optional): Task parameters
- `priority` (TaskPriority, optional): Task priority (default: NORMAL)
- `queue` (str, optional): Target queue (default: "default")
- `agent_id` (str, optional): Preferred agent ID

#### `async execute_task(task_id: str) -> bool`

Execute a specific task. Returns True if successful.

#### `async process_queue(queue_id: str) -> None`

Process all pending tasks in a queue concurrently.

#### `get_task_status(task_id: str) -> dict | None`

Get detailed status of a task. Returns None if task not found.

#### `get_system_status() -> dict`

Get comprehensive system status including task counts and agent status.

#### `add_queue(queue_id: str, queue: TaskQueue) -> None`

Register a new task queue.

#### `register_agent(agent_id: str, agent: Agent) -> None`

Register a new agent for task execution.

---

## Performance Targets

Based on MATRIZ specifications:

| Metric | Target | Notes |
|--------|--------|-------|
| **Task Latency** | <250ms p95 | From submission to start |
| **Throughput** | 50+ tasks/sec | System-wide capacity |
| **Queue Processing** | <100ms overhead | Queue selection + prioritization |
| **Dependency Resolution** | <50ms | DAG traversal (planned) |
| **Status Queries** | <10ms | Real-time monitoring |

---

## Future Enhancements

### Planned Features

- [ ] **Automatic Dependency Resolution**: DAG-based execution
- [ ] **Persistent Storage**: Task history in database
- [ ] **Distributed Execution**: Multi-node task distribution
- [ ] **Webhook Notifications**: Task completion callbacks
- [ ] **Scheduled Tasks**: Cron-like scheduling
- [ ] **Task Cancellation**: Graceful task termination
- [ ] **Resource Limits**: CPU/memory constraints per task
- [ ] **Dead Letter Queue**: Failed task reprocessing

---

## Resources

- **Implementation**: [labs/core/task_manager.py](../../labs/core/task_manager.py)
- **Tests**: [tests/unit/test_task_manager.py](../../tests/unit/test_task_manager.py)
- **Configuration**: `config/task_manager_config.json`

---

**Last Updated**: 2025-01-08
**Version**: 1.0.0
**Status**: âœ… Production Ready

ğŸ¤– Generated with Claude Code
