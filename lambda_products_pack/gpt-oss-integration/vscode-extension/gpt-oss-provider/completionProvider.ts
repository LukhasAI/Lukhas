/**
 * GPT-OSS Enhanced Completion Provider for VSCode
 * Integrates with OpenAI's GPT-OSS-20b for intelligent code completion
 */

import * as vscode from 'vscode';
import { ModelLoader } from '../models/modelLoader';
import { PromptBuilder } from '../../shared/promptBuilder';

export class GPTOSSCompletionProvider implements vscode.InlineCompletionItemProvider {
    private modelLoader: ModelLoader;
    private contextWindow: number = 8192; // GPT-OSS context window
    private isModelLoaded: boolean = false;
    private shadowMode: boolean = true;
    
    constructor(private context: vscode.ExtensionContext) {
        this.modelLoader = new ModelLoader();
        this.initializeModel();
    }
    
    private async initializeModel() {
        try {
            // Load GPT-OSS-20b model (requires 16GB RAM)
            await this.modelLoader.loadModel('gpt-oss-20b');
            this.isModelLoaded = true;
            vscode.window.showInformationMessage('âœ… GPT-OSS-20b loaded successfully');
        } catch (error) {
            console.error('Failed to load GPT-OSS model:', error);
            vscode.window.showErrorMessage('Failed to load GPT-OSS model. Using fallback.');
        }
    }
    
    async provideInlineCompletionItems(
        document: vscode.TextDocument,
        position: vscode.Position,
        context: vscode.InlineCompletionContext,
        token: vscode.CancellationToken
    ): Promise<vscode.InlineCompletionList> {
        
        if (!this.isModelLoaded) {
            return { items: [] };
        }
        
        // Build context from surrounding code
        const promptContext = await this.buildContext(document, position);
        
        // Create LUKHAS-aware prompt
        const prompt = PromptBuilder.buildCompletionPrompt(promptContext, {
            style: 'lukhas_patterns',
            includeAGIConcepts: true,
            preserveSymbolicNotation: true
        });
        
        try {
            // Generate completion with GPT-OSS
            const completion = await this.modelLoader.generate({
                prompt: prompt,
                max_tokens: 500,
                temperature: 0.7,
                stop: ['\n\n', '```'],
                stream: false
            });
            
            // Parse and format completion
            const formattedCompletion = this.formatCompletion(completion, document, position);
            
            // Shadow mode: compare with existing provider
            if (this.shadowMode) {
                await this.logShadowComparison(promptContext, completion);
            }
            
            return {
                items: [{
                    insertText: formattedCompletion,
                    range: new vscode.Range(position, position),
                    command: {
                        command: 'gpt-oss.trackCompletion',
                        title: 'Track GPT-OSS Completion',
                        arguments: [completion]
                    }
                }]
            };
            
        } catch (error) {
            console.error('GPT-OSS completion error:', error);
            return { items: [] };
        }
    }
    
    private async buildContext(
        document: vscode.TextDocument,
        position: vscode.Position
    ): Promise<CompletionContext> {
        const linePrefix = document.lineAt(position.line).text.substr(0, position.character);
        const lineSuffix = document.lineAt(position.line).text.substr(position.character);
        
        // Get surrounding code context
        const startLine = Math.max(0, position.line - 50);
        const endLine = Math.min(document.lineCount - 1, position.line + 20);
        
        const beforeText = document.getText(new vscode.Range(startLine, 0, position.line, position.character));
        const afterText = document.getText(new vscode.Range(position.line, position.character, endLine, 0));
        
        // Detect LUKHAS-specific patterns
        const isLambdaProduct = this.detectLambdaProduct(document.fileName);
        const isAGIModule = this.detectAGIModule(document.fileName);
        const isBrainArchitecture = document.getText().includes('MultiBrainSymphony') || 
                                    document.getText().includes('SpecializedBrainCore');
        
        // Get related files for better context
        const relatedFiles = await this.getRelatedFiles(document.uri);
        
        return {
            linePrefix,
            lineSuffix,
            beforeText,
            afterText,
            language: document.languageId,
            fileName: document.fileName,
            isLambdaProduct,
            isAGIModule,
            isBrainArchitecture,
            relatedContext: relatedFiles,
            symbols: await this.extractSymbols(document)
        };
    }
    
    private detectLambdaProduct(fileName: string): boolean {
        return fileName.includes('lambda-products') || 
               fileName.includes('QRG') || 
               fileName.includes('NIÎ›S') || 
               fileName.includes('Î›BAS') ||
               fileName.includes('DÎ›ST');
    }
    
    private detectAGIModule(fileName: string): boolean {
        return fileName.includes('brain') || 
               fileName.includes('consciousness') || 
               fileName.includes('guardian') ||
               fileName.includes('core/') ||
               fileName.includes('agi');
    }
    
    private async getRelatedFiles(uri: vscode.Uri): Promise<string[]> {
        // Get imports and related files for context
        const workspace = vscode.workspace.getWorkspaceFolder(uri);
        if (!workspace) return [];
        
        // Find related files (imports, similar names, etc.)
        const pattern = new vscode.RelativePattern(workspace, '**/*.{py,ts,js}');
        const files = await vscode.workspace.findFiles(pattern, null, 10);
        
        const relatedContent: string[] = [];
        for (const file of files) {
            if (file.toString() !== uri.toString()) {
                const doc = await vscode.workspace.openTextDocument(file);
                relatedContent.push(doc.getText().substring(0, 500)); // First 500 chars
            }
        }
        
        return relatedContent;
    }
    
    private async extractSymbols(document: vscode.TextDocument): Promise<vscode.DocumentSymbol[]> {
        const symbols = await vscode.commands.executeCommand<vscode.DocumentSymbol[]>(
            'vscode.executeDocumentSymbolProvider',
            document.uri
        );
        return symbols || [];
    }
    
    private formatCompletion(completion: string, document: vscode.TextDocument, position: vscode.Position): string {
        // Format completion based on document type and position
        const indentation = this.detectIndentation(document, position);
        
        // Apply LUKHAS-specific formatting
        let formatted = completion;
        
        // Add proper indentation
        formatted = formatted.split('\n').map(line => indentation + line).join('\n');
        
        // Add symbolic notation if in Lambda products
        if (this.detectLambdaProduct(document.fileName)) {
            formatted = this.addSymbolicNotation(formatted);
        }
        
        // Add logging for AGI modules
        if (this.detectAGIModule(document.fileName)) {
            formatted = this.addAGILogging(formatted);
        }
        
        return formatted;
    }
    
    private detectIndentation(document: vscode.TextDocument, position: vscode.Position): string {
        const line = document.lineAt(position.line).text;
        const match = line.match(/^(\s*)/);
        return match ? match[1] : '';
    }
    
    private addSymbolicNotation(code: string): string {
        // Add Lambda (Î›) symbols where appropriate
        return code
            .replace(/Lambda/g, 'Î›')
            .replace(/lambda_/g, 'Î›_')
            .replace(/LAMBDA/g, 'Î›');
    }
    
    private addAGILogging(code: string): string {
        // Add appropriate logging for AGI modules
        const lines = code.split('\n');
        const enhancedLines: string[] = [];
        
        for (const line of lines) {
            enhancedLines.push(line);
            
            // Add logging for function definitions
            if (line.includes('def ') && !line.includes('_')) {
                const funcName = line.match(/def\s+(\w+)/)?.[1];
                if (funcName) {
                    const indent = line.match(/^(\s*)/)?.[1] || '';
                    enhancedLines.push(`${indent}    logger.info(f"ðŸ§  ${funcName} initiated")`);
                }
            }
        }
        
        return enhancedLines.join('\n');
    }
    
    private async logShadowComparison(context: CompletionContext, completion: string) {
        // Log shadow mode comparison for analysis
        const shadowLog = {
            timestamp: new Date().toISOString(),
            context: {
                file: context.fileName,
                language: context.language,
                isLambdaProduct: context.isLambdaProduct,
                isAGIModule: context.isAGIModule
            },
            completion: completion.substring(0, 200), // First 200 chars
            metrics: {
                length: completion.length,
                lines: completion.split('\n').length
            }
        };
        
        // Store in experimental results
        const shadowResults = this.context.globalState.get<any[]>('gpt-oss-shadow-results', []);
        shadowResults.push(shadowLog);
        await this.context.globalState.update('gpt-oss-shadow-results', shadowResults.slice(-100)); // Keep last 100
    }
}

interface CompletionContext {
    linePrefix: string;
    lineSuffix: string;
    beforeText: string;
    afterText: string;
    language: string;
    fileName: string;
    isLambdaProduct: boolean;
    isAGIModule: boolean;
    isBrainArchitecture: boolean;
    relatedContext: string[];
    symbols: vscode.DocumentSymbol[];
}

// Model Loader implementation
export class ModelLoader {
    private model: any;
    private modelPath: string = '';
    
    async loadModel(modelName: string): Promise<void> {
        // In production, this would load the actual GPT-OSS model
        // For now, we'll create a mock that demonstrates the interface
        
        this.modelPath = `/models/${modelName}`;
        
        // Check if Ollama is available for local model
        try {
            const { exec } = require('child_process');
            const util = require('util');
            const execPromise = util.promisify(exec);
            
            // Check if model is available
            const { stdout } = await execPromise(`ollama list | grep ${modelName}`);
            if (!stdout.includes(modelName)) {
                // Pull the model if not available
                await execPromise(`ollama pull ${modelName}`);
            }
            
            this.model = modelName;
            console.log(`âœ… Loaded ${modelName} via Ollama`);
            
        } catch (error) {
            console.error('Ollama not available, using mock model:', error);
            this.model = 'mock';
        }
    }
    
    async generate(options: GenerateOptions): Promise<string> {
        if (this.model === 'mock') {
            // Return mock completion for testing
            return this.mockGenerate(options);
        }
        
        // Use Ollama for actual generation
        const { exec } = require('child_process');
        const util = require('util');
        const execPromise = util.promisify(exec);
        
        try {
            const prompt = options.prompt.replace(/"/g, '\\"').replace(/\n/g, '\\n');
            const command = `echo "${prompt}" | ollama run ${this.model} --verbose=false`;
            const { stdout } = await execPromise(command, { maxBuffer: 1024 * 1024 * 10 });
            
            return stdout.trim();
        } catch (error) {
            console.error('Generation error:', error);
            return this.mockGenerate(options);
        }
    }
    
    private mockGenerate(options: GenerateOptions): string {
        // Mock generation for testing
        const prompts = {
            python: `def process_data(self, data: Dict[str, Any]) -> Dict[str, Any]:
        """Process data with GPT-OSS enhanced logic"""
        # Validate input
        if not data:
            raise ValueError("Data cannot be empty")
        
        # Process with AI enhancement
        result = {
            'processed': True,
            'timestamp': datetime.now().isoformat(),
            'confidence': 0.95
        }
        
        logger.info(f"âœ… Processing complete: {result}")
        return result`,
            
            typescript: `async processWithGPTOSS(input: string): Promise<ProcessResult> {
        // Enhanced processing with GPT-OSS
        const context = await this.buildContext(input);
        
        const result = await this.model.generate({
            prompt: context,
            temperature: 0.7,
            maxTokens: 500
        });
        
        return {
            success: true,
            output: result,
            confidence: 0.95
        };
    }`
        };
        
        // Return appropriate mock based on context
        if (options.prompt.includes('python') || options.prompt.includes('def ')) {
            return prompts.python;
        }
        return prompts.typescript;
    }
}

interface GenerateOptions {
    prompt: string;
    max_tokens?: number;
    temperature?: number;
    stop?: string[];
    stream?: boolean;
}