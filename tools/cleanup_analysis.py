#!/usr/bin/env python3
"""
Cleanup Analysis for LUKHAS AI
Identifies truly unused code vs test/experimental/backup code
Trinity Framework: âš›ï¸ðŸ§ ðŸ›¡ï¸
"""

import contextlib
import json
from pathlib import Path


def analyze_cleanup_candidates():
    """Analyze which files can be safely cleaned up"""

    # Load the usage report
    with open("module_usage_report.json") as f:
        report = json.load(f)

    never_imported = report["never_imported"]

    # Categorize files
    categories = {
        "agent_workspaces": [],  # CLAUDE_ARMY workspaces
        "nias_theory": [],  # Theoretical/experimental NIAS code
        "test_files": [],  # Test files not in tests/ directory
        "examples": [],  # Example and demo files
        "duplicate_apis": [],  # Duplicate API implementations
        "archive_candidates": [],  # Old implementations to archive
        "bio_variants": [],  # Bio module variants
        "memory_variants": [],  # Memory module variants
        "bridge_legacy": [],  # Legacy bridge implementations
        "consciousness_old": [],  # Old consciousness implementations
        "keep_for_reference": [],  # Keep for documentation/reference
        "safe_to_delete": [],  # Truly unused and safe to delete
    }

    for file in never_imported:
        path = Path(file)

        # Categorize based on path patterns
        if "CLAUDE_ARMY/workspaces" in file:
            categories["agent_workspaces"].append(file)
        elif "NIAS_THEORY" in file:
            categories["nias_theory"].append(file)
        elif "test_" in path.name or "tests" in file:
            categories["test_files"].append(file)
        elif "example" in file.lower() or "demo" in file.lower():
            categories["examples"].append(file)
        elif file.startswith("api/") and "_api.py" in file:
            categories["duplicate_apis"].append(file)
        elif "bio/" in file and ("variant" in file or "legacy" in file or "old" in file):
            categories["bio_variants"].append(file)
        elif "memory/" in file and ("variant" in file or "legacy" in file or "old" in file):
            categories["memory_variants"].append(file)
        elif "bridge/" in file and ("legacy" in file or "api_legacy" in file):
            categories["bridge_legacy"].append(file)
        elif "consciousness/" in file and ("old" in file or "legacy" in file):
            categories["consciousness_old"].append(file)
        elif any(keep in file for keep in ["README", "INFO", "MANIFEST", "LICENSE", "__pycache__"]):
            continue  # Skip these
        elif file.endswith("__init__.py"):
            continue  # Keep __init__ files
        else:
            # Check if it's a variant or duplicate
            if (
                "_old" in file
                or "_backup" in file
                or "_copy" in file
                or "deprecated" in file
                or "obsolete" in file
            ):
                categories["archive_candidates"].append(file)
            else:
                categories["safe_to_delete"].append(file)

    return categories


def generate_cleanup_script(categories: dict[str, list[str]]):
    """Generate shell script to perform cleanup"""

    script = """#!/bin/bash
# LUKHAS AI Cleanup Script
# Generated by cleanup_analysis.py
# Trinity Framework: âš›ï¸ðŸ§ ðŸ›¡ï¸

set -e  # Exit on error

ARCHIVE_DIR="/Users/agi_dev/lukhas-archive/2025-08-13-cleanup"
mkdir -p "$ARCHIVE_DIR"

echo "ðŸ§¹ Starting LUKHAS AI cleanup..."

"""

    # Archive agent workspaces (they're isolated experiments)
    if categories["agent_workspaces"]:
        script += """
# Archive agent workspaces (isolated experiments)
echo "ðŸ“¦ Archiving agent workspaces..."
mkdir -p "$ARCHIVE_DIR/agent_workspaces"
"""
        for file in categories["agent_workspaces"][:20]:  # First 20
            script += f'mv "{file}" "$ARCHIVE_DIR/agent_workspaces/" 2>/dev/null || true\n'

    # Archive NIAS theory files
    if categories["nias_theory"]:
        script += """
# Archive NIAS theory files (experimental)
echo "ðŸ”¬ Archiving NIAS theory files..."
mkdir -p "$ARCHIVE_DIR/nias_theory"
"""
        for file in categories["nias_theory"][:20]:
            script += f'mv "{file}" "$ARCHIVE_DIR/nias_theory/" 2>/dev/null || true\n'

    # Archive duplicate APIs
    if categories["duplicate_apis"]:
        script += """
# Archive duplicate API implementations
echo "ðŸ”„ Archiving duplicate APIs..."
mkdir -p "$ARCHIVE_DIR/duplicate_apis"
"""
        for file in categories["duplicate_apis"][:10]:
            script += f'mv "{file}" "$ARCHIVE_DIR/duplicate_apis/" 2>/dev/null || true\n'

    # Archive old variants
    if categories["archive_candidates"]:
        script += """
# Archive old implementations
echo "ðŸ“š Archiving old implementations..."
mkdir -p "$ARCHIVE_DIR/old_implementations"
"""
        for file in categories["archive_candidates"][:30]:
            script += f'mv "{file}" "$ARCHIVE_DIR/old_implementations/" 2>/dev/null || true\n'

    script += """
echo "âœ… Cleanup complete!"
echo "ðŸ“Š Files archived to: $ARCHIVE_DIR"
echo "ðŸ’¾ Run 'git status' to see changes"
"""

    return script


def main():
    print("ðŸ” Analyzing cleanup candidates...")
    categories = analyze_cleanup_candidates()

    # Print summary
    print("\nðŸ“Š CLEANUP ANALYSIS")
    print("=" * 60)

    total_candidates = sum(len(files) for files in categories.values())
    print(f"Total cleanup candidates: {total_candidates}")
    print()

    for category, files in categories.items():
        if files:
            print(f"\n{category.upper().replace('_', ' ')}: {len(files)} files")
            for file in files[:5]:  # Show first 5
                print(f"  - {file}")
            if len(files) > 5:
                print(f"  ... and {len(files) - 5} more")

    # Generate cleanup script
    script = generate_cleanup_script(categories)
    script_path = Path("scripts/cleanup_orphaned_files.sh")
    with open(script_path, "w") as f:
        f.write(script)
    script_path.chmod(0o755)

    print(f"\nâœ… Cleanup script generated: {script_path}")
    print("Run: ./scripts/cleanup_orphaned_files.sh")

    # Calculate space savings
    total_size = 0
    for category, files in categories.items():
        for file in files:
            with contextlib.suppress(Exception):
                total_size += Path(file).stat().st_size

    print(f"\nðŸ’¾ Estimated space savings: {total_size / 1024 / 1024:.1f} MB")


if __name__ == "__main__":
    main()
