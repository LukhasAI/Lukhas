#!/usr/bin/env python3
"""
Matrix Tracks SLO Monitor

Calculates and reports Service Level Objectives for Matrix Tracks system:
- Gate Flakiness Rate: Infrastructure failure percentage
- OSV Mean Time to Green: Vulnerability response time
- Telemetry Compliance Rate: Semconv attribute compliance

Generates weekly SLO reports and triggers alerts when SLOs are at risk.
"""

import json
import yaml
import sys
import glob
import pathlib
import subprocess
from datetime import datetime, timedelta
from typing import Dict, List, Optional, Any, Tuple
import argparse


class SLOMonitor:
    """Monitor Matrix Tracks SLOs and generate reports."""

    def __init__(self, slo_config_path: str = "docs/matrix_tracks_slos.yaml"):
        """Initialize with SLO configuration."""
        self.config = self._load_config(slo_config_path)
        self.measurement_window = timedelta(days=7)  # Weekly by default
        self.timestamp = datetime.utcnow()

    def _load_config(self, config_path: str) -> Dict[str, Any]:
        """Load SLO configuration from YAML."""
        try:
            with open(config_path) as f:
                return yaml.safe_load(f)
        except FileNotFoundError:
            print(f"‚ùå SLO config not found: {config_path}")
            return {}
        except yaml.YAMLError as e:
            print(f"‚ùå Invalid YAML in SLO config: {e}")
            return {}

    def calculate_gate_flakiness_rate(self) -> Dict[str, Any]:
        """
        Calculate gate flakiness rate from CI artifacts.

        In production, this would query GitHub Actions API or parse CI logs.
        For now, we'll use mock data and artifacts.
        """
        # Mock calculation - in production, query GitHub Actions API
        try:
            # Look for gate failure artifacts
            reports = glob.glob("reports/matrix_tracks_status_*.md")

            if not reports:
                return {
                    "value": 0.0,
                    "status": "unknown",
                    "message": "No gate reports found",
                    "data_points": 0
                }

            # Mock data - in production, parse actual CI failure logs
            total_runs = 50  # Mock: CI runs in past week
            infrastructure_failures = 0  # Mock: Failures due to infrastructure

            flakiness_rate = (infrastructure_failures / total_runs) * 100
            target = 0.5  # <0.5%

            status = "passing" if flakiness_rate < target else "failing"

            return {
                "value": flakiness_rate,
                "target": target,
                "status": status,
                "message": f"{flakiness_rate:.2f}% flakiness rate (target: <{target}%)",
                "data_points": total_runs,
                "details": {
                    "total_runs": total_runs,
                    "infrastructure_failures": infrastructure_failures,
                    "failure_types": []  # Would list specific failure types
                }
            }

        except Exception as e:
            return {
                "value": None,
                "status": "error",
                "message": f"Failed to calculate flakiness rate: {e}",
                "data_points": 0
            }

    def calculate_osv_mean_time_to_green(self) -> Dict[str, Any]:
        """
        Calculate mean time to resolve high-severity vulnerabilities.

        In production, this would track OSV scan results and PR merge times.
        """
        try:
            # Look for OSV scan artifacts
            osv_artifacts = glob.glob("artifacts/*osv*.json")

            # Mock calculation - in production, track vulnerability lifecycle
            high_severity_vulns = 0  # Count from past week
            total_resolution_hours = 0.0  # Sum of resolution times

            if high_severity_vulns == 0:
                return {
                    "value": 0.0,
                    "status": "no_data",
                    "message": "No high-severity vulnerabilities detected in measurement window",
                    "data_points": 0
                }

            mean_time_hours = total_resolution_hours / high_severity_vulns
            target_hours = 48  # <48h

            status = "passing" if mean_time_hours < target_hours else "failing"

            return {
                "value": mean_time_hours,
                "target": target_hours,
                "status": status,
                "message": f"{mean_time_hours:.1f}h mean resolution time (target: <{target_hours}h)",
                "data_points": high_severity_vulns,
                "details": {
                    "vulnerabilities_detected": high_severity_vulns,
                    "total_resolution_time": total_resolution_hours,
                    "resolution_times": []  # Individual resolution times
                }
            }

        except Exception as e:
            return {
                "value": None,
                "status": "error",
                "message": f"Failed to calculate OSV MTTG: {e}",
                "data_points": 0
            }

    def calculate_telemetry_compliance_rate(self) -> Dict[str, Any]:
        """
        Calculate telemetry semconv compliance rate.

        Based on telemetry smoke test results and fixture validation.
        """
        try:
            # Check telemetry test results
            telemetry_fixtures = glob.glob("telemetry/fixtures/*.json")

            if not telemetry_fixtures:
                return {
                    "value": 0.0,
                    "status": "no_data",
                    "message": "No telemetry fixtures found",
                    "data_points": 0
                }

            # Mock calculation - in production, analyze test results
            compliant_fixtures = 0
            total_fixtures = len(telemetry_fixtures)

            for fixture_path in telemetry_fixtures:
                try:
                    with open(fixture_path) as f:
                        fixture_data = json.load(f)

                    # Check for required semconv attributes
                    is_compliant = self._validate_telemetry_fixture(fixture_data)
                    if is_compliant:
                        compliant_fixtures += 1

                except Exception:
                    continue  # Skip invalid fixtures

            compliance_rate = (compliant_fixtures / total_fixtures) * 100
            target_rate = 95.0  # >95%

            status = "passing" if compliance_rate > target_rate else "failing"

            return {
                "value": compliance_rate,
                "target": target_rate,
                "status": status,
                "message": f"{compliance_rate:.1f}% compliance rate (target: >{target_rate}%)",
                "data_points": total_fixtures,
                "details": {
                    "compliant_fixtures": compliant_fixtures,
                    "total_fixtures": total_fixtures,
                    "non_compliant": total_fixtures - compliant_fixtures
                }
            }

        except Exception as e:
            return {
                "value": None,
                "status": "error",
                "message": f"Failed to calculate telemetry compliance: {e}",
                "data_points": 0
            }

    def _validate_telemetry_fixture(self, fixture_data: Dict) -> bool:
        """Validate that telemetry fixture has required semconv attributes."""
        required_attrs = {"code.function", "lukhas.module", "otel.semconv.version"}

        # Check spans
        spans = fixture_data.get("spans", [])
        for span in spans:
            attrs = span.get("attributes", {})
            if not required_attrs.issubset(set(attrs.keys())):
                return False

        # Check metrics (if any)
        metrics = fixture_data.get("metrics", [])
        # Metrics compliance check would be different

        return True

    def generate_slo_report(self) -> Dict[str, Any]:
        """Generate comprehensive SLO report."""
        print("üìä Calculating Matrix Tracks SLOs...")

        # Calculate all SLOs
        slos = {}
        slos["gate_flakiness_rate"] = self.calculate_gate_flakiness_rate()
        slos["osv_mean_time_to_green"] = self.calculate_osv_mean_time_to_green()
        slos["telemetry_compliance_rate"] = self.calculate_telemetry_compliance_rate()

        # Calculate overall status
        statuses = [slo["status"] for slo in slos.values()]
        overall_status = "passing" if all(s in ["passing", "no_data"] for s in statuses) else "failing"

        report = {
            "timestamp": self.timestamp.isoformat(),
            "measurement_window": "7d",
            "overall_status": overall_status,
            "slos": slos,
            "summary": self._generate_summary(slos),
            "recommendations": self._generate_recommendations(slos)
        }

        return report

    def _generate_summary(self, slos: Dict[str, Any]) -> str:
        """Generate human-readable summary of SLO status."""
        summary_lines = []

        for slo_name, slo_data in slos.items():
            status_emoji = {
                "passing": "‚úÖ",
                "failing": "‚ùå",
                "no_data": "‚ÑπÔ∏è",
                "error": "‚ö†Ô∏è"
            }.get(slo_data["status"], "‚ùì")

            summary_lines.append(f"{status_emoji} **{slo_name.replace('_', ' ').title()}**: {slo_data['message']}")

        return "\n".join(summary_lines)

    def _generate_recommendations(self, slos: Dict[str, Any]) -> List[str]:
        """Generate actionable recommendations based on SLO status."""
        recommendations = []

        for slo_name, slo_data in slos.items():
            if slo_data["status"] == "failing":
                if slo_name == "gate_flakiness_rate":
                    recommendations.append("üîß Improve OSV scanner fallback logic and add retry mechanisms")
                elif slo_name == "osv_mean_time_to_green":
                    recommendations.append("üö® Accelerate security vulnerability response process")
                elif slo_name == "telemetry_compliance_rate":
                    recommendations.append("üì° Add pre-commit hooks for telemetry validation")

            elif slo_data["status"] == "no_data":
                recommendations.append(f"üìä Improve data collection for {slo_name.replace('_', ' ')}")

        if not recommendations:
            recommendations.append("üéâ All SLOs are healthy! Continue monitoring.")

        return recommendations

    def save_report(self, report: Dict[str, Any], output_path: Optional[str] = None) -> str:
        """Save SLO report to file."""
        if not output_path:
            timestamp = self.timestamp.strftime("%Y%m%d_%H%M%S")
            output_path = f"reports/slo/matrix_tracks_slo_report_{timestamp}.md"

        pathlib.Path(output_path).parent.mkdir(parents=True, exist_ok=True)

        # Generate markdown report
        markdown_content = self._generate_markdown_report(report)

        with open(output_path, 'w') as f:
            f.write(markdown_content)

        # Also save JSON for programmatic access
        json_path = output_path.replace('.md', '.json')
        with open(json_path, 'w') as f:
            json.dump(report, f, indent=2)

        return output_path

    def _generate_markdown_report(self, report: Dict[str, Any]) -> str:
        """Generate markdown formatted SLO report."""
        content = f"""# Matrix Tracks SLO Report

**Generated:** {report['timestamp']}
**Measurement Window:** {report['measurement_window']}
**Overall Status:** {"‚úÖ HEALTHY" if report['overall_status'] == 'passing' else "‚ùå NEEDS ATTENTION"}

---

## üìä SLO Summary

{report['summary']}

---

## üìã Detailed Results

"""

        for slo_name, slo_data in report['slos'].items():
            status_emoji = {
                "passing": "‚úÖ",
                "failing": "‚ùå",
                "no_data": "‚ÑπÔ∏è",
                "error": "‚ö†Ô∏è"
            }.get(slo_data["status"], "‚ùì")

            content += f"""### {status_emoji} {slo_name.replace('_', ' ').title()}

- **Status:** {slo_data['status']}
- **Message:** {slo_data['message']}
- **Data Points:** {slo_data['data_points']}
"""

            if 'details' in slo_data:
                content += f"- **Details:** {json.dumps(slo_data['details'], indent=2)}\n"

            content += "\n"

        content += f"""---

## üéØ Recommendations

"""
        for rec in report['recommendations']:
            content += f"- {rec}\n"

        content += f"""
---

## üìà Trend Analysis

[Trend analysis would be added based on historical data]

## üîî Alert Status

[Alert status would be checked against thresholds]

---

*Report generated by Matrix Tracks SLO Monitor*
"""

        return content

    def run(self, output_path: Optional[str] = None) -> str:
        """Main entry point - generate and save SLO report."""
        report = self.generate_slo_report()
        report_path = self.save_report(report, output_path)

        print(f"\nüìÑ SLO Report saved to: {report_path}")
        print(f"üìä Overall Status: {'‚úÖ HEALTHY' if report['overall_status'] == 'passing' else '‚ùå NEEDS ATTENTION'}")
        print(f"\nSummary:")
        print(report['summary'])

        return report_path


def main():
    """CLI entry point."""
    parser = argparse.ArgumentParser(description="Monitor Matrix Tracks SLOs")
    parser.add_argument("--config", default="docs/matrix_tracks_slos.yaml",
                       help="Path to SLO configuration file")
    parser.add_argument("--output", help="Output path for report")
    parser.add_argument("--json-only", action="store_true",
                       help="Output JSON only (for programmatic use)")

    args = parser.parse_args()

    monitor = SLOMonitor(args.config)

    if args.json_only:
        report = monitor.generate_slo_report()
        print(json.dumps(report, indent=2))
    else:
        monitor.run(args.output)


if __name__ == "__main__":
    main()