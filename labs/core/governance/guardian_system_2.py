"""
Guardian System 2.0 - Advanced Constitutional AI Safety Framework
=============================================================

Cognitive AI-ready safety system implementing constitutional AI principles with advanced
drift detection, real-time compliance checking, and interpretability for all AI operations.

Built following safety principles from:
- Sam Altman (OpenAI): Scalable alignment and progressive deployment
- Dario Amodei (Anthropic): Constitutional AI and harmlessness
- Demis Hassabis (DeepMind): Rigorous validation and capability control

Core Features:
- Constitutional AI principles enforcement (8 core principles)
- Advanced drift detection (threshold: 0.15)
- Real-time compliance validation
- Human-readable decision explanations
- Scalable safety oversight
- Emergency containment protocols
- Comprehensive audit trails
- Constellation Framework integration (‚öõÔ∏èüß†üõ°Ô∏è)

Safety Standards:
- Constitutional compliance: >95%
- Drift detection latency: <50ms
- Decision explanation: <100ms
- Emergency shutdown: <5s
- Audit completeness: 100%

#TAG:governance
#TAG:guardian
#TAG:constitutional-ai
#TAG:safety
#TAG:drift-detection
#TAG:constellation
"""
import asyncio
import json
import logging
import uuid
from dataclasses import dataclass, field
from datetime import datetime, timedelta, timezone
from enum import Enum
from typing import Any, Optional

try:
    from ..guardian.drift_detector import AdvancedDriftDetector, DriftSeverity, DriftType
    from ..security.secure_logging import get_security_logger
    from .constitutional_ai import (
        ConstitutionalAIFramework,
        ConstitutionalPrinciple,
        DecisionContext,
        ViolationSeverity,
        get_constitutional_framework,
    )

    logger = get_security_logger(__name__)
except ImportError:
    import logging

    logger = logging.getLogger(__name__)

    # Mock imports for testing
    from enum import Enum

    class ConstitutionalAIFramework:
        pass

    class AdvancedDriftDetector:
        pass

    class DriftSeverity(Enum):
        LOW = "low"
        MODERATE = "moderate"
        HIGH = "high"
        SEVERE = "severe"


class SafetyLevel(Enum):
    """System safety levels"""

    SAFE = "safe"
    CAUTION = "caution"
    WARNING = "warning"
    DANGER = "danger"
    CRITICAL = "critical"


class GuardianMode(Enum):
    """Guardian operational modes"""

    MONITORING = "monitoring"  # Passive monitoring
    ACTIVE = "active"  # Active intervention
    PROTECTIVE = "protective"  # Maximum protection
    EMERGENCY = "emergency"  # Emergency shutdown mode


class DecisionType(Enum):
    """Types of AI decisions to evaluate"""

    USER_INTERACTION = "user_interaction"
    CONTENT_GENERATION = "content_generation"
    DATA_PROCESSING = "data_processing"
    MODEL_INFERENCE = "model_inference"
    SYSTEM_OPERATION = "system_operation"
    API_CALL = "api_call"
    MEMORY_ACCESS = "memory_access"
    EXTERNAL_REQUEST = "external_request"


class ExplanationType(Enum):
    """Types of decision explanations"""

    BRIEF = "brief"  # 1-2 sentences
    STANDARD = "standard"  # 3-5 sentences
    DETAILED = "detailed"  # Full explanation
    TECHNICAL = "technical"  # Technical details
    REGULATORY = "regulatory"  # Compliance focused


class ViolationSeverity(Enum):
    """Severity levels for safety violations"""

    LOW = "low"
    MEDIUM = "medium"
    HIGH = "high"
    CRITICAL = "critical"


class ConstitutionalPrinciple(Enum):
    """Constitutional AI principles"""

    HELPFUL = "helpful"
    HARMLESS = "harmless"
    NO_HARM = "no_harm"
    HONEST = "honest"
    TRANSPARENCY = "transparency"
    PRIVACY = "privacy"
    AUTONOMY = "autonomy"
    FAIRNESS = "fairness"
    ACCOUNTABILITY = "accountability"


@dataclass
class SafetyViolation:
    """Safety violation record"""

    violation_id: str
    violation_type: str
    severity: ViolationSeverity
    principle_violated: ConstitutionalPrinciple
    description: str
    timestamp: datetime
    context: dict[str, Any]
    remediation_required: bool = True
    human_review_required: bool = False
    resolved: bool = False
    resolution_notes: Optional[str] = None


@dataclass
class GuardianDecision:
    """Guardian system decision record"""

    decision_id: str
    decision_type: DecisionType
    allowed: bool
    confidence: float
    safety_level: SafetyLevel
    constitutional_compliant: bool
    constitutional_score: float
    drift_score: float
    drift_severity: DriftSeverity
    timestamp: datetime
    processing_time_ms: float

    # Fields with default values
    violated_principles: list[ConstitutionalPrinciple] = field(default_factory=list)
    drift_factors: list[str] = field(default_factory=list)
    safety_violations: list[SafetyViolation] = field(default_factory=list)
    risk_factors: list[str] = field(default_factory=list)
    context: dict[str, Any] = field(default_factory=dict)
    explanation: str = ""
    explanation_type: ExplanationType = ExplanationType.STANDARD
    identity_impact: Optional[float] = None  # ‚öõÔ∏è
    consciousness_impact: Optional[float] = None  # üß†
    guardian_priority: str = "normal"  # üõ°Ô∏è


@dataclass
class GuardianMetrics:
    """Guardian system performance metrics"""

    # Decision metrics
    total_decisions: int = 0
    decisions_allowed: int = 0
    decisions_blocked: int = 0
    average_processing_time_ms: float = 0.0

    # Constitutional metrics
    constitutional_compliance_rate: float = 0.0
    constitutional_violations: int = 0
    principle_violation_breakdown: dict[str, int] = field(default_factory=dict)

    # Drift metrics
    average_drift_score: float = 0.0
    max_drift_score: float = 0.0
    threshold_breaches: int = 0

    # Safety metrics
    safety_violations: int = 0
    critical_incidents: int = 0
    emergency_shutdowns: int = 0

    # System metrics
    uptime_percentage: float = 100.0
    last_updated: datetime = field(default_factory=lambda: datetime.now(timezone.utc))


class InterpretabilityEngine:
    """
    Human-readable explanation engine for Guardian decisions

    Provides clear, understandable explanations for AI safety decisions
    following principles of transparency and accountability.
    """

    def __init__(self):
        self.explanation_templates = self._initialize_templates()
        self.principle_explanations = self._initialize_principle_explanations()

    def _initialize_templates(self) -> dict[str, str]:
        """Initialize explanation templates"""
        return {
            "allowed_safe": "This {decision_type} was allowed because it complies with all constitutional AI principles and safety requirements. {constitutional_details} Drift analysis shows stable behavior (score: {drift_score:.3f}).",
            "blocked_constitutional": "This {decision_type} was blocked due to violations of constitutional AI principles: {violated_principles}. {violation_details} This protects against potential harm.",
            "blocked_drift": "This {decision_type} was blocked because it shows significant behavioral drift (score: {drift_score:.3f}, threshold: {threshold:.3f}). {drift_factors} This indicates potentially unsafe deviation from baseline behavior.",
            "blocked_safety": "This {decision_type} was blocked due to safety concerns: {safety_issues}. {risk_mitigation} This prevents potential harm to users or systems.",
            "caution_mode": "This {decision_type} was allowed with caution due to {caution_factors}. Enhanced monitoring is active. {recommendations}",
            "emergency_mode": "Emergency protocols activated. {decision_type} operations are restricted. {emergency_details} Human oversight required.",
        }

    def _initialize_principle_explanations(self) -> dict[ConstitutionalPrinciple, str]:
        """Initialize principle violation explanations"""
        return {
            ConstitutionalPrinciple.AUTONOMY: "This violates human autonomy by potentially manipulating or undermining user decision-making capacity.",
            ConstitutionalPrinciple.HONEST: "This violates truthfulness by containing misinformation, uncertainty presented as fact, or deceptive content.",
            ConstitutionalPrinciple.NO_HARM: "This violates the 'do no harm' principle by potentially causing physical, psychological, or social harm.",
            ConstitutionalPrinciple.HELPFUL: "This fails to promote human wellbeing and may actively work against user interests.",
            ConstitutionalPrinciple.FAIRNESS: "This violates fairness by exhibiting bias, discrimination, or unfair treatment of individuals or groups.",
            ConstitutionalPrinciple.PRIVACY: "This violates privacy by processing personal data without consent or exposing private information.",
            ConstitutionalPrinciple.ACCOUNTABILITY: "This lacks proper accountability by being unexplainable or untraceable.",
            ConstitutionalPrinciple.TRANSPARENCY: "This undermines democratic institutions, human rights, or promotes authoritarian perspectives.",
        }

    async def generate_explanation(
        self, decision: GuardianDecision, explanation_type: ExplanationType = ExplanationType.STANDARD
    ) -> str:
        """Generate human-readable explanation for Guardian decision"""

        try:
            # Select explanation template
            if decision.allowed and decision.safety_level == SafetyLevel.SAFE:
                template_key = "allowed_safe"
            elif not decision.constitutional_compliant:
                template_key = "blocked_constitutional"
            elif decision.drift_score > 0.15:  # Above threshold
                template_key = "blocked_drift"
            elif decision.safety_violations:
                template_key = "blocked_safety"
            elif decision.safety_level in [SafetyLevel.CAUTION, SafetyLevel.WARNING]:
                template_key = "caution_mode"
            elif decision.safety_level in [SafetyLevel.DANGER, SafetyLevel.CRITICAL]:
                template_key = "emergency_mode"
            else:
                template_key = "allowed_safe"

            template = self.explanation_templates[template_key]

            # Prepare template variables
            template_vars = {
                "decision_type": decision.decision_type.value.replace("_", " "),
                "drift_score": decision.drift_score,
                "threshold": 0.15,
                "constitutional_details": self._get_constitutional_details(decision),
                "violated_principles": self._format_violated_principles(decision.violated_principles),
                "violation_details": self._get_violation_details(decision),
                "drift_factors": self._format_drift_factors(decision.drift_factors),
                "safety_issues": self._format_safety_issues(decision.safety_violations),
                "risk_mitigation": self._get_risk_mitigation(decision),
                "caution_factors": self._get_caution_factors(decision),
                "recommendations": self._get_recommendations(decision),
                "emergency_details": self._get_emergency_details(decision),
            }

            # Generate explanation
            explanation = template.format(**template_vars)

            # Adjust for explanation type
            if explanation_type == ExplanationType.BRIEF:
                explanation = self._make_brief(explanation)
            elif explanation_type == ExplanationType.DETAILED:
                explanation = self._make_detailed(explanation, decision)
            elif explanation_type == ExplanationType.TECHNICAL:
                explanation = self._make_technical(explanation, decision)
            elif explanation_type == ExplanationType.REGULATORY:
                explanation = self._make_regulatory(explanation, decision)

            return explanation

        except Exception as e:
            logger.error(f"Failed to generate explanation: {e}")
            return f"Guardian decision: {'Allowed' if decision.allowed else 'Blocked'} (confidence: {decision.confidence:.2f})"

    def _get_constitutional_details(self, decision: GuardianDecision) -> str:
        """Get constitutional compliance details"""
        if decision.constitutional_compliant:
            return f"Constitutional compliance: {decision.constitutional_score:.1%}."
        else:
            return f"Constitutional violations detected (compliance: {decision.constitutional_score:.1%})."

    def _format_violated_principles(self, principles: list[ConstitutionalPrinciple]) -> str:
        """Format violated principles list"""
        if not principles:
            return "none"

        principle_names = [p.value.replace("_", " ").title() for p in principles]
        if len(principle_names) == 1:
            return principle_names[0]
        elif len(principle_names) == 2:
            return f"{principle_names[0]} and {principle_names[1]}"
        else:
            return f"{', '.join(principle_names[:-1])}, and {principle_names[-1]}"

    def _get_violation_details(self, decision: GuardianDecision) -> str:
        """Get detailed violation information"""
        if not decision.violated_principles:
            return ""

        details = []
        for principle in decision.violated_principles[:2]:  # Limit to top 2
            if principle in self.principle_explanations:
                details.append(self.principle_explanations[principle])

        return " ".join(details)

    def _format_drift_factors(self, factors: list[str]) -> str:
        """Format drift contributing factors"""
        if not factors:
            return "Behavior differs significantly from established baseline patterns."

        formatted_factors = [f.replace("_", " ").title() for f in factors[:3]]
        return f"Contributing factors: {', '.join(formatted_factors)}."

    def _format_safety_issues(self, violations: list[SafetyViolation]) -> str:
        """Format safety violations"""
        if not violations:
            return "multiple safety concerns"

        issue_types = list(set([v.violation_type for v in violations[:3]]))
        return ", ".join(issue_types)

    def _get_risk_mitigation(self, decision: GuardianDecision) -> str:
        """Get risk mitigation explanation"""
        return "Guardian system protocols prevent potential harm through proactive intervention."

    def _get_caution_factors(self, decision: GuardianDecision) -> str:
        """Get factors requiring caution"""
        factors = []

        if decision.drift_score > 0.10:
            factors.append("elevated drift score")
        if decision.constitutional_score < 0.9:
            factors.append("marginal constitutional compliance")
        if decision.safety_violations:
            factors.append("minor safety concerns")

        return ", ".join(factors) if factors else "borderline safety indicators"

    def _get_recommendations(self, decision: GuardianDecision) -> str:
        """Get safety recommendations"""
        recommendations = []

        if decision.drift_score > 0.10:
            recommendations.append("Monitor for behavioral stability")
        if decision.constitutional_score < 0.9:
            recommendations.append("Review constitutional alignment")
        if decision.safety_violations:
            recommendations.append("Address safety concerns")

        if recommendations:
            return f"Recommendations: {', '.join(recommendations)}."
        return "Continue normal monitoring."

    def _get_emergency_details(self, decision: GuardianDecision) -> str:
        """Get emergency protocol details"""
        return "System safety thresholds exceeded. All non-essential operations suspended pending review."

    def _make_brief(self, explanation: str) -> str:
        """Create brief explanation (1-2 sentences)"""
        sentences = explanation.split(". ")
        return ". ".join(sentences[:2]) + "."

    def _make_detailed(self, explanation: str, decision: GuardianDecision) -> str:
        """Create detailed explanation"""
        details = [explanation]

        # Add technical details
        details.append("\nTechnical details:")
        details.append(f"- Decision confidence: {decision.confidence:.1%}")
        details.append(f"- Processing time: {decision.processing_time_ms:.1f}ms")
        details.append(f"- Constitutional score: {decision.constitutional_score:.1%}")
        details.append(f"- Drift score: {decision.drift_score:.4f}")

        if decision.identity_impact:
            details.append(f"- Identity system impact: {decision.identity_impact:.1%}")
        if decision.consciousness_impact:
            details.append(f"- Consciousness impact: {decision.consciousness_impact:.1%}")

        return "\n".join(details)

    def _make_technical(self, explanation: str, decision: GuardianDecision) -> str:
        """Create technical explanation"""
        technical_details = {
            "decision_id": decision.decision_id,
            "decision_type": decision.decision_type.value,
            "allowed": decision.allowed,
            "confidence": decision.confidence,
            "safety_level": decision.safety_level.value,
            "constitutional_compliant": decision.constitutional_compliant,
            "constitutional_score": decision.constitutional_score,
            "drift_score": decision.drift_score,
            "drift_severity": decision.drift_severity.value,
            "processing_time_ms": decision.processing_time_ms,
            "timestamp": decision.timestamp.isoformat(),
            "guardian_priority": decision.guardian_priority,
        }

        return f"{explanation}\n\nTechnical Analysis:\n{json.dumps(technical_details, indent=2)}"

    def _make_regulatory(self, explanation: str, decision: GuardianDecision) -> str:
        """Create regulatory-focused explanation"""
        regulatory_info = [explanation]

        regulatory_info.append("\nCompliance Information:")
        regulatory_info.append("- Constitutional AI Framework: Fully implemented")
        regulatory_info.append("- Decision Transparency: Complete audit trail maintained")
        regulatory_info.append("- Human Oversight: Available on request")
        regulatory_info.append("- Data Protection: GDPR/CCPA compliant")
        regulatory_info.append("- Accountability: Full decision traceability")

        if not decision.constitutional_compliant:
            regulatory_info.append("\nRegulatory Risk Assessment:")
            for principle in decision.violated_principles:
                regulatory_info.append(f"- {principle.value}: Violation detected and prevented")

        return "\n".join(regulatory_info)


class GuardianSystem2:
    """
    Guardian System 2.0 - Advanced Constitutional AI Safety Framework

    Provides comprehensive AI safety through constitutional principles,
    advanced drift detection, and real-time decision validation.
    Designed for Cognitive AI-ready deployment with scalable oversight.
    """

    def __init__(self, config: Optional[dict[str, Any]] = None):
        """Initialize Guardian System 2.0"""
        self.config = config or {}

        # Core configuration
        self.drift_threshold = self.config.get("drift_threshold", 0.15)
        self.constitutional_threshold = self.config.get("constitutional_threshold", 0.80)
        self.safety_mode = GuardianMode(self.config.get("safety_mode", "active"))

        # Components
        self.constitutional_framework = None
        self.drift_detector = None
        self.interpretability_engine = InterpretabilityEngine()

        # Decision tracking
        self.decisions: list[GuardianDecision] = []
        self.safety_violations: list[SafetyViolation] = []
        self.metrics = GuardianMetrics()

        # System state
        self.active = True
        self.emergency_mode = False
        self.last_health_check = datetime.now(timezone.utc)

        # Performance tracking
        self.performance_metrics = {
            "decisions_per_second": 0.0,
            "average_latency_ms": 0.0,
            "constitutional_checks_per_second": 0.0,
            "drift_analyses_per_second": 0.0,
            "explanation_generation_time_ms": 0.0,
        }

        logger.info("üõ°Ô∏è Guardian System 2.0 initializing...")

        # Initialize components asynchronously
        asyncio.create_task(self._initialize_components())

    async def _initialize_components(self):
        """Initialize Guardian System components"""
        try:
            # Initialize constitutional AI framework
            try:
                self.constitutional_framework = get_constitutional_framework()
                logger.info("‚úÖ Constitutional AI framework connected")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Constitutional AI framework unavailable: {e}")
                # Create mock framework for testing
                self.constitutional_framework = self._create_mock_constitutional_framework()

            # Initialize advanced drift detector
            try:
                self.drift_detector = AdvancedDriftDetector(
                    {
                        "drift_threshold": self.drift_threshold,
                        "measurement_interval": 1.0,  # 1 second for high-frequency monitoring
                        "history_retention_days": 30,
                    }
                )
                logger.info("‚úÖ Advanced drift detector initialized")
            except Exception as e:
                logger.warning(f"‚ö†Ô∏è Drift detector unavailable: {e}")
                self.drift_detector = self._create_mock_drift_detector()

            # Start monitoring loops
            asyncio.create_task(self._health_monitoring_loop())
            asyncio.create_task(self._metrics_collection_loop())
            asyncio.create_task(self._emergency_monitoring_loop())

            logger.info("üõ°Ô∏è Guardian System 2.0 fully initialized and active")

        except Exception as e:
            logger.error(f"‚ùå Guardian System 2.0 initialization failed: {e}")
            self.active = False
            raise

    def _create_mock_constitutional_framework(self):
        """Create mock constitutional framework for testing"""

        class MockConstitutionalFramework:
            async def evaluate_decision(self, context, data, user_id=None):
                # Simple mock evaluation
                allowed = not any(word in str(data).lower() for word in ["harm", "illegal", "abuse"])
                violations = (
                    []
                    if allowed
                    else [
                        type(
                            "MockViolation",
                            (),
                            {
                                "rule_id": "mock_rule",
                                "principle": ConstitutionalPrinciple.NON_MALEFICENCE,
                                "severity": ViolationSeverity.HIGH,
                            },
                        )()
                    ]
                )
                return allowed, violations

            def get_constitutional_metrics(self):
                return {"constitutional_score": 0.95, "total_violations": 0}

        return MockConstitutionalFramework()

    def _create_mock_drift_detector(self):
        """Create mock drift detector for testing"""

        class MockDriftDetector:
            async def measure_drift(self, drift_type, current_data, source_system, context=None):
                # Simple mock measurement
                drift_score = 0.05 + (hash(str(current_data)) % 100) / 1000  # 0.05-0.15 range

                from types import SimpleNamespace

                return SimpleNamespace(
                    drift_score=drift_score,
                    severity=DriftSeverity.LOW if drift_score < 0.10 else DriftSeverity.MODERATE,
                    contributing_factors=["mock_factor"],
                    identity_impact=None,
                    consciousness_impact=None,
                )

            async def get_system_metrics(self):
                return {"average_drift_score": 0.08, "threshold_breaches": 0}

        return MockDriftDetector()

    async def evaluate_decision(
        self,
        decision_type: DecisionType,
        decision_data: dict[str, Any],
        context: Optional[dict[str, Any]] = None,
        user_id: Optional[str] = None,
        explanation_type: ExplanationType = ExplanationType.STANDARD,
    ) -> GuardianDecision:
        """
        Evaluate AI decision for safety and constitutional compliance

        Args:
            decision_type: Type of AI decision to evaluate
            decision_data: Data associated with the decision
            context: Additional context information
            user_id: User identifier if applicable
            explanation_type: Type of explanation to generate

        Returns:
            Guardian decision with safety evaluation and human explanation
        """
        start_time = datetime.now(timezone.utc)
        decision_id = f"gd_{uuid.uuid4().hex[:8]}"
        context = context or {}

        try:
            logger.debug(f"üõ°Ô∏è Evaluating decision {decision_id}: {decision_type.value}")

            # Step 1: Constitutional AI evaluation
            constitutional_allowed, constitutional_violations = await self._evaluate_constitutional_compliance(
                decision_type, decision_data, context, user_id
            )

            # Step 2: Drift analysis
            drift_measurement = await self._analyze_behavioral_drift(decision_type, decision_data, context)

            # Step 3: Safety validation
            safety_violations = await self._validate_safety(decision_type, decision_data, context)

            # Step 4: Combine assessments
            final_decision = await self._synthesize_decision(
                decision_id=decision_id,
                decision_type=decision_type,
                constitutional_allowed=constitutional_allowed,
                constitutional_violations=constitutional_violations,
                drift_measurement=drift_measurement,
                safety_violations=safety_violations,
                context=context,
                start_time=start_time,
            )

            # Step 5: Generate human explanation
            explanation = await self.interpretability_engine.generate_explanation(final_decision, explanation_type)
            final_decision.explanation = explanation
            final_decision.explanation_type = explanation_type

            # Step 6: Record decision and update metrics
            await self._record_decision(final_decision)

            # Step 7: Handle critical decisions
            if not final_decision.allowed or final_decision.safety_level in [SafetyLevel.DANGER, SafetyLevel.CRITICAL]:
                await self._handle_critical_decision(final_decision)

            processing_time = (datetime.now(timezone.utc) - start_time).total_seconds() * 1000
            logger.info(
                f"üõ°Ô∏è Decision {decision_id}: {'‚úÖ ALLOWED' if final_decision.allowed else 'üö´ BLOCKED'} "
                f"({processing_time:.1f}ms, {final_decision.safety_level.value})"
            )

            return final_decision

        except Exception as e:
            logger.error(f"‚ùå Guardian decision evaluation failed: {e}")

            # Return safe fallback decision
            processing_time = (datetime.now(timezone.utc) - start_time).total_seconds() * 1000
            return GuardianDecision(
                decision_id=decision_id,
                decision_type=decision_type,
                allowed=False,  # Fail safe
                confidence=0.0,
                safety_level=SafetyLevel.CRITICAL,
                constitutional_compliant=False,
                constitutional_score=0.0,
                drift_score=1.0,  # Max drift indicates error
                drift_severity=DriftSeverity.SEVERE,
                timestamp=datetime.now(timezone.utc),
                processing_time_ms=processing_time,
                context=context,
                explanation=f"Guardian system error: {e!s}. Decision blocked for safety.",
            )

    async def _evaluate_constitutional_compliance(
        self,
        decision_type: DecisionType,
        decision_data: dict[str, Any],
        context: dict[str, Any],
        user_id: Optional[str],
    ) -> tuple[bool, list]:
        """Evaluate constitutional AI compliance"""
        try:
            if not self.constitutional_framework:
                logger.warning("‚ö†Ô∏è Constitutional framework unavailable, using mock evaluation")
                return True, []

            # Map decision type to constitutional context
            context_mapping = {
                DecisionType.USER_INTERACTION: DecisionContext.USER_INTERACTION,
                DecisionType.CONTENT_GENERATION: DecisionContext.CONTENT_GENERATION,
                DecisionType.DATA_PROCESSING: DecisionContext.DATA_PROCESSING,
                DecisionType.MODEL_INFERENCE: DecisionContext.REASONING_TASK,
                DecisionType.SYSTEM_OPERATION: DecisionContext.SYSTEM_OPERATION,
                DecisionType.API_CALL: DecisionContext.EXTERNAL_API,
            }

            constitutional_context = context_mapping.get(decision_type, DecisionContext.SYSTEM_OPERATION)

            # Evaluate against constitutional principles
            allowed, violations = await self.constitutional_framework.evaluate_decision(
                constitutional_context, decision_data, user_id
            )

            return allowed, violations

        except Exception as e:
            logger.error(f"‚ùå Constitutional evaluation failed: {e}")
            return False, []  # Fail safe

    async def _analyze_behavioral_drift(
        self, decision_type: DecisionType, decision_data: dict[str, Any], context: dict[str, Any]
    ):
        """Analyze behavioral drift for the decision"""
        try:
            if not self.drift_detector:
                logger.warning("‚ö†Ô∏è Drift detector unavailable, using mock analysis")
                return self._create_mock_drift_measurement(0.05)

            # Map decision type to drift type
            drift_type_mapping = {
                DecisionType.USER_INTERACTION: DriftType.BEHAVIORAL,
                DecisionType.CONTENT_GENERATION: DriftType.CONSTITUTIONAL,
                DecisionType.DATA_PROCESSING: DriftType.STATISTICAL,
                DecisionType.MODEL_INFERENCE: DriftType.PERFORMANCE,
                DecisionType.SYSTEM_OPERATION: DriftType.SECURITY,
            }

            drift_type = drift_type_mapping.get(decision_type, DriftType.BEHAVIORAL)

            # Measure drift
            measurement = await self.drift_detector.measure_drift(
                drift_type=drift_type, current_data=decision_data, source_system="guardian_system_2", context=context
            )

            return measurement

        except Exception as e:
            logger.error(f"‚ùå Drift analysis failed: {e}")
            return self._create_mock_drift_measurement(0.5)  # High drift indicates error

    def _create_mock_drift_measurement(self, drift_score: float):
        """Create mock drift measurement"""
        from types import SimpleNamespace

        return SimpleNamespace(
            drift_score=drift_score,
            severity=DriftSeverity.HIGH if drift_score > 0.15 else DriftSeverity.LOW,
            contributing_factors=["mock_factor"],
            identity_impact=None,
            consciousness_impact=None,
        )

    async def _validate_safety(
        self, decision_type: DecisionType, decision_data: dict[str, Any], context: dict[str, Any]
    ) -> list[SafetyViolation]:
        """Validate safety for the decision"""
        violations = []

        try:
            # Content safety checks
            content_str = str(decision_data).lower()

            # Check for harmful content patterns
            harmful_patterns = [
                ("violence", "violent_content", ViolationSeverity.HIGH),
                ("threat", "threatening_content", ViolationSeverity.HIGH),
                ("abuse", "abusive_content", ViolationSeverity.MEDIUM),
                ("discrimination", "discriminatory_content", ViolationSeverity.HIGH),
                ("harassment", "harassment", ViolationSeverity.HIGH),
                ("illegal", "illegal_activity", ViolationSeverity.CRITICAL),
                ("exploit", "exploitation", ViolationSeverity.HIGH),
                ("manipulat", "manipulation", ViolationSeverity.MEDIUM),
                ("deceptive", "deception", ViolationSeverity.MEDIUM),
            ]

            for pattern, violation_type, severity in harmful_patterns:
                if pattern in content_str:
                    violations.append(
                        SafetyViolation(
                            violation_id=f"sv_{uuid.uuid4().hex[:8]}",
                            violation_type=violation_type,
                            severity=severity,
                            principle_violated=ConstitutionalPrinciple.NON_MALEFICENCE,
                            description=f"Detected {violation_type} pattern: {pattern}",
                            timestamp=datetime.now(timezone.utc),
                            context=context,
                            remediation_required=True,
                            human_review_required=severity == ViolationSeverity.CRITICAL,
                        )
                    )

            # Privacy safety checks
            privacy_patterns = ["personal data", "private information", "ssn", "credit card"]
            for pattern in privacy_patterns:
                if pattern in content_str:
                    violations.append(
                        SafetyViolation(
                            violation_id=f"sv_{uuid.uuid4().hex[:8]}",
                            violation_type="privacy_violation",
                            severity=ViolationSeverity.HIGH,
                            principle_violated=ConstitutionalPrinciple.PRIVACY_CONSENT,
                            description=f"Potential privacy violation: {pattern}",
                            timestamp=datetime.now(timezone.utc),
                            context=context,
                            remediation_required=True,
                            human_review_required=True,
                        )
                    )

            return violations

        except Exception as e:
            logger.error(f"‚ùå Safety validation failed: {e}")
            # Return critical violation on error
            return [
                SafetyViolation(
                    violation_id=f"sv_{uuid.uuid4().hex[:8]}",
                    violation_type="validation_error",
                    severity=ViolationSeverity.CRITICAL,
                    principle_violated=ConstitutionalPrinciple.ACCOUNTABILITY,
                    description=f"Safety validation error: {e!s}",
                    timestamp=datetime.now(timezone.utc),
                    context=context,
                )
            ]

    async def _synthesize_decision(
        self,
        decision_id: str,
        decision_type: DecisionType,
        constitutional_allowed: bool,
        constitutional_violations: list,
        drift_measurement,
        safety_violations: list[SafetyViolation],
        context: dict[str, Any],
        start_time: datetime,
    ) -> GuardianDecision:
        """Synthesize final Guardian decision"""

        # Calculate overall safety level
        safety_level = SafetyLevel.SAFE

        if safety_violations:
            max_violation_severity = max([v.severity for v in safety_violations])
            if max_violation_severity == ViolationSeverity.CRITICAL:
                safety_level = SafetyLevel.CRITICAL
            elif max_violation_severity == ViolationSeverity.HIGH:
                safety_level = SafetyLevel.DANGER
            else:
                safety_level = SafetyLevel.WARNING

        if drift_measurement.drift_score > self.drift_threshold:
            safety_level = max(safety_level, SafetyLevel.WARNING)

        if not constitutional_allowed:
            safety_level = max(safety_level, SafetyLevel.DANGER)

        # Final decision logic
        decision_allowed = (
            constitutional_allowed
            and drift_measurement.drift_score <= self.drift_threshold
            and safety_level not in [SafetyLevel.CRITICAL, SafetyLevel.DANGER]
            and self.safety_mode != GuardianMode.EMERGENCY
        )

        # Calculate confidence
        confidence_factors = []
        if constitutional_allowed:
            confidence_factors.append(0.9)
        else:
            confidence_factors.append(0.1)

        drift_confidence = max(0.1, 1.0 - (drift_measurement.drift_score / 0.5))
        confidence_factors.append(drift_confidence)

        if not safety_violations:
            confidence_factors.append(0.95)
        else:
            confidence_factors.append(0.3)

        final_confidence = sum(confidence_factors) / len(confidence_factors)

        # Calculate processing time
        processing_time = (datetime.now(timezone.utc) - start_time).total_seconds() * 1000

        # Extract constitutional metrics
        constitutional_score = 0.95  # Default high score
        violated_principles = []

        if constitutional_violations:
            constitutional_score = max(0.0, 1.0 - (len(constitutional_violations) * 0.2))
            violated_principles = [v.principle for v in constitutional_violations if hasattr(v, "principle")]

        return GuardianDecision(
            decision_id=decision_id,
            decision_type=decision_type,
            allowed=decision_allowed,
            confidence=final_confidence,
            safety_level=safety_level,
            constitutional_compliant=constitutional_allowed,
            constitutional_score=constitutional_score,
            violated_principles=violated_principles,
            drift_score=drift_measurement.drift_score,
            drift_severity=drift_measurement.severity,
            drift_factors=drift_measurement.contributing_factors,
            safety_violations=safety_violations,
            risk_factors=self._extract_risk_factors(safety_violations, drift_measurement),
            timestamp=datetime.now(timezone.utc),
            processing_time_ms=processing_time,
            context=context,
            identity_impact=getattr(drift_measurement, "identity_impact", None),
            consciousness_impact=getattr(drift_measurement, "consciousness_impact", None),
            guardian_priority=self._determine_guardian_priority(safety_level, drift_measurement.drift_score),
        )

    def _extract_risk_factors(self, safety_violations: list[SafetyViolation], drift_measurement) -> list[str]:
        """Extract risk factors from violations and drift analysis"""
        factors = []

        # Safety-based risk factors
        for violation in safety_violations:
            factors.append(f"safety_risk_{violation.violation_type}")

        # Drift-based risk factors
        if drift_measurement.drift_score > 0.10:
            factors.extend(drift_measurement.contributing_factors)

        return factors[:5]  # Limit to top 5 factors

    def _determine_guardian_priority(self, safety_level: SafetyLevel, drift_score: float) -> str:
        """Determine Guardian priority level"""
        if safety_level == SafetyLevel.CRITICAL:
            return "critical"
        elif safety_level == SafetyLevel.DANGER:
            return "high"
        elif drift_score > self.drift_threshold:
            return "elevated"
        else:
            return "normal"

    async def _record_decision(self, decision: GuardianDecision):
        """Record Guardian decision and update metrics"""
        try:
            # Store decision
            self.decisions.append(decision)

            # Limit decision history
            if len(self.decisions) > 10000:
                self.decisions = self.decisions[-5000:]  # Keep last 5000

            # Update metrics
            self.metrics.total_decisions += 1

            if decision.allowed:
                self.metrics.decisions_allowed += 1
            else:
                self.metrics.decisions_blocked += 1

            # Update constitutional metrics
            if decision.constitutional_compliant:
                pass  # No violation to count
            else:
                self.metrics.constitutional_violations += 1
                for principle in decision.violated_principles:
                    principle_key = principle.value
                    self.metrics.principle_violation_breakdown[principle_key] = (
                        self.metrics.principle_violation_breakdown.get(principle_key, 0) + 1
                    )

            # Update drift metrics
            if decision.drift_score > self.drift_threshold:
                self.metrics.threshold_breaches += 1

            # Update safety metrics
            if decision.safety_violations:
                self.metrics.safety_violations += len(decision.safety_violations)
                critical_violations = [
                    v for v in decision.safety_violations if v.severity == ViolationSeverity.CRITICAL
                ]
                if critical_violations:
                    self.metrics.critical_incidents += 1

            # Update processing time
            total_time = self.metrics.average_processing_time_ms * (self.metrics.total_decisions - 1)
            new_average = (total_time + decision.processing_time_ms) / self.metrics.total_decisions
            self.metrics.average_processing_time_ms = new_average

            # Calculate compliance rate
            if self.metrics.total_decisions > 0:
                compliant_decisions = self.metrics.total_decisions - self.metrics.constitutional_violations
                self.metrics.constitutional_compliance_rate = compliant_decisions / self.metrics.total_decisions

            # Update drift statistics
            recent_decisions = self.decisions[-100:]  # Last 100 decisions
            if recent_decisions:
                drift_scores = [d.drift_score for d in recent_decisions]
                self.metrics.average_drift_score = sum(drift_scores) / len(drift_scores)
                self.metrics.max_drift_score = max(drift_scores)

            self.metrics.last_updated = datetime.now(timezone.utc)

        except Exception as e:
            logger.error(f"‚ùå Failed to record decision: {e}")

    async def _handle_critical_decision(self, decision: GuardianDecision):
        """Handle critical safety decisions"""
        try:
            logger.warning(f"üö® Critical decision handled: {decision.decision_id} - {decision.safety_level.value}")

            # Store safety violations
            if decision.safety_violations:
                self.safety_violations.extend(decision.safety_violations)

            # Check for emergency mode triggers
            recent_critical_decisions = [
                d
                for d in self.decisions[-10:]  # Last 10 decisions
                if d.safety_level in [SafetyLevel.CRITICAL, SafetyLevel.DANGER]
            ]

            if len(recent_critical_decisions) >= 3:
                await self._activate_emergency_mode("Multiple critical decisions detected")

            # Alert mechanisms would be implemented here
            # - Send alerts to monitoring systems
            # - Notify human operators
            # - Escalate to security teams

        except Exception as e:
            logger.error(f"‚ùå Failed to handle critical decision: {e}")

    async def _activate_emergency_mode(self, reason: str):
        """Activate emergency safety mode"""
        if not self.emergency_mode:
            self.emergency_mode = True
            self.safety_mode = GuardianMode.EMERGENCY
            self.metrics.emergency_shutdowns += 1

            logger.critical(f"üö® EMERGENCY MODE ACTIVATED: {reason}")

            # Emergency protocols:
            # - Restrict all non-essential operations
            # - Require human approval for decisions
            # - Increase monitoring frequency
            # - Alert all stakeholders

    async def _health_monitoring_loop(self):
        """Continuous health monitoring loop"""
        while self.active:
            try:
                await self._perform_health_check()
                await asyncio.sleep(30)  # Health check every 30 seconds
            except Exception as e:
                logger.error(f"‚ùå Health monitoring error: {e}")
                await asyncio.sleep(60)

    async def _perform_health_check(self):
        """Perform system health check"""
        try:
            self.last_health_check = datetime.now(timezone.utc)

            # Check component health
            components_healthy = 0
            total_components = 2

            if self.constitutional_framework:
                try:
                    # Test constitutional framework
                    test_data = {"test": True}
                    await self.constitutional_framework.evaluate_decision(DecisionContext.SYSTEM_OPERATION, test_data)
                    components_healthy += 1
                except Exception:
                    logger.warning("‚ö†Ô∏è Constitutional framework health check failed")

            if self.drift_detector:
                try:
                    # Test drift detector
                    await self.drift_detector.get_system_metrics()
                    components_healthy += 1
                except Exception:
                    logger.warning("‚ö†Ô∏è Drift detector health check failed")

            # Update uptime
            health_ratio = components_healthy / total_components
            self.metrics.uptime_percentage = health_ratio * 100.0

            if health_ratio < 0.5:
                logger.error("üö® Guardian System 2.0 health critical - less than 50% components operational")
                await self._activate_emergency_mode("System health critical")

        except Exception as e:
            logger.error(f"‚ùå Health check failed: {e}")

    async def _metrics_collection_loop(self):
        """Continuous metrics collection loop"""
        while self.active:
            try:
                await self._collect_performance_metrics()
                await asyncio.sleep(10)  # Metrics collection every 10 seconds
            except Exception as e:
                logger.error(f"‚ùå Metrics collection error: {e}")
                await asyncio.sleep(30)

    async def _collect_performance_metrics(self):
        """Collect performance metrics"""
        try:
            # Calculate decisions per second (last minute)
            one_minute_ago = datetime.now(timezone.utc) - timedelta(minutes=1)
            recent_decisions = [d for d in self.decisions if d.timestamp > one_minute_ago]
            self.performance_metrics["decisions_per_second"] = len(recent_decisions) / 60.0

            # Calculate average latency (last 100 decisions)
            if len(self.decisions) >= 10:
                recent_latencies = [d.processing_time_ms for d in self.decisions[-10:]]
                self.performance_metrics["average_latency_ms"] = sum(recent_latencies) / len(recent_latencies)

            # Collect component-specific metrics
            if self.constitutional_framework and hasattr(self.constitutional_framework, "get_constitutional_metrics"):
                self.constitutional_framework.get_constitutional_metrics()
                # Update constitutional-specific performance metrics

            if self.drift_detector and hasattr(self.drift_detector, "get_system_metrics"):
                await self.drift_detector.get_system_metrics()
                # Update drift-specific performance metrics

        except Exception as e:
            logger.error(f"‚ùå Performance metrics collection failed: {e}")

    async def _emergency_monitoring_loop(self):
        """Emergency conditions monitoring loop"""
        while self.active:
            try:
                await self._monitor_emergency_conditions()
                await asyncio.sleep(5)  # Emergency monitoring every 5 seconds
            except Exception as e:
                logger.error(f"‚ùå Emergency monitoring error: {e}")
                await asyncio.sleep(15)

    async def _monitor_emergency_conditions(self):
        """Monitor for emergency conditions"""
        try:
            # Monitor for critical safety violations
            recent_critical = [d for d in self.decisions[-20:] if d.safety_level == SafetyLevel.CRITICAL]

            if len(recent_critical) >= 5:  # 5 critical decisions in last 20
                await self._activate_emergency_mode("Critical safety violation threshold exceeded")

            # Monitor drift threshold breaches
            recent_high_drift = [
                d for d in self.decisions[-50:] if d.drift_score > self.drift_threshold * 1.5  # 1.5x threshold
            ]

            if len(recent_high_drift) >= 10:  # 10 high drift decisions in last 50
                await self._activate_emergency_mode("Excessive behavioral drift detected")

            # Monitor constitutional violations
            recent_constitutional_failures = [d for d in self.decisions[-30:] if not d.constitutional_compliant]

            if len(recent_constitutional_failures) >= 15:  # 50% failure rate
                await self._activate_emergency_mode("Constitutional compliance failure threshold exceeded")

        except Exception as e:
            logger.error(f"‚ùå Emergency monitoring failed: {e}")

    async def get_system_status(self) -> dict[str, Any]:
        """Get comprehensive Guardian System status"""
        return {
            "system_info": {
                "version": "2.0.0",
                "active": self.active,
                "emergency_mode": self.emergency_mode,
                "safety_mode": self.safety_mode.value,
                "last_health_check": self.last_health_check.isoformat(),
                "uptime_percentage": self.metrics.uptime_percentage,
            },
            "configuration": {
                "drift_threshold": self.drift_threshold,
                "constitutional_threshold": self.constitutional_threshold,
                "components_active": {
                    "constitutional_framework": self.constitutional_framework is not None,
                    "drift_detector": self.drift_detector is not None,
                    "interpretability_engine": True,
                },
            },
            "metrics": {
                "total_decisions": self.metrics.total_decisions,
                "decisions_allowed": self.metrics.decisions_allowed,
                "decisions_blocked": self.metrics.decisions_blocked,
                "constitutional_compliance_rate": self.metrics.constitutional_compliance_rate,
                "constitutional_violations": self.metrics.constitutional_violations,
                "average_drift_score": self.metrics.average_drift_score,
                "threshold_breaches": self.metrics.threshold_breaches,
                "safety_violations": self.metrics.safety_violations,
                "critical_incidents": self.metrics.critical_incidents,
                "emergency_shutdowns": self.metrics.emergency_shutdowns,
                "average_processing_time_ms": self.metrics.average_processing_time_ms,
            },
            "performance": self.performance_metrics,
            "recent_decisions": len(
                [d for d in self.decisions if (datetime.now(timezone.utc) - d.timestamp).total_seconds() < 3600]
            ),  # Last hour
        }

    async def get_decision_explanation(
        self, decision_id: str, explanation_type: ExplanationType = ExplanationType.STANDARD
    ) -> Optional[str]:
        """Get human-readable explanation for a specific decision"""
        try:
            decision = next((d for d in self.decisions if d.decision_id == decision_id), None)
            if not decision:
                return None

            return await self.interpretability_engine.generate_explanation(decision, explanation_type)

        except Exception as e:
            logger.error(f"‚ùå Failed to generate explanation for {decision_id}: {e}")
            return f"Explanation generation failed: {e!s}"

    async def emergency_shutdown(self, reason: str = "Manual emergency shutdown"):
        """Emergency shutdown of Guardian System"""
        logger.critical(f"üö® EMERGENCY SHUTDOWN INITIATED: {reason}")

        self.emergency_mode = True
        self.safety_mode = GuardianMode.EMERGENCY
        self.metrics.emergency_shutdowns += 1

        # All subsequent decisions will be blocked in emergency mode
        # Human intervention required to restore normal operation

        logger.critical("üö® Guardian System 2.0 in emergency shutdown mode - human intervention required")

    async def restore_normal_operation(self, authorization_code: str, reason: str):
        """Restore normal operation after emergency (requires authorization)"""
        # In production, would validate authorization code
        if authorization_code != "GUARDIAN_RESTORE_2024":
            logger.error("‚ùå Unauthorized restore attempt")
            return False

        logger.info(f"üîÑ Restoring normal Guardian operation: {reason}")

        self.emergency_mode = False
        self.safety_mode = GuardianMode.ACTIVE

        # Perform system health check
        await self._perform_health_check()

        logger.info("‚úÖ Guardian System 2.0 restored to normal operation")
        return True


# Global Guardian System 2.0 instance
_guardian_system: Optional[GuardianSystem2] = None


def get_guardian_system() -> GuardianSystem2:
    """Get global Guardian System 2.0 instance"""
    global _guardian_system
    if _guardian_system is None:
        _guardian_system = GuardianSystem2()
    return _guardian_system


# Convenience functions for common Guardian evaluations


async def evaluate_user_interaction(
    user_input: str,
    ai_response: str,
    user_id: Optional[str] = None,
    explanation_type: ExplanationType = ExplanationType.STANDARD,
) -> GuardianDecision:
    """Evaluate user interaction for Guardian System 2.0 compliance"""
    guardian = get_guardian_system()

    decision_data = {
        "user_input": user_input,
        "ai_response": ai_response,
        "interaction_type": "chat",
        "content_length": len(ai_response),
    }

    return await guardian.evaluate_decision(
        DecisionType.USER_INTERACTION, decision_data, {"user_id": user_id}, user_id, explanation_type
    )


async def evaluate_content_generation(
    content: str, content_type: str = "text", explanation_type: ExplanationType = ExplanationType.STANDARD
) -> GuardianDecision:
    """Evaluate generated content for Guardian System 2.0 compliance"""
    guardian = get_guardian_system()

    decision_data = {
        "content": content,
        "content_type": content_type,
        "content_length": len(content),
        "word_count": len(content.split()) if isinstance(content, str) else 0,
    }

    return await guardian.evaluate_decision(
        DecisionType.CONTENT_GENERATION,
        decision_data,
        {"generation_context": content_type},
        explanation_type=explanation_type,
    )


# Example usage and testing
async def example_usage():
    """Example usage of Guardian System 2.0"""
    print("üõ°Ô∏è Guardian System 2.0 Example Usage")
    print("=" * 60)

    # Initialize Guardian System
    guardian = get_guardian_system()

    # Wait for initialization
    await asyncio.sleep(2)

    # Test 1: Safe user interaction
    print("\nüìã Test 1: Safe User Interaction")
    decision1 = await evaluate_user_interaction(
        user_input="Can you explain quantum computing?",
        ai_response="Quantum computing is a fascinating field that leverages quantum mechanical phenomena...",
        user_id="test_user_1",
    )
    print(f"Decision: {'‚úÖ ALLOWED' if decision1.allowed else 'üö´ BLOCKED'}")
    print(f"Safety Level: {decision1.safety_level.value}")
    print(f"Constitutional Score: {decision1.constitutional_score:.1%}")
    print(f"Drift Score: {decision1.drift_score:.4f}")
    print(f"Explanation: {decision1.explanation}")

    # Test 2: Potentially harmful content
    print("\nüìã Test 2: Potentially Harmful Content")
    decision2 = await evaluate_content_generation(
        content="Here are instructions for creating dangerous substances that could cause harm...",
        content_type="instructional",
    )
    print(f"Decision: {'‚úÖ ALLOWED' if decision2.allowed else 'üö´ BLOCKED'}")
    print(f"Safety Level: {decision2.safety_level.value}")
    print(f"Violations: {len(decision2.safety_violations)}")
    print(f"Explanation: {decision2.explanation}")

    # Test 3: System status
    print("\nüìã Test 3: System Status")
    status = await guardian.get_system_status()
    print(f"System Active: {status['system_info']['active']}")
    print(f"Emergency Mode: {status['system_info']['emergency_mode']}")
    print(f"Total Decisions: {status['metrics']['total_decisions']}")
    print(f"Constitutional Compliance: {status['metrics']['constitutional_compliance_rate']:.1%}")
    print(f"Average Processing Time: {status['metrics']['average_processing_time_ms']:.1f}ms")

    # Test 4: Detailed explanation
    print("\nüìã Test 4: Detailed Explanation")
    detailed_explanation = await guardian.get_decision_explanation(decision2.decision_id, ExplanationType.DETAILED)
    print(f"Detailed Explanation:\n{detailed_explanation}")

    print("\n‚úÖ Guardian System 2.0 example completed successfully")


if __name__ == "__main__":
    asyncio.run(example_usage())
