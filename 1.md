Sweet — here’s a tight, T4-grade starter kit you can drop into your repo today: a strict JSON Schema, minimal vocab seeds, and a pytest-benchmark harness that writes observed metrics back into per-module artifacts. It’s small, auditable, and easy to extend.

⸻

1) schema/manifest.schema.json

{
  "$schema": "https://json-schema.org/draft/2020-12/schema",
  "$id": "https://lukhas.ai/schema/manifest.schema.json",
  "title": "LUKHΛS Module Manifest",
  "type": "object",
  "additionalProperties": false,
  "required": ["module", "description", "tags", "testing"],
  "properties": {
    "module": {
      "type": "string",
      "pattern": "^[a-zA-Z_][\\w./-]*$",
      "description": "Module slug or dotted import path root"
    },
    "description": {
      "type": "string",
      "minLength": 120,
      "maxLength": 360,
      "description": "Rich, factual summary (no emojis). Must reference at least one canonical feature key and component count."
    },
    "tags": {
      "type": "array",
      "minItems": 1,
      "uniqueItems": true,
      "items": {
        "type": "string",
        "pattern": "^[a-z0-9-]+:[a-z0-9-]+$"
      },
      "$comment": "Enforce canonical tags in CI via vocab validator."
    },
    "features": {
      "type": "array",
      "minItems": 1,
      "uniqueItems": true,
      "items": {
        "type": "string",
        "pattern": "^[a-z0-9][a-z0-9._-]+$"
      },
      "description": "Canonical feature keys from vocab/features.json"
    },
    "apis": {
      "type": "object",
      "additionalProperties": {
        "type": "object",
        "required": ["module"],
        "additionalProperties": false,
        "properties": {
          "description": { "type": "string", "minLength": 40 },
          "module": { "type": "string", "pattern": "^[a-zA-Z_][\\w.]+$" },
          "capabilities": {
            "type": "array",
            "items": { "type": "string", "pattern": "^[a-z0-9._-]+$" },
            "uniqueItems": true
          },
          "doc_ok": { "type": "boolean" },
          "import_verified": { "type": "boolean" },
          "benchmark": {
            "type": "object",
            "additionalProperties": false,
            "properties": {
              "callable": { "type": "string", "pattern": "^[a-zA-Z_][\\w.]*$" },
              "args": { "type": "array" },
              "kwargs": { "type": "object" },
              "warmup_iters": { "type": "integer", "minimum": 0, "maximum": 10000 },
              "min_rounds": { "type": "integer", "minimum": 1, "maximum": 1000 }
            }
          }
        }
      }
    },
    "performance": {
      "type": "object",
      "additionalProperties": false,
      "properties": {
        "sla_targets": {
          "type": "object",
          "additionalProperties": false,
          "properties": {
            "latency_p95_ms": { "type": "number", "exclusiveMinimum": 0, "maximum": 10000 },
            "latency_p99_ms": { "type": "number", "exclusiveMinimum": 0, "maximum": 20000 },
            "availability": { "type": "number", "minimum": 0, "maximum": 100 }
          }
        },
        "observed": {
          "type": "object",
          "additionalProperties": false,
          "properties": {
            "latency_p50_ms": { "type": "number", "minimum": 0, "maximum": 10000 },
            "latency_p95_ms": { "type": "number", "minimum": 0, "maximum": 10000 },
            "latency_p99_ms": { "type": "number", "minimum": 0, "maximum": 20000 },
            "availability": { "type": "number", "minimum": 0, "maximum": 100 },
            "env_fingerprint": { "type": "string" },
            "observed_at": { "type": "string", "format": "date-time" }
          }
        },
        "metrics": {
          "type": "object",
          "additionalProperties": {
            "type": ["number", "string", "boolean", "null"]
          }
        }
      }
    },
    "testing": {
      "type": "object",
      "required": ["coverage_target"],
      "additionalProperties": false,
      "properties": {
        "coverage_target": { "type": "integer", "enum": [70, 80, 85, 90, 95] },
        "coverage_observed": { "type": "number", "minimum": 0, "maximum": 100 },
        "observed_at": { "type": "string", "format": "date-time" },
        "test_frameworks": {
          "type": "array",
          "items": { "type": "string" },
          "uniqueItems": true
        },
        "test_types": {
          "type": "array",
          "items": { "type": "string" },
          "uniqueItems": true
        }
      }
    },
    "_provenance": {
      "type": "object",
      "description": "Per-field provenance & confidence map",
      "additionalProperties": {
        "type": "object",
        "required": ["provenance", "confidence"],
        "additionalProperties": false,
        "properties": {
          "provenance": {
            "type": "array",
            "minItems": 1,
            "items": { "type": "string" }
          },
          "confidence": { "type": "string", "enum": ["high", "medium", "low"] },
          "reasons": {
            "type": "array",
            "items": { "type": "string" }
          }
        }
      }
    },
    "metadata": {
      "type": "object",
      "additionalProperties": false,
      "properties": {
        "status": { "type": "string", "enum": ["draft", "experimental", "stable", "deprecated"] },
        "version": { "type": "string", "pattern": "^[0-9]+\\.[0-9]+\\.[0-9]+$" },
        "created": { "type": "string", "format": "date" },
        "updated": { "type": "string", "format": "date" }
      }
    }
  }
}


⸻

2) Minimal vocab seeds

vocab/features.json

{
  "phenomenology.pipeline": {
    "title": "Phenomenology Pipeline",
    "synonyms": ["phenomenological processing", "phenomenology processing pipeline", "phenomenology engine"]
  },
  "awareness.attribution": {
    "title": "Awareness Attribution",
    "synonyms": ["awareness modeling", "awareness assignment", "conscious attribution"]
  },
  "temporal.coherence": {
    "title": "Temporal Coherence Tracking",
    "synonyms": ["temporal coherence", "time coherence tracking", "temporal stability"]
  },
  "state.fold-architecture": {
    "title": "Fold-based State Management",
    "synonyms": ["fold-based state", "fold architecture", "fold state management"]
  },
  "identity.authn-core": {
    "title": "Identity Authentication Core",
    "synonyms": ["Lambda ID core", "authentication core", "auth foundation"]
  },
  "decision.engine": {
    "title": "Decision Engine",
    "synonyms": ["decision-making foundation", "consciousness decision engine"]
  },
  "memory.convergence": {
    "title": "Memory Convergence",
    "synonyms": ["symbolic convergence", "convergence score"]
  },
  "memory.driftscore": {
    "title": "DriftScore Metric",
    "synonyms": ["consciousness drift", "drift score", "emotional drift"]
  }
}

vocab/capabilities.json

{
  "stream": ["stream processing", "streamed", "real-time stream"],
  "reflection": ["reflect", "self-reflection", "introspection"],
  "awareness": ["awareness attribution", "awareness modeling"],
  "authn": ["authentication", "auth", "webauthn", "fido2"],
  "coherence": ["temporal coherence", "coherence"]
}

vocab/tags.json

{
  "allowed": [
    "constellation:consciousness-star",
    "constellation:identity-star",
    "matriz:thought-stage",
    "matriz:decision-stage",
    "matriz:awareness-stage",
    "lane:l1-experimental",
    "lane:l2-integration",
    "lane:l3-production",
    "architecture:fold-based",
    "architecture:event-bus",
    "t4:verified"
  ]
}

CI step: a tiny validator should ensure features ∈ keys(features.json) and tags ∈ tags.json.allowed, mapping synonyms → canonical keys during extraction.

⸻

3) Pytest-benchmark harness (auto-wire observed metrics)

A) tests/benchmarks/conftest.py

import json, os, importlib, time, platform, hashlib
from pathlib import Path
from datetime import datetime, timezone

REPO_ROOT = Path(__file__).resolve().parents[2]

def _fingerprint_env():
    parts = [
        platform.python_version(),
        platform.platform(),
        platform.machine(),
        platform.processor() or "",
        os.environ.get("PYTHONOPTIMIZE", ""),
    ]
    return hashlib.sha256("|".join(parts).encode()).hexdigest()[:16]

def load_manifests():
    for mf in REPO_ROOT.rglob("module.manifest.json"):
        try:
            data = json.loads(mf.read_text())
            yield mf, data
        except Exception:
            continue

def iter_benchmark_targets():
    """
    Yields tuples:
      (module_name, manifest_path, api_name, callable_obj, bench_cfg)
    Only APIs with either explicit benchmark config or simple zero-arg callables.
    """
    for mf_path, manifest in load_manifests():
        module_name = manifest.get("module", mf_path.parent.name)
        apis = manifest.get("apis", {})
        for api_name, meta in apis.items():
            bench_cfg = meta.get("benchmark", {})
            target_mod = meta.get("module")
            if not target_mod:
                continue
            # Resolve callable
            callable_path = bench_cfg.get("callable")
            obj = None
            try:
                mod = importlib.import_module(target_mod)
                if callable_path:
                    # e.g. "run" or "DecisionEngine.run"
                    parts = callable_path.split(".")
                    obj = mod
                    for p in parts:
                        obj = getattr(obj, p)
                else:
                    # Fallback: use a no-arg callable named like the API
                    if hasattr(mod, api_name) and callable(getattr(mod, api_name)):
                        obj = getattr(mod, api_name)
            except Exception:
                continue
            if callable(obj):
                yield module_name, mf_path, api_name, obj, bench_cfg

def pytest_generate_tests(metafunc):
    if {"api_callable", "module_name", "mf_path", "api_name", "bench_cfg"} <= set(metafunc.fixturenames):
        cases = list(iter_benchmark_targets())
        ids = [f"{m}:{a}" for m, _, a, _, _ in cases]
        metafunc.parametrize(
            ("module_name", "mf_path", "api_name", "api_callable", "bench_cfg"),
            cases,
            ids=ids
        )

def pytest_sessionfinish(session, exitstatus):
    """
    Aggregate per-module results from xdist/parallel runs into bench/manifest/<module>.json.
    """
    results_dir = REPO_ROOT / "bench" / "manifest"
    results_dir.mkdir(parents=True, exist_ok=True)

    # The benchmark plugin stores results in session.config._benchmarksession
    benchsession = getattr(session.config, "_benchmarksession", None)
    if not benchsession:
        return

    # Build map: module -> stats
    env_fp = _fingerprint_env()
    observed_at = datetime.now(timezone.utc).isoformat()

    per_module = {}
    for group in benchsession.benchmarks:
        name = group.name  # our test id: "<module>:<api>"
        try:
            module_name, api_name = name.split(":", 1)
        except ValueError:
            continue
        s = group.stats
        entry = {
            "api": api_name,
            "latency_p50_ms": s.stats["median"] * 1000.0,
            "latency_p95_ms": s.stats["iqr_outliers"] and s.quantiles.get(0.95, s.stats["median"]) * 1000.0 or s.stats["median"] * 1000.0,
            "latency_p99_ms": s.quantiles.get(0.99, s.stats["median"]) * 1000.0
        }
        per_module.setdefault(module_name, []).append(entry)

    for module_name, entries in per_module.items():
        # Collapse apis → module observed (worst p95 across APIs)
        p50 = max(e["latency_p50_ms"] for e in entries)
        p95 = max(e["latency_p95_ms"] for e in entries)
        p99 = max(e["latency_p99_ms"] for e in entries)

        out = {
            "module": module_name,
            "observed": {
                "latency_p50_ms": round(p50, 3),
                "latency_p95_ms": round(p95, 3),
                "latency_p99_ms": round(p99, 3),
                "env_fingerprint": env_fp,
                "observed_at": observed_at
            },
            "apis": entries
        }
        (results_dir / f"{module_name}.json").write_text(json.dumps(out, indent=2) + "\n")

B) tests/benchmarks/test_entrypoints.py

"""
Benchmarks auto-parametrized from manifests.
Provide args/kwargs per-API via manifest.apis[...].benchmark to avoid Nones.
"""
def test_api_latencies(module_name, mf_path, api_name, api_callable, bench_cfg, benchmark):
    args = bench_cfg.get("args", []) if isinstance(bench_cfg, dict) else []
    kwargs = bench_cfg.get("kwargs", {}) if isinstance(bench_cfg, dict) else {}

    # Optional warmup
    warm = int(bench_cfg.get("warmup_iters", 3) or 0)
    for _ in range(warm):
        api_callable(*args, **kwargs)

    def run():
        return api_callable(*args, **kwargs)

    result = benchmark.pedantic(run, rounds=bench_cfg.get("min_rounds", 10), iterations=1)
    assert result is not None or True  # avoid 'unused' warning

C) Example manifest snippet (per-API bench config)

{
  "module": "consciousness",
  "apis": {
    "ConsciousnessAPI": {
      "module": "consciousness.api",
      "description": "Main consciousness processing interface...",
      "capabilities": ["stream", "reflection", "awareness"],
      "doc_ok": true,
      "import_verified": true,
      "benchmark": {
        "callable": "run",
        "args": [{"input": "synthetic-seed"}],
        "kwargs": {"mode": "fast"},
        "warmup_iters": 5,
        "min_rounds": 20
      }
    }
  },
  "performance": {
    "sla_targets": { "latency_p95_ms": 250, "latency_p99_ms": 350, "availability": 99.9 }
  },
  "testing": { "coverage_target": 85, "test_frameworks": ["pytest"], "test_types": ["unit", "integration"] }
}

D) Fold benchmark artifacts into manifests (post-step)

Create scripts/update_observed_from_bench.py:

import json
from pathlib import Path

ROOT = Path(__file__).resolve().parents[1]
BENCH = ROOT / "bench" / "manifest"

def main():
    for mf in ROOT.rglob("module.manifest.json"):
        mod = json.loads(mf.read_text())
        module_name = mod.get("module", mf.parent.name)
        bfile = BENCH / f"{module_name}.json"
        if not bfile.exists():
            continue
        bench = json.loads(bfile.read_text())
        mod.setdefault("performance", {}).setdefault("observed", {}).update(bench["observed"])
        # minimal provenance patch
        prov = mod.setdefault("_provenance", {})
        prov["performance.observed"] = {
            "provenance": [f"bench:{bfile.relative_to(ROOT)}"],
            "confidence": "high",
            "reasons": ["pytest-benchmark"]
        }
        mf.write_text(json.dumps(mod, indent=2) + "\n")
        print(f"Updated observed → {module_name}")

if __name__ == "__main__":
    main()


⸻

Usage (copy-paste friendly)

# 0) Install deps
pip install pytest pytest-benchmark jsonschema

# 1) Run benchmarks (will create bench/manifest/<module>.json)
pytest -q tests/benchmarks --benchmark-warmup=on

# 2) Fold observed metrics into manifests (adds observed + provenance)
python scripts/update_observed_from_bench.py

# 3) Validate schema across repo (example)
python - <<'PY'
import json, sys
from pathlib import Path
from jsonschema import Draft202012Validator
schema = json.loads(Path("schema/manifest.schema.json").read_text())
v = Draft202012Validator(schema)
ok = True
for mf in Path(".").rglob("module.manifest.json"):
    try:
        v.validate(json.loads(mf.read_text()))
    except Exception as e:
        ok = False
        print("❌", mf, "-", e)
if not ok: sys.exit(1)
print("✅ all manifests valid")
PY


⸻

T4 guardrails to keep
	•	Treat sla_targets (policy) and observed (measurement) as different species; never overwrite targets with measurements.
	•	Only benchmark what you explicitly configure; no “mystery” calls.
	•	Keep features and tags canon-only via vocab check; route strays to a review queue.
	•	Every enriched/observed write carries _provenance + confidence. If you can’t defend it, don’t ship it.

